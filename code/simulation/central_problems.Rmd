---
title: "PEDLR - Simulation"
output:
  html_document:
    toc: yes
    self_contained: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

# Paths and libraries

- Import libraries

```{r message=FALSE}
library(reshape2)
library(ggplot2)
library(plotly)
library(plyr)
library(Rfast)
library(data.table)
library(knitr)
#library(rstudioapi)
#library(here)
library(viridis)
library(cowplot)
library(here)
library(optparse)
```


- Set paths

```{r}
base_path = file.path(here::here(), fsep = .Platform$file.sep)
derivatives_path = file.path(base_path, 'derivatives', 'simulation',
                             fsep = .Platform$file.sep)
source_path = file.path(base_path, 'code', fsep = .Platform$file.sep)
```

- Import own functions

```{r}
# Source functions required for this script
source(file.path(source_path, 'design', 'Beta_pseudo_sim.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Gaussian_pseudo_sim.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Uniform_pseudo_sim.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Bimodal_pseudo_sim.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Create_design_complete.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Sample_subjects.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'models', 'Pedlr.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'models', 'Pedlr_interdep.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'models', 'Softmax_choice.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Apply_model.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_results.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_bias_value.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_perc_correct.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_bias_correct.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Free_bimodal_modes.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Free_bimodal_means.R',
                 fsep = .Platform$file.sep))
```

---

# Set parameters for simulation

```{r}
# Number of simulated subjects
n_subjects = 100

# Distributions
# Gaussian
# SD similar for all Gaussians
gaussian_sd = (100 * 1/6) / 3
# Mean of different Gaussians
# Lower end
gaussian_le_mean = (100 * 1/6) * 1
# Middle
gaussian_mid_mean = (100 * 1/6) * 3
# Upper end
gaussian_ue_mean = (100 * 1/6) * 5
# Bimodal
# Same sd for each mode
bimodal_sd = gaussian_sd
# Same relative proportion (between modes) for both bimodals
bimodal_rel_proportion = 0.2
## Overall mean bimodal distributions
bimodal_mean_gain = (100 * 1/6) * 2
bimodal_mean_loss = (100 * 1/6) * 4
# Distance between modes in gain and loss bimodal
bimodal_distance_gain = 40
bimodal_distance_loss = -40

# Design
# Number of overall blocks
n_blocks = 6
# Number of blocks dedicated to each task version
blocks_per_task = 3
# Percentage of forced choice trials
perc_forced = 20
# List of different distributions with parameters (three distributions for each 
# task version)
dist_list = list(c('gaussian', gaussian_le_mean, gaussian_sd),
                 c('bimodal',
                   bimodal_mean_gain,
                   bimodal_rel_proportion,
                   bimodal_distance_gain,
                   bimodal_sd,
                   bimodal_sd),
                 c('gaussian', gaussian_mid_mean, gaussian_sd),
                 c('gaussian', gaussian_mid_mean, gaussian_sd),
                 c('bimodal',
                   bimodal_mean_loss,
                   bimodal_rel_proportion,
                   bimodal_distance_loss,
                   bimodal_sd,
                   bimodal_sd),
                 c('gaussian', gaussian_ue_mean, gaussian_sd))

# Other parameters
set_seed = TRUE
reward_space_ub = 100
init_values = list(c(50,50,50),
                   c(50,50,50))
shrink_distance_vec = 0
```

---

# 1. Errors on side with rare outcomes increase with alpha 1

- Set parameters for this simulation

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = seq(from = 0, to = 1, length.out = 20)
steps_temperature = 7
steps_policy = c('softmax')
steps_interdep = 0.5
# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')
# Add constant parameters
parameters$reward_space_ub = reward_space_ub
```

## Pedlr

```{r}
# Model
model = 'Pedlr'

# Saving simulation
# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_1_pedlr.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias_correct = Model_bias_correct(n_subjects = n_subjects,
                                  set_seed = set_seed,
                                  n_blocks = n_blocks,
                                  perc_forced = perc_forced,
                                  blocks_per_task = blocks_per_task,
                                  dist_list = dist_list,
                                  model = model,
                                  parameters = parameters,
                                  init_values = init_values,
                                  shrink_distance_vec = shrink_distance_vec,
                                  save_data = save_data,
                                  save_file = save_file,
                                  load_data = load_data,
                                  load_file = load_file)
```

### Errors

```{r}
# Plot data
data_plot = bias_correct$data_bias_correct
data_plot$alpha1 = round(data_plot$alpha1, 3)
data_plot$perc_correct = data_plot$perc_correct * 100
data_plot = ddply(data_plot,
                  .(task_version,
                    comp,
                    alpha0,
                    alpha1,
                    temperature,
                    choice_policy,
                    reward_space_ub,
                    distance_shrinkage),
                  summarize,
                  mean_perc_correct = mean(perc_correct),
                  sd_perc_correct = sd(perc_correct))

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=alpha1, group=alpha1)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Influence of alpha1 on correct choices')
```

Increasing alpha_1 produces more errors on the side of the distribution with rare events.

### SD of choice correctness

```{r}
# Plot data
data_plot = bias_correct$data_bias_correct
data_plot$alpha1 = round(data_plot$alpha1, 3)
data_plot$perc_correct = data_plot$perc_correct * 100
data_plot = ddply(data_plot,
                  .(task_version,
                    comp,
                    alpha0,
                    alpha1,
                    temperature,
                    choice_policy,
                    reward_space_ub,
                    distance_shrinkage),
                  summarize,
                  mean_perc_correct = mean(perc_correct),
                  sd_perc_correct = sd(perc_correct))

ggplot(data=data_plot, aes(x=comp, y=sd_perc_correct, color=alpha1, group=alpha1)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Influence of alpha1 on SD of correct choices')
```
Standard deviation of percentage of correct choices rises with increasing alpha_1.

### Balance

```{r}
data_balance = subset(bias_correct$data_bias_correct, comp == '1v2')
data_balance = select(data_balance, -c(comp, perc_correct))
data_balance$corr_1v2 = subset(bias_correct$data_bias_correct, comp == '1v2')$perc_correct
data_balance$corr_2v3 = subset(bias_correct$data_bias_correct, comp == '2v3')$perc_correct
data_balance$corr_1v3 = subset(bias_correct$data_bias_correct, comp == '1v3')$perc_correct
data_balance$balance = data_balance$corr_2v3 - data_balance$corr_1v2
data_balance = select(data_balance, -c(corr_1v2, corr_2v3, corr_1v3))
data_balance = ddply(data_balance,
                     .(task_version,
                       alpha0,
                       alpha1,
                       temperature,
                       choice_policy,
                       reward_space_ub,
                       distance_shrinkage),
                     summarize,
                     mean_balance = mean(balance),
                     sd_balance = sd(balance))

ggplot(data=data_balance, aes(x=alpha1, y=mean_balance)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_balance - sd_balance, ymax = mean_balance + sd_balance),
              alpha=0.2) +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Balance of correct choices')
```

Increasing alpha_1 creates negative balance in gain case and strengthens 
positive balance in loss case (as intended).

---

# 2. Moving means of distributions closer to center causes more errors

- Set parameters for this simulation

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = 0.7
steps_temperature = 7
steps_policy = c('softmax')
steps_interdep = 0.5
# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')
# Add constant parameters
parameters$reward_space_ub = reward_space_ub

# Add increments of pushing distributions closer together
shrink_distance_vec = round(seq(0,100/6), 2)
```

## Pedlr

```{r}
# Model
model = 'Pedlr'

# Saving simulation
# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_2_pedlr.tsv')
save_data = TRUE
save_file = file
load_data = FALSE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias_correct = Model_bias_correct(n_subjects = n_subjects,
                                  set_seed = set_seed,
                                  n_blocks = n_blocks,
                                  perc_forced = perc_forced,
                                  blocks_per_task = blocks_per_task,
                                  dist_list = dist_list,
                                  model = model,
                                  parameters = parameters,
                                  init_values = init_values,
                                  shrink_distance_vec = shrink_distance_vec,
                                  save_data = save_data,
                                  save_file = save_file,
                                  load_data = load_data,
                                  load_file = load_file)
```

### Error

```{r}
# Plot data
data_plot = bias_correct$data_bias_correct
data_plot$alpha1 = round(data_plot$alpha1, 3)
data_plot$perc_correct = data_plot$perc_correct * 100
data_plot = ddply(data_plot,
                  .(task_version,
                    comp,
                    alpha0,
                    alpha1,
                    temperature,
                    choice_policy,
                    reward_space_ub,
                    distance_shrinkage),
                  summarize,
                  mean_perc_correct = mean(perc_correct),
                  sd_perc_correct = sd(perc_correct))

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=distance_shrinkage, group=distance_shrinkage)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Influence of alpha1 on correct choices')
```

### Balance

```{r}
data_balance = subset(bias_correct$data_bias_correct, comp == '1v2')
data_balance = select(data_balance, -c(comp, perc_correct))
data_balance$corr_1v2 = subset(bias_correct$data_bias_correct, comp == '1v2')$perc_correct
data_balance$corr_2v3 = subset(bias_correct$data_bias_correct, comp == '2v3')$perc_correct
data_balance$corr_1v3 = subset(bias_correct$data_bias_correct, comp == '1v3')$perc_correct
data_balance$balance = data_balance$corr_2v3 - data_balance$corr_1v2
data_balance = select(data_balance, -c(corr_1v2, corr_2v3, corr_1v3))
data_balance = ddply(data_balance,
                     .(task_version,
                       alpha0,
                       alpha1,
                       temperature,
                       choice_policy,
                       reward_space_ub,
                       distance_shrinkage),
                     summarize,
                     mean_balance = mean(balance),
                     sd_balance = sd(balance))

ggplot(data=data_balance, aes(x=distance_shrinkage, y=mean_balance)) +
  geom_line() +
  geom_ribbon(aes(ymin = mean_balance - sd_balance, ymax = mean_balance + sd_balance),
              alpha=0.2) +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Balance of correct choices')
```

Why is moving the means closer to the center enhancing the sampling bias (balance gets more positive in gain case) instead of diminishing it (balance should get negative in gain case)?

---

# 3. Systematic mean-value bias gets stronger with alpha 1

## Pedlr

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = round(seq(from = 0, to = 1, length.out = 20),2)
steps_temperature = 7
steps_policy = c('softmax')
steps_interdep = 0.5
# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')
# Add constant parameters
parameters$reward_space_ub = reward_space_ub

# Parameters for bias assessment
last_perc_of_trials = 35
average_subjects = TRUE
mean_value_calculation = 'chosen_trials'

# No increments of pushing distributions closer together
shrink_distance_vec = 0
```

```{r}
# Set model for simulation
model = 'Pedlr'

# Parameters to save and loas data output
file = file.path(derivatives_path, 'central_prob_sim_3_pedlr.tsv')
save_data = TRUE
save_file = file
load_data = FALSE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias = Model_bias_value(n_subjects = n_subjects,
                        set_seed = set_seed,
                        n_blocks = n_blocks,
                        perc_forced = perc_forced,
                        blocks_per_task = blocks_per_task,
                        dist_list = dist_list,
                        model = model,
                        parameters = parameters,
                        init_values = init_values,
                        last_perc_of_trials = last_perc_of_trials,
                        average_subjects = average_subjects,
                        mean_value_calculation = mean_value_calculation,
                        save_data = save_data,
                        save_file = save_file,
                        load_data = load_data,
                        load_file = load_file)

bias$plot_softmax
```

The bias in the value estimate of the asymmetrical distribution (calculated over
the last 35% of trials and only over chosen trials) increases with alpha_1.

---

# 4. Increasing distance between modes in bimodal distribution strengthens the effect

## Pedlr

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.3
steps_alpha1 = 0.7
steps_temperature = 7
steps_policy = 'softmax'
steps_interdep = 0.5

# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')

# Add constant parameters
parameters$reward_space_ub = reward_space_ub

# Parameters for bias assessment
shrink_distance_vec = 0
mode_distance_change_vec = seq(from = 0, to = 30)
```

```{r}
# Set model for simulation
model = 'Pedlr'

# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_4_pedlr.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

outcome = Free_bimodal_modes(n_subjects = n_subjects,
                             set_seed = set_seed,
                             n_blocks = n_blocks,
                             perc_forced = perc_forced,
                             blocks_per_task = blocks_per_task,
                             dist_list = dist_list,
                             model = model,
                             parameters = parameters,
                             init_values = init_values,
                             shrink_distance_vec = shrink_distance_vec,
                             save_data = save_data,
                             save_file = save_file,
                             load_data = load_data,
                             load_file = load_file,
                             mode_distance_change_vec = mode_distance_change_vec)
```

### Errors

```{r}
# Plot data
data_plot = outcome
data_plot$perc_correct = data_plot$perc_correct * 100
data_plot = ddply(data_plot,
                  .(task_version,
                    comp,
                    alpha0,
                    alpha1,
                    temperature,
                    choice_policy,
                    reward_space_ub,
                    distance_shrinkage,
                    bimodal_distance_change),
                  summarize,
                  mean_perc_correct = mean(perc_correct),
                  sd_perc_correct = sd(perc_correct))

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=bimodal_distance_change, group=bimodal_distance_change)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ alpha1 + task_version) +
  labs(title = 'Influence of higher (gain) and lower (loss) distance of modes on error rates')
```

Larger distance between modes (while keeping the mean constant) does not 
facilitate errors on the asymmetric side of the bimodal distribution. It seems 
to enhance sampling bias in both task versions.

---
