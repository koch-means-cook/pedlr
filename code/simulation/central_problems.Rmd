---
title: "PEDLR - Simulation"
output:
  html_document:
    toc: yes
    self_contained: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

# Paths and libraries

- Import libraries

```{r message=FALSE}
library(reshape2)
library(ggplot2)
library(plotly)
library(plyr)
library(Rfast)
library(data.table)
library(knitr)
#library(rstudioapi)
#library(here)
library(viridis)
library(cowplot)
library(here)
library(optparse)
```


- Set paths

```{r}
base_path = file.path(here::here(), fsep = .Platform$file.sep)
derivatives_path = file.path(base_path, 'derivatives', 'simulation',
                             fsep = .Platform$file.sep)
source_path = file.path(base_path, 'code', fsep = .Platform$file.sep)
```

- Import own functions

```{r}
# Source functions required for this script
source(file.path(source_path, 'design', 'Beta_pseudo_sim.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Gaussian_pseudo_sim.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Uniform_pseudo_sim.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Bimodal_pseudo_sim.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'design', 'Create_design.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Sample_subjects.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'models', 'Pedlr.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'models', 'Pedlr_interdep.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'models', 'Softmax_choice.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Apply_model.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_results.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_bias_value.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_perc_correct.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Model_bias_correct.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Free_bimodal_modes.R',
                 fsep = .Platform$file.sep))
source(file.path(source_path, 'simulation', 'Free_bimodal_means.R',
                 fsep = .Platform$file.sep))
```

---

# Set parameters for simulation

```{r}
# Number of simulated subjects
n_subjects = 100

# Distributions
# Gaussian
# SD similar for all Gaussians
gaussian_sd = (100 * 1/6) / 3
# Mean of different Gaussians
# Lower end
gaussian_le_mean = (100 * 1/6) * 1
# Middle
gaussian_mid_mean = (100 * 1/6) * 3
# Upper end
gaussian_ue_mean = (100 * 1/6) * 5
# Bimodal
# Same sd for each mode
bimodal_sd = gaussian_sd
# Same relative proportion (between modes) for both bimodals
bimodal_rel_proportion = 0.2
## Overall mean bimodal distributions
bimodal_mean_gain = (100 * 1/6) * 2
bimodal_mean_loss = (100 * 1/6) * 4
# Distance between modes in gain and loss bimodal
bimodal_distance_gain = 40
bimodal_distance_loss = -40

# Design
# Number of overall blocks
n_blocks = 6
# Number of blocks dedicated to each task version
blocks_per_task = 3
# Percentage of forced choice trials
perc_forced = 20
# List of different distributions with parameters (three distributions for each 
# task version)
dist_list = list(c('gaussian', gaussian_le_mean, gaussian_sd),
                 c('bimodal',
                   bimodal_mean_gain,
                   bimodal_rel_proportion,
                   bimodal_distance_gain,
                   bimodal_sd,
                   bimodal_sd),
                 c('gaussian', gaussian_mid_mean, gaussian_sd),
                 c('gaussian', gaussian_mid_mean, gaussian_sd),
                 c('bimodal',
                   bimodal_mean_loss,
                   bimodal_rel_proportion,
                   bimodal_distance_loss,
                   bimodal_sd,
                   bimodal_sd),
                 c('gaussian', gaussian_ue_mean, gaussian_sd))

# Other parameters
set_seed = TRUE
reward_space_ub = 100
init_values = list(c(50,50,50),
                   c(50,50,50))
shrink_distance_vec = 0
```

---

# Display distributions

```{r}
# Simulate single design to display samples
sim = Create_design(n_blocks = n_blocks,
                    blocks_per_task = blocks_per_task,
                    perc_forced = 20,
                    dist_list = dist_list)
# Get sampled rewards from both options
samples = data.table(option = c(sim$option_left, sim$option_right),
                     reward = c(sim$reward_stim_1, sim$reward_stim_2),
                     task_version = c(sim$task_version, sim$task_version))
# Plot samples for each option
ggplot(data = samples, aes(x = reward, fill = as.character(option))) +
  geom_histogram(binwidth = 1,
                 color = NA,
                 alpha = 0.8) +
  facet_grid(task_version~option)
```


---

# 1. Errors on side with rare outcomes increase with alpha 1

- Set parameters for this simulation

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = seq(from = 0, to = 1, length.out = 10)
steps_temperature = 7
steps_policy = c('softmax')
steps_interdep = 0.5
# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')
# Add constant parameters
parameters$reward_space_ub = reward_space_ub
```

## Pedlr {.tabset}

```{r}
# Model
model = 'Pedlr'

# Saving simulation
# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_1_pedlr.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias_correct = Model_bias_correct(n_subjects = n_subjects,
                                  set_seed = set_seed,
                                  n_blocks = n_blocks,
                                  perc_forced = perc_forced,
                                  blocks_per_task = blocks_per_task,
                                  dist_list = dist_list,
                                  model = model,
                                  parameters = parameters,
                                  init_values = init_values,
                                  shrink_distance_vec = shrink_distance_vec,
                                  save_data = save_data,
                                  save_file = save_file,
                                  load_data = load_data,
                                  load_file = load_file)
```

### Errors

```{r}
# Plot data
data_plot = data.table(bias_correct$data_bias_correct) %>%
  # Adjust column values
  .[, ':='(alpha1 = round(alpha1, 3),
           perc_correct = perc_correct * 100) ] %>%
  # Get mean and sd of percentage of correct answers for each parameter
  # combination
  .[, keyby = .(task_version,
                comp,
                alpha0,
                alpha1,
                temperature,
                choice_policy,
                reward_space_ub,
                distance_shrinkage),
    .(mean_perc_correct = mean(perc_correct),
      sd_perc_correct = sd(perc_correct))]

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=alpha1, group=alpha1)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Influence of alpha1 on correct choices')
```

Increasing alpha_1 produces more errors on the side of the distribution with rare events.

### SD of choice correctness

```{r}
ggplot(data=data_plot, aes(x=comp, y=sd_perc_correct, color=alpha1, group=alpha1)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Influence of alpha1 on SD of correct choices')
```
Standard deviation of percentage of correct choices rises with increasing alpha_1.

### Balance


'Balance' describes the relationship in errors between the comparisons of edge distributions to the middle distribution.

```{r}
data_balance = data.table(bias_correct$data_bias_correct) %>%
  .[, ':='(comp = paste('perc_correct_', comp, sep = ''))] %>%
  # Get one perc_correct column for each comparison type
  data.table::dcast(sub_id +
                      task_version + 
                      alpha0 + 
                      alpha1 + 
                      temperature + 
                      choice_policy + 
                      reward_space_ub + 
                      distance_shrinkage ~ comp,
                    value.var = 'perc_correct',
                    sep = '_') %>%
  # Get balance by difference between % of correct choices when comparing edge distirbutions to middle
  .[, ':='(balance = perc_correct_2v3 - perc_correct_1v2)] %>%
  # Get mean and sd over simulated participants
  .[, keyby = .(task_version,
                       alpha0,
                       alpha1,
                       temperature,
                       choice_policy,
                       reward_space_ub,
                       distance_shrinkage),
    .(mean_balance = mean(balance),
      sem_balance = sd(balance) / sqrt(length(balance)))]

ggplot(data=data_balance, aes(x=alpha1, y=mean_balance)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_line() +
  geom_ribbon(aes(ymin = mean_balance - sem_balance, ymax = mean_balance + sem_balance),
              alpha=0.2) +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Balance of correct choices')
```

Increasing alpha_1 creates negative balance in gain case and strengthens 
positive balance in loss case (as intended).

## Pedlr with interdependency {.tabset}

```{r}
# Model
model = 'Pedlr_interdep'

# Saving simulation
# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_1_pedlr_interdep.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias_correct = Model_bias_correct(n_subjects = n_subjects,
                                  set_seed = set_seed,
                                  n_blocks = n_blocks,
                                  perc_forced = perc_forced,
                                  blocks_per_task = blocks_per_task,
                                  dist_list = dist_list,
                                  model = model,
                                  parameters = parameters,
                                  init_values = init_values,
                                  shrink_distance_vec = shrink_distance_vec,
                                  save_data = save_data,
                                  save_file = save_file,
                                  load_data = load_data,
                                  load_file = load_file)

```

### Errors

```{r}
# Plot data
data_plot = data.table(bias_correct$data_bias_correct) %>%
  # Adjust column values
  .[, ':='(alpha1 = round(alpha1, 3),
           perc_correct = perc_correct * 100) ] %>%
  # Get mean and sd of percentage of correct answers for each parameter
  # combination
  .[, keyby = .(task_version,
                comp,
                alpha0,
                alpha1,
                temperature,
                choice_policy,
                reward_space_ub,
                distance_shrinkage),
    .(mean_perc_correct = mean(perc_correct),
      sd_perc_correct = sd(perc_correct))]

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=alpha1, group=alpha1)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = paste('Influence of alpha1 on correct choices', ' (interdep = ', as.character(steps_interdep), ')', sep = ''))
```

### SD of choice correctness

```{r}
ggplot(data=data_plot, aes(x=comp, y=sd_perc_correct, color=alpha1, group=alpha1)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = paste('Influence of alpha1 on SD of correct choices', ' (interdep = ', as.character(steps_interdep), ')', sep = ''))
```

### Balance

'Balance' describes the relationship in errors between the comparisons of edge distributions to the middle distribution.

```{r}
data_balance = data.table(bias_correct$data_bias_correct) %>%
  .[, ':='(comp = paste('perc_correct_', comp, sep = ''))] %>%
  # Get one perc_correct column for each comparison type
  data.table::dcast(sub_id +
                      task_version + 
                      alpha0 + 
                      alpha1 + 
                      temperature + 
                      choice_policy + 
                      reward_space_ub + 
                      distance_shrinkage ~ comp,
                    value.var = 'perc_correct',
                    sep = '_') %>%
  # Get balance by difference between % of correct choices when comparing edge distirbutions to middle
  .[, ':='(balance = perc_correct_2v3 - perc_correct_1v2)] %>%
  # Get mean and sd over simulated participants
  .[, keyby = .(task_version,
                       alpha0,
                       alpha1,
                       temperature,
                       choice_policy,
                       reward_space_ub,
                       distance_shrinkage),
    .(mean_balance = mean(balance),
      sem_balance = sd(balance) / sqrt(length(balance)))]

ggplot(data=data_balance, aes(x=alpha1, y=mean_balance)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_line() +
  geom_ribbon(aes(ymin = mean_balance - sem_balance, ymax = mean_balance + sem_balance),
              alpha=0.2) +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = paste('Balance of correct choices', ' (interdep = ', as.character(steps_interdep), ')', sep = ''))

```

---

# 2. Moving means of distributions closer to center causes more errors

- Set parameters for this simulation

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = 0.7
steps_temperature = 7
steps_policy = c('softmax')
steps_interdep = 0.5
# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')
# Add constant parameters
parameters$reward_space_ub = reward_space_ub

# Add increments of pushing distributions closer together
shrink_distance_vec = round(seq(0,100/6), 2)
```

## Pedlr {.tabset}

```{r}
# Model
model = 'Pedlr'

# Saving simulation
# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_2_pedlr.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias_correct = Model_bias_correct(n_subjects = n_subjects,
                                  set_seed = set_seed,
                                  n_blocks = n_blocks,
                                  perc_forced = perc_forced,
                                  blocks_per_task = blocks_per_task,
                                  dist_list = dist_list,
                                  model = model,
                                  parameters = parameters,
                                  init_values = init_values,
                                  shrink_distance_vec = shrink_distance_vec,
                                  save_data = save_data,
                                  save_file = save_file,
                                  load_data = load_data,
                                  load_file = load_file)
```

### Error

```{r}
# Plot data
data_plot = data.table(bias_correct$data_bias_correct) %>%
  # Adjust column values
  .[, ':='(alpha1 = round(alpha1, 3),
           perc_correct = perc_correct * 100) ] %>%
  # Get mean and sd of percentage of correct answers for each parameter
  # combination
  .[, keyby = .(task_version,
                comp,
                alpha0,
                alpha1,
                temperature,
                choice_policy,
                reward_space_ub,
                distance_shrinkage),
    .(mean_perc_correct = mean(perc_correct),
      sd_perc_correct = sd(perc_correct))]

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=distance_shrinkage, group=distance_shrinkage)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Influence of alpha1 on correct choices')
```

The closer we move the distributions together...

2. ...the more errors we get in general
3. ...even the easiest choice (1v3) gets harder


### Balance

```{r}
data_balance = data.table(bias_correct$data_bias_correct) %>%
  .[, ':='(comp = paste('perc_correct_', comp, sep = ''))] %>%
  # Get one perc_correct column for each comparison type
  data.table::dcast(sub_id +
                      task_version + 
                      alpha0 + 
                      alpha1 + 
                      temperature + 
                      choice_policy + 
                      reward_space_ub + 
                      distance_shrinkage ~ comp,
                    value.var = 'perc_correct',
                    sep = '_') %>%
  # Get balance by difference between % of correct choices when comparing edge distirbutions to middle
  .[, ':='(balance = perc_correct_2v3 - perc_correct_1v2)] %>%
  # Get mean and sd over simulated participants
  .[, keyby = .(task_version,
                       alpha0,
                       alpha1,
                       temperature,
                       choice_policy,
                       reward_space_ub,
                       distance_shrinkage),
    .(mean_balance = mean(balance),
      sem_balance = sd(balance) / sqrt(length(balance)))]

ggplot(data=data_balance, aes(x=distance_shrinkage, y=mean_balance)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_line() +
  geom_ribbon(aes(ymin = mean_balance - sem_balance, ymax = mean_balance + sem_balance),
              alpha=0.2) +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = 'Balance of correct choices')
```

Balance in the gain case starts below 0 since `alpha1 = 0.7`.

Why is moving the means closer to the center enhancing the sampling bias (balance gets more positive in gain case) instead of diminishing it (balance should get negative in gain case)?

Means this would work against us for the balance variable.

## Pedlr with interdependency

```{r}
# Model
model = 'Pedlr_interdep'

# Saving simulation
# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_2_pedlr_interdep.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias_correct = Model_bias_correct(n_subjects = n_subjects,
                                  set_seed = set_seed,
                                  n_blocks = n_blocks,
                                  perc_forced = perc_forced,
                                  blocks_per_task = blocks_per_task,
                                  dist_list = dist_list,
                                  model = model,
                                  parameters = parameters,
                                  init_values = init_values,
                                  shrink_distance_vec = shrink_distance_vec,
                                  save_data = save_data,
                                  save_file = save_file,
                                  load_data = load_data,
                                  load_file = load_file)
```

### Error

```{r}
# Plot data
data_plot = data.table(bias_correct$data_bias_correct) %>%
  # Adjust column values
  .[, ':='(alpha1 = round(alpha1, 3),
           perc_correct = perc_correct * 100) ] %>%
  # Get mean and sd of percentage of correct answers for each parameter
  # combination
  .[, keyby = .(task_version,
                comp,
                alpha0,
                alpha1,
                temperature,
                choice_policy,
                reward_space_ub,
                distance_shrinkage),
    .(mean_perc_correct = mean(perc_correct),
      sd_perc_correct = sd(perc_correct))]

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=distance_shrinkage, group=distance_shrinkage)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = paste('Influence of alpha1 on correct choices', ' (interdep = ', as.character(steps_interdep), ')', sep = ''))
```


### Balance

```{r}
data_balance = data.table(bias_correct$data_bias_correct) %>%
  .[, ':='(comp = paste('perc_correct_', comp, sep = ''))] %>%
  # Get one perc_correct column for each comparison type
  data.table::dcast(sub_id +
                      task_version + 
                      alpha0 + 
                      alpha1 + 
                      temperature + 
                      choice_policy + 
                      reward_space_ub + 
                      distance_shrinkage ~ comp,
                    value.var = 'perc_correct',
                    sep = '_') %>%
  # Get balance by difference between % of correct choices when comparing edge distirbutions to middle
  .[, ':='(balance = perc_correct_2v3 - perc_correct_1v2)] %>%
  # Get mean and sd over simulated participants
  .[, keyby = .(task_version,
                       alpha0,
                       alpha1,
                       temperature,
                       choice_policy,
                       reward_space_ub,
                       distance_shrinkage),
    .(mean_balance = mean(balance),
      sem_balance = sd(balance) / sqrt(length(balance)))]

ggplot(data=data_balance, aes(x=distance_shrinkage, y=mean_balance)) +
  geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_line() +
  geom_ribbon(aes(ymin = mean_balance - sem_balance, ymax = mean_balance + sem_balance),
              alpha=0.2) +
  facet_grid(alpha0 + temperature ~ task_version) +
  labs(title = paste('Balance of correct choices', ' (interdep = ', as.character(steps_interdep), ')', sep = ''))
```

---

# 3. Systematic mean-value bias gets stronger with alpha 1

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = round(seq(from = 0, to = 1, length.out = 20),2)
steps_temperature = 7
steps_policy = c('softmax')
steps_interdep = 0.5
# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')
# Add constant parameters
parameters$reward_space_ub = reward_space_ub

# Parameters for bias assessment
last_perc_of_trials = 35
average_subjects = TRUE
mean_value_calculation = 'chosen_trials'

# No increments of pushing distributions closer together
shrink_distance_vec = 0
```

## Pedlr {.tabset}

```{r}
# Set model for simulation
model = 'Pedlr'

# Parameters to save and loas data output
file = file.path(derivatives_path, 'central_prob_sim_3_pedlr.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias = Model_bias_value(n_subjects = n_subjects,
                        set_seed = set_seed,
                        n_blocks = n_blocks,
                        perc_forced = perc_forced,
                        blocks_per_task = blocks_per_task,
                        dist_list = dist_list,
                        model = model,
                        parameters = parameters,
                        init_values = init_values,
                        last_perc_of_trials = last_perc_of_trials,
                        average_subjects = average_subjects,
                        mean_value_calculation = mean_value_calculation,
                        save_data = save_data,
                        save_file = save_file,
                        load_data = load_data,
                        load_file = load_file)

bias$plot_softmax
```

The bias in the value estimate of the asymmetrical distribution (calculated over
the last 35% of trials and only over chosen trials) increases with alpha_1.

## Pedlr with interdependency

```{r}
# Set model for simulation
model = 'Pedlr_interdep'

# Parameters to save and loas data output
file = file.path(derivatives_path, 'central_prob_sim_3_pedlr_interdep.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Assess bias (if load data is true it will load .tsv of data_bias)
bias = Model_bias_value(n_subjects = n_subjects,
                        set_seed = set_seed,
                        n_blocks = n_blocks,
                        perc_forced = perc_forced,
                        blocks_per_task = blocks_per_task,
                        dist_list = dist_list,
                        model = model,
                        parameters = parameters,
                        init_values = init_values,
                        last_perc_of_trials = last_perc_of_trials,
                        average_subjects = average_subjects,
                        mean_value_calculation = mean_value_calculation,
                        save_data = save_data,
                        save_file = save_file,
                        load_data = load_data,
                        load_file = load_file)

bias$plot_softmax
```


---

# 4. Increasing distance between modes in bimodal distribution strengthens the effect

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = 0.7
steps_temperature = 7
steps_policy = 'softmax'
steps_interdep = 0.5

# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')

# Add constant parameters
parameters$reward_space_ub = reward_space_ub

# Parameters for bias assessment
shrink_distance_vec = 0
mode_distance_change_vec = seq(from = 0, to = 30, by = 5)
```

## Pedlr {.tabset}

```{r}
# Set model for simulation
model = 'Pedlr'

# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_4_pedlr.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

outcome = Free_bimodal_modes(n_subjects = n_subjects,
                             set_seed = set_seed,
                             n_blocks = n_blocks,
                             perc_forced = perc_forced,
                             blocks_per_task = blocks_per_task,
                             dist_list = dist_list,
                             model = model,
                             parameters = parameters,
                             init_values = init_values,
                             shrink_distance_vec = shrink_distance_vec,
                             save_data = save_data,
                             save_file = save_file,
                             load_data = load_data,
                             load_file = load_file,
                             mode_distance_change_vec = mode_distance_change_vec)
```

### Errors

```{r}
# Plot data
data_plot = data.table(outcome) %>%
  .[, perc_correct := perc_correct * 100] %>%
  .[, keyby = .(task_version,
                comp,
                alpha0,
                alpha1,
                temperature,
                choice_policy,
                reward_space_ub,
                distance_shrinkage,
                bimodal_distance_change),
    .(mean_perc_correct = mean(perc_correct),
      sd_perc_correct = sd(perc_correct))]

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=bimodal_distance_change, group=bimodal_distance_change)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ alpha1 + task_version) +
  labs(title = 'Influence of distance of modes on error rates')
```

## Pedlr with interdependency {.tabset}

```{r}
# Set model for simulation
model = 'Pedlr_interdep'

# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_4_pedlr_interdep.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

outcome = Free_bimodal_modes(n_subjects = n_subjects,
                             set_seed = set_seed,
                             n_blocks = n_blocks,
                             perc_forced = perc_forced,
                             blocks_per_task = blocks_per_task,
                             dist_list = dist_list,
                             model = model,
                             parameters = parameters,
                             init_values = init_values,
                             shrink_distance_vec = shrink_distance_vec,
                             save_data = save_data,
                             save_file = save_file,
                             load_data = load_data,
                             load_file = load_file,
                             mode_distance_change_vec = mode_distance_change_vec)
```

### Errors

```{r}
# Plot data
data_plot = data.table(outcome) %>%
  .[, perc_correct := perc_correct * 100] %>%
  .[, keyby = .(task_version,
                comp,
                alpha0,
                alpha1,
                temperature,
                choice_policy,
                reward_space_ub,
                distance_shrinkage,
                bimodal_distance_change),
    .(mean_perc_correct = mean(perc_correct),
      sd_perc_correct = sd(perc_correct))]

ggplot(data=data_plot, aes(x=comp, y=mean_perc_correct, color=bimodal_distance_change, group=bimodal_distance_change)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_grid(alpha0 + temperature ~ alpha1 + task_version) +
  labs(title = 'Influence of distance of modes on error rates')
```

---

# 5. Create loss paradigm where standard RW has balanced errors

```{r}
# Design
# Number of overall blocks
n_blocks = 3
# Number of blocks dedicated to each task version
blocks_per_task = 3
# Percentage of forced choice trials
perc_forced = 30
# List of different distributions with parameters (three distributions for each 
# task version)
dist_list = list(c('gaussian', gaussian_mid_mean, gaussian_sd),
                 c('bimodal',
                   bimodal_mean_loss,
                   bimodal_rel_proportion,
                   bimodal_distance_loss,
                   bimodal_sd,
                   bimodal_sd),
                 c('gaussian', gaussian_ue_mean, gaussian_sd))

# Simulate single design to display samples
sim = Create_design(n_blocks = n_blocks,
                    blocks_per_task = blocks_per_task,
                    perc_forced = perc_forced,
                    dist_list = dist_list)
# Get sampled rewards from both options
samples = data.table(option = c(sim$option_left, sim$option_right),
                     reward = c(sim$reward_stim_1, sim$reward_stim_2),
                     task_version = c(sim$task_version, sim$task_version))
# Plot samples for each option
ggplot(data = samples, aes(x = reward, fill = as.character(option))) +
  geom_histogram(binwidth = 1,
                 color = NA,
                 alpha = 0.8,
                 position = 'identity')
```

```{r}
# Steps in which to loop over each parameter
steps_alpha0 = 0.2
steps_alpha1 = c(0, 0.3, 0.7)
steps_temperature = 7
steps_policy = c('softmax')
steps_interdep = 0.5
# Get all combinations of parameters
parameters = data.table(expand.grid(steps_alpha0,
                                    steps_alpha1,
                                    steps_temperature,
                                    steps_policy,
                                    steps_interdep))
colnames(parameters) = c('alpha0',
                         'alpha1',
                         'temperature',
                         'choice_policy',
                         'interdep')
# Add constant parameters
parameters$reward_space_ub = reward_space_ub
shrink_distance_vec = c(0,5)
mode_distance_change_vec = c(-10, -5, 0, 5)
init_values = list(c(50,50,50))
```

Sampling with lowered mode distance

```{r}
Plot_hist_with_mode_change = function(dist_list,
                                      mode_distance_change){
  dist_list_hist = dist_list
  dist_list_hist[[2]][4] = (
    (abs(as.numeric(dist_list_hist[[2]][4])) + mode_distance_change) * 
      ((as.numeric(as.numeric(dist_list_hist[[2]][4]) > 0) * 2) - 1)
  )
  
  # Simulate single design to display samples
  sim = Create_design(n_blocks = n_blocks,
                      blocks_per_task = blocks_per_task,
                      perc_forced = perc_forced,
                      dist_list = dist_list_hist)
  # Get sampled rewards from both options
  samples = data.table(option = c(sim$option_left, sim$option_right),
                       reward = c(sim$reward_stim_1, sim$reward_stim_2),
                       task_version = c(sim$task_version, sim$task_version))
  # Plot samples for each option
  ggplot(data = samples, aes(x = reward, fill = as.character(option))) +
    geom_histogram(binwidth = 1,
                   color = NA,
                   alpha = 0.7,
                   position = 'identity') +
    labs(title = paste('Mode distance change: ', as.character(mode_distance_change), sep = ''))
}

Plot_hist_with_mode_change(dist_list = dist_list,
                           mode_distance_change = -10)
Plot_hist_with_mode_change(dist_list = dist_list,
                           mode_distance_change = -5)

```

```{r}
# Set model for simulation
model = 'Pedlr'

# Parameters to save and load data output
file = file.path(derivatives_path, 'central_prob_sim_5_pedlr.tsv')
save_data = FALSE
save_file = file
load_data = TRUE
load_file = file

# Asses accuracy to see if RW has equal amount of errors for lower and upper
# Gaussian under which circumstances
outcome = Free_bimodal_modes(n_subjects = n_subjects,
                             set_seed = set_seed,
                             n_blocks = n_blocks,
                             perc_forced = perc_forced,
                             blocks_per_task = blocks_per_task,
                             dist_list = dist_list,
                             model = model,
                             parameters = parameters,
                             init_values = init_values,
                             shrink_distance_vec = shrink_distance_vec,
                             save_data = save_data,
                             save_file = save_file,
                             load_data = load_data,
                             load_file = load_file,
                             mode_distance_change_vec = mode_distance_change_vec)
```


```{r}
# Plot data
data_plot = data.table(outcome) %>%
  .[, ':='(comp = paste('perc_correct_', comp, sep = ''))] %>%
  data.table::dcast(sub_id +
                      task_version +
                      alpha0 +
                      alpha1 +
                      temperature +
                      choice_policy +
                      reward_space_ub +
                      distance_shrinkage +
                      bimodal_distance_change ~ comp,
                    value.var = 'perc_correct',
                    sep = '_') %>%
  .[, ':='(balance = perc_correct_2v3 - perc_correct_1v2)] %>%
  .[, keyby = .(task_version,
                alpha0,
                alpha1,
                temperature,
                choice_policy,
                reward_space_ub,
                distance_shrinkage,
                bimodal_distance_change),
    .(mean_balance = mean(balance),
      sd_balance = sd(balance))]

ggplot(data=data_plot, aes(x=bimodal_distance_change, y=mean_balance, color=distance_shrinkage, group=distance_shrinkage)) +
  geom_line() +
  scale_color_viridis(option = 'D') +
  facet_wrap(~alpha1) +
  labs(title = 'Influence of alpha1 and correct choices')
```

Putting main mode and second mode of bimodal dist closer together balances false choices between both Gaussians in the loss case.
Higher alpha1 causes more errors on the intended side.
