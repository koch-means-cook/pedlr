---
title: "Posterior Predictive Checks"
format: 
  html:
    theme:
      light: yeti
      dark: superhero
    fontsize: small
    grid:
      sidebar-width: 15em
    code-fold: true
    toc: true
    toc-location: left
    toc-title: Contents
    toc-depth: 3
    toc-expand: true
    embed-resources: true
    html-math-method: katex
editor: source
---

# Setup

```{r}
library(here)
library(data.table)
library(magrittr)
library(car)
library(MASS)

# Get repo path
base_path = file.path(here::here(),
                      fsep = .Platform$file.sep)

# Source own functions
source_path = file.path(base_path, 'code', fsep = .Platform$file.sep)
source(file.path(source_path, 'utils', 'Load_model_fits_new.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'utils', 'Add_comp.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'utils', 'Get_running_avg.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'utils', 'Neurocodify_plot.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'utils', 'Prepare_data_for_plot.R', fsep = .Platform$file.sep))
source(file.path(source_path, 'utils', 'Get_plot_guides.R', fsep = .Platform$file.sep))

custom_guides = Get_plot_guides()


# # Load data
# files_pattern = file.path(base_path, 'derivatives', 'posterior_pred_checks', 'postpred-*_model-*.tsv',
#                           fsep = .Platform$file.sep)
# files = Sys.glob(files_pattern)
# data = data.table::data.table()
# for(file in files){
#   temp = data.table::fread(file, sep = '\t', na.strings = 'n/a')
#   data = rbind(data, temp)
# }

# Glob files based on naming pattern
files_pattern = file.path(base_path, 'derivatives', 'posterior_pred_checks', 'postpred-*_model-*.tsv',
                          fsep = .Platform$file.sep)
files = Sys.glob(files_pattern)

# Function to load text files (.tsv)
Load_tsv = function(file_path){
  tsv = data.table::fread(file_path,
                          sep = '\t',
                          na.strings = 'n/a')
  return(tsv)
}

# Get list of all text files using parallel processing
data_list = parallel::mclapply(X = files,
                               FUN = Load_tsv,
                               mc.cores = 4)
# Bind individual list entries (loaded text files) to single data frame
# (using optimized bind function by data.table package)
data = data.table::rbindlist(data_list)
```

# Checks

```{r}
# Load model fitting data to check if correct parameters were used for PPC
data_fitting = Load_model_fits_new()
fit_params = data_fitting %>%
  .[starting_values == 'random',] %>%
  .[variable == 'coefs',] %>%
  # Exclude betas weights based in z-scored predictors (could not be used in
  # data simulation for posterior pred checks)
  .[substr(x, 0, 2) != 'z_', ] %>%
  # Recode beta weight names
  .[x == '(Intercept)', names := 'b0'] %>%
  .[x == 'V1', names := 'b1'] %>%
  .[x == 'V2', names := 'b2'] %>%
  .[x == 'V1u', names := 'b3'] %>%
  .[x == 'V2u', names := 'b4'] %>%
  .[is.na(names), names := paste0('x', seq(.N)),
    by = c('participant_id', 'model')] %>%
  .[, c('participant_id', 'model', 'names', 'value')] %>%
  .[, (c('participant_id', 'model', 'names')) := lapply(.SD, factor), .SDcols = c('participant_id', 'model', 'names')] %>%
  data.table::setnames(old = 'value', new = 'value_fit')

sim_params = data %>%
  .[,.(x1 = unique(x1),
       x2 = unique(x2),
       x3 = unique(x3),
       x4 = unique(x4),
       b0 = unique(b0),
       b1 = unique(b1),
       b2 = unique(b2),
       b3 = unique(b3),
       b4 = unique(b4)),
    by = c('participant_id', 'model')] %>%
  data.table::melt(id.vars = c('participant_id', 'model'),
                   variable.name = 'names') %>%
  .[!is.na(value),] %>%
  data.table::setorder(participant_id, model, names) %>%
  data.table::setnames(old = 'value', new = 'value_sim')

check = data.table::merge.data.table(fit_params,
                                     sim_params,
                                     by = c('participant_id', 'model', 'names')) %>%
  .[, value_fit := round(value_fit, 5)] %>%
  .[, value_sim := round(value_sim, 5)] %>%
  .[, check := value_fit == value_sim]

fails = check[check != TRUE,]
```

# Choice comparison

```{r}
data_choicecomp = data
```

# Value comparison

```{r}
# Load modeldata (from model fitting)
files_pattern = file.path(base_path, 'derivatives', 'model_fitting', 'modeldata-*_sv-random.tsv',
                          fsep = .Platform$file.sep)
files = Sys.glob(files_pattern)

# Get list of all text files using parallel processing
data_list = parallel::mclapply(X = files,
                               FUN = Load_tsv,
                               mc.cores = 4)
# Bind individual list entries (loaded text files) to single data frame
# (using optimized bind function by data.table package)
modeldata_fit = data.table::rbindlist(data_list)

# key columns
cols = c('participant_id', 'model', 'run', 'trial', 'option_left', 
         'option_right', 'is_rare', 'reward_stim_1', 'reward_stim_2', 'trial_type',
         'forced', 'choice', 'option_choice', 'outcome',
         'value_low', 'value_mid', 'value_high')

data_real = modeldata_fit[, ..cols] %>%
  .[, datatype := 'participant']

data_pred = data %>%
  .[, choice := 'left'] %>%
  .[model_choice_side == 2, choice := 'right'] %>%
  .[, ':='(option_choice = model_choice_option,
           outcome = model_outcome,
           value_low = val_b_1,
           value_mid = val_b_2,
           value_high = val_b_3)] %>%
  .[, ..cols] %>%
  .[, datatype := 'prediction']

data_ppc = rbind(data_real, data_pred) %>%
  .[, datatype := factor(datatype)] %>%
  data.table::setorder(participant_id, model, trial, datatype)
```

---

# Choice correctness

```{r}
# Add group information to ppc
group_info = data_fitting %>%
  .[, .(group = unique(group)),
    by = 'participant_id']

# Percentage of optimal choices
check_noc = data[model %in% c('surprise', 'seplr')] %>%
  # Add group information to ppc data
  data.table::merge.data.table(., group_info, by = 'participant_id') %>%
  # Add bandit comparison to ppc data
  Add_comp(.) %>%
  # Add trial variable
  .[, trial := seq(.N),
    by = c('participant_id', 'run', 'model')] %>%
  # Select only free choices (no guided choices)
  .[trial_type == 'choice',] %>%
  # Set correct choice by selecting (on average) higher outcome bandit
  .[, correct_choice := if(option_left > option_right) 'left' else 'right',
    by = c('participant_id', 'run', 'model', 'trial')] %>%
  .[, correct := correct_choice == choice] %>%
  # Get percentage of correct choices (exclude timeouts from overall trials)
  .[, .(perc_correct = sum(as.numeric(correct), na.rm = TRUE) / length(which(!is.na(as.numeric(correct))))),
    by = c('participant_id', 'group', 'model', 'run', 'comp')] %>%
  # Set data types
  .[, c('participant_id', 'group', 'run', 'comp', 'model') := lapply(.SD, factor),
    .SDcols = c('participant_id', 'group', 'run', 'comp', 'model')]
```

## Surprise model

### Plot

```{r}
data_plot = Prepare_data_for_plot(check_noc[model == 'surprise'])
p_noc_diff_comp_line = ggplot(data = data_plot,
                              aes(x = comp,
                                  y = perc_correct,
                                  color = group,
                                  fill = group)) +
  geom_point(size = 0.2,
             position = position_jitterdodge(dodge.width = 0.3,
                                             jitter.width = 0.2,
                                             jitter.height = 0)) +
  geom_boxplot(width = 0.15,
               color = 'black',
               outlier.shape = NA,
               position = position_dodge(width = 0.3),
               show.legend = FALSE) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 1.5,
               stroke = 0.5,
               position = position_dodge(width = 0.3)) +
  scale_color_manual(values = custom_guides) +
  scale_fill_manual(values = custom_guides) +
  facet_grid( ~ run) +
  theme(strip.text.y = element_text(angle = 0),
        legend.position = 'none')
Neurocodify_plot(p_noc_diff_comp_line)
```

### Stats

```{r}
# Model percent of correct choices by group, run, and bandit comparison
lme_noc = lme4::lmer(data = check_noc[model == 'surprise'],
                     perc_correct ~ group * run * comp + (1 | participant_id))

# Display results
papeR::prettify(car::Anova(lme_noc))
```

## Valence model

### Plot

```{r}
data_plot = Prepare_data_for_plot(check_noc[model == 'seplr'])
p_noc_diff_comp_line = ggplot(data = data_plot,
                              aes(x = comp,
                                  y = perc_correct,
                                  color = group,
                                  fill = group)) +
  geom_point(size = 0.2,
             position = position_jitterdodge(dodge.width = 0.3,
                                             jitter.width = 0.2,
                                             jitter.height = 0)) +
  geom_boxplot(width = 0.15,
               color = 'black',
               outlier.shape = NA,
               position = position_dodge(width = 0.3),
               show.legend = FALSE) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 1.5,
               stroke = 0.5,
               position = position_dodge(width = 0.3)) +
  scale_color_manual(values = custom_guides) +
  scale_fill_manual(values = custom_guides) +
  facet_grid( ~ run) +
  theme(strip.text.y = element_text(angle = 0),
        legend.position = 'none')
Neurocodify_plot(p_noc_diff_comp_line)
```

### Stats

```{r}
# Model percent of correct choices by group, run, and bandit comparison
lme_noc = lme4::lmer(data = check_noc[model == 'seplr'],
                     perc_correct ~ group * run * comp + (1 | participant_id))

# Display results
papeR::prettify(car::Anova(lme_noc))
```

---

# CBE in predictions

```{r}
# Load windowrize data
file_pattern = file.path(base_path, 'derivatives', 'posterior_pred_checks', 
                         paste0('windowrizepred-*.tsv'),
                         fsep = .Platform$file.sep)
files = Sys.glob(file_pattern)
window_data_run = data.table::data.table()
for(file in files){
  temp = data.table::fread(file, sep = '\t', na.strings = 'n/a')
  window_data_run = rbind(window_data_run, temp)
}

# Add group information
groupinfo = modeldata_fit[, .(group = unique(group)),
                          by = c('participant_id')]
window_data_run = data.table::merge.data.table(window_data_run,
                                                       groupinfo)

# Get mean across runs
window_data_participant = window_data_run %>%
  .[, .(accuracy = mean(mean_accuracy, na.rm = TRUE)),
    by = c('participant_id', 'group', 'model', 'window_relative')]

# Get age group specific mean and sd
window_data_group = window_data_participant %>%
  .[, .(mean_accuracy = mean(accuracy, na.rm = TRUE),
        sem_accuracy = sd(accuracy, na.rm = TRUE)/sqrt(.N)),
    by = c('group', 'model', 'window_relative')] %>%
  .[order(rank(group), rank(model), rank(window_relative)),]
```

## CBE plot (all models)

```{r}
data_plot = Prepare_data_for_plot(window_data_participant[window_relative != 0,]) %>%
  .[, group := factor(group, levels = rev(levels(group)))]
data_plot_mean = Prepare_data_for_plot(window_data_group[window_relative != 0,]) %>%
  .[, group := factor(group, levels = rev(levels(group)))]

# Function to plot
Plot_windowrize = function(data_plot,
                           data_plot_mean,
                           plot_guides){
  
  data_plot = Prepare_data_for_plot(data_plot)
  data_plot_mean = Prepare_data_for_plot(data_plot_mean)
  
  p = ggplot(data = data_plot,
             aes(x = window_relative,
                 y = accuracy,
                 color = group)) +
    # Add data points of individual participant
    geom_point(position = position_jitterdodge(jitter.width = 0.2,
                                               jitter.height = 0,
                                               dodge.width = 0.5,
                                               seed = 666),
               size = 0.2,
               alpha = 0.5) +
    # Line connecting group means
    geom_line(data = data_plot_mean,
              aes(y = mean_accuracy),
              linewidth = 1,
              position = position_dodge(width = 0.5)) +
    # Errorbars (SEM) of group means
    geom_errorbar(data = data_plot_mean,
                  inherit.aes = FALSE,
                  aes(x = window_relative,
                      ymin = mean_accuracy - sem_accuracy,
                      ymax = mean_accuracy + sem_accuracy,
                      group = group),
                  color = 'black',
                  width = 0.2,
                  position = position_dodge(width = 0.5)) +
    # Group means
    geom_point(data = data_plot_mean,
               aes(y = mean_accuracy,
                   fill = group),
               shape = 21,
               size = 1,
               color = 'black',
               position = position_dodge(width = 0.5)) +
    # Scale group colors according to predefined guides
    scale_color_manual(values = plot_guides) +
    scale_fill_manual(values = plot_guides) +
    facet_wrap(~model)
  
  p = Neurocodify_plot(p) +
    theme(panel.spacing.x = unit(30, 'pt'),
          legend.position = 'top',
          panel.grid.major.x = element_blank(),
          panel.grid.minor.x = element_blank(),
          legend.title = element_blank())
  return(p)
}

# Plot
Plot_windowrize(data_plot = data_plot,
                data_plot_mean = data_plot_mean,
                plot_guides = plot_guides)
```

## CBE plot (PXP-relevant models)

```{r}
# Plot only for PXP-relevant models
Plot_windowrize(data_plot = data_plot[model %in% c('Valence', 'Surprise'),],
                data_plot_mean = data_plot_mean[model %in% c('Valence', 'Surprise'),],
                plot_guides = plot_guides)
```

### CBE analysis for PXP-relevant models

#### Surprise model

```{r}
data_cbe_surprise = window_data_run[model == 'surprise']

# LME for immediate before, and immediate after rare outcome
data_lme_surprise = data_cbe_surprise[window_relative %in% c('-1', '1')] %>%
  .[, group := as.factor(group)] %>%
  .[, participant_id := as.factor(participant_id)]

lme = lme4::lmer(data = data_lme_surprise,
                 mean_accuracy ~ window_relative * group * run + (1 | participant_id))

res = Anova(lme)
papeR::prettify(res)
```

#### Valence model

```{r}
data_cbe_valence = window_data_run[model == 'seplr']

# LME for immediate before, and immediate after rare outcome
data_lme_valence = data_cbe_valence[window_relative %in% c('-1', '1')] %>%
  .[, group := as.factor(group)] %>%
  .[, participant_id := as.factor(participant_id)]

lme = lme4::lmer(data = data_lme_valence,
                 mean_accuracy ~ window_relative * group * run + (1 | participant_id))

res = Anova(lme)
papeR::prettify(res)
```

## Plot simplified CBE for PXP-relevant models

```{r}
# Use only data pre and post rare outcome
data_cbe = window_data_participant[!window_relative %in% c(0,2),] %>%
  # Relabel to pre and post (from -1 and 1)
  .[,window_relative := factor(window_relative,
                               labels = c('pre', 'post'))] %>%
  # Cast into wide format to easier calculate difference between pre and post
  data.table::dcast(participant_id + group + model ~ window_relative, value.var = 'accuracy') %>%
  # Calculate strength of behavioral effect (larger values = stronger negative 
  # impact of rare outcomes on choices of mid bandit)
  .[, cbe := pre-post]

# Rename groups and set order (older adults first)
data_plot = Prepare_data_for_plot(data_cbe) %>%
  .[, group := factor(group, levels = rev(levels(group)))] %>%
  .[model %in% c('Valence', 'Surprise'),]

p = ggplot(data = data_plot,
           aes(x = group,
               y = cbe,
               color = group,
               fill = group)) +
  geom_hline(yintercept = 0,
             color = 'black',
             linewidth = 0.5) +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0),
             size = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = 0.3,
               alpha = 0.5) +
  stat_summary(fun = 'mean',
               geom = 'line',
               linewidth = 0.5,
               color = 'grey',
               group = 'group') +
  stat_summary(fun = 'mean',
               geom = 'point',
               shape = 23,
               fill = 'white',
               color = 'black',
               stroke = 1,
               size = 2) +
  scale_color_manual(values = plot_guides) +
  scale_fill_manual(values = plot_guides) +
  facet_wrap(~model)

p = Neurocodify_plot(p) +
  theme(legend.position = 'none',
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.spacing = unit(30, 'pt'),
        axis.title.x = element_blank())
p
```

## Stats: Different age trajectories for PXP-relevant models

```{r}
data_lme = data_cbe[model %in% c('surprise', 'seplr')] %>%
  .[, c('participant_id', 'group', 'model') := lapply(.SD, factor),
    .SDcols = c('participant_id', 'group', 'model')]
lme = lme4::lmer(data = data_lme,
                 cbe ~ group * model + (1|participant_id))
res = car::Anova(lme)
papeR::prettify(res)
```

### Interaction: Group X Model

```{r}
em = emmeans::emmeans(lme,
                      pairwise ~ group | model)
em
```

### Statistical comparison: Predictions from which candidate model are closer to real behavioral data?

```{r}
# Load real behavioral data
file = file.path(base_path, 'derivatives', 'analysis', 'windowrize.tsv',
                 fsep = .Platform$file.sep)
window_real = data.table::fread(file, sep = '\t', na.strings = 'n/a')

# Summarize data
window_real_run = window_real %>%
  # Get mean accuracy across all relative window positions (-2 to +3)
  .[, .(mean_accuracy = mean(correct_choice, na.rm = TRUE),
        n_data = sum(!is.na(correct_choice))),
    by = c('participant_id', 'group', 'run', 'window_relative')]

# Get mean across runs
window_real_participant = window_real_run %>%
  .[, .(accuracy = mean(mean_accuracy, na.rm = TRUE)),
    by = c('participant_id', 'group', 'window_relative')] %>%
  # add faux model grouping for easier data table merge
  .[, model := 'real_data']

# Same summary for predicted data
window_model_participant = window_data_run %>%
  .[, .(accuracy = mean(mean_accuracy, na.rm = TRUE)),
    by = c('participant_id', 'group', 'model', 'window_relative')] %>%
  # Restrict model to pxp-relevant models
  .[model %in% c('seplr', 'surprise')]

# Merge real and model data
data_cbe_comp = rbind(window_real_participant, window_model_participant) %>%
  # restrict data to pre/post
  .[window_relative %in% c(-1, 1)] %>%
  # Convert id vars to factors
  .[, c('participant_id', 'group', 'model') := lapply(.SD, factor),
    .SDcols = c('participant_id', 'group', 'model')] %>%
  # rename relative window to pre and post for wide format
  .[, window_relative := as.character(window_relative)] %>%
  .[window_relative == -1, window_relative := 'pre'] %>%
  .[window_relative == 1, window_relative := 'post'] %>%
  # wide format for pre and post accuracy
  data.table::dcast(participant_id + group + model ~ window_relative,
                    value.var = 'accuracy') %>%
  # Get critical behavioral effect (pre minus post)
  .[, cbe := pre - post] %>%
  # wide format for cbe
  data.table::dcast(participant_id + group ~ paste0('cbe_', model), value.var = 'cbe')

# CBE for all types (surprise, seplr, real data)
data_cbe_comp_all = data_cbe_comp %>%
  data.table::melt(id.vars = c('participant_id', 'group'))

data_cbe_comp_diff = data_cbe_comp %>%
  # Get differences between models and real data
  .[, ':='(abs_REALmS = abs(cbe_real_data - cbe_surprise),
           abs_REALmSEPLR = abs(cbe_real_data - cbe_seplr))] %>%
  data.table::melt(id.vars = c('participant_id', 'group'),
                   measure.vars = c('abs_REALmS', 'abs_REALmSEPLR'))
```

```{r}
p = ggplot(data = Prepare_data_for_plot(data_cbe_comp_all),
           aes(x = group,
               y = value)) +
  geom_hline(yintercept = 0,
             linewidth = 0.5) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(position=position_jitter(width = 0.1, height = 0, seed = 666)) +
  stat_summary(fun = mean, geom = 'path', group = 'variable', color = 'grey', linewidth = 1) +
  stat_summary(fun = mean, geom = 'point', color = 'black', fill = 'white', shape = 23, stroke = 1.5, size = 3) +
  facet_wrap(~variable)
p = Neurocodify_plot(p) +
  theme(panel.grid = element_blank())
p
```

```{r}
ggplot(data = data_cbe_comp_all[group == 'older'], 
       aes(x = variable,
           y = value)) +
  geom_point()
```


```{r}
p = ggplot(data = data_cbe_comp_diff,
           aes(x = variable,
               y = value)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(position=position_jitter(width = 0.1, height = 0, seed = 666)) +
  stat_summary(fun = mean, geom = 'point', color = 'black', fill = 'white', shape = 23, stroke = 1.5, size = 3) +
  facet_wrap(~group)
Neurocodify_plot(p)
```

