---
title: "Results 02"
output:
  html_document:
    toc: yes
    self_contained: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(data.table)
library(here)
library(magrittr)
library(ggplot2)
library(viridis)
library(binhf)
library(pwr)
library(knitr)
library(kableExtra)
library(sdamr)
library(gghalves)
library(lme4)
library(emmeans)
library(papeR)
library(ggridges)
```

```{r, message=FALSE}
# Get directory of repository
base_path = here::here()

# Load pre-written functions
source_path = file.path(base_path, 'code', 'utils',
                        fsep = .Platform$file.sep)
source_files = list.files(source_path, pattern = "[.][rR]$",
                          full.names = TRUE, recursive = TRUE)
invisible(lapply(source_files, function(x) source(x)))

# Load model_fitting functions
source_path = file.path(base_path, 'code', 'model_fitting',
                        fsep = .Platform$file.sep)
source_files = list.files(source_path, pattern = "[.][rR]$",
                          full.names = TRUE, recursive = TRUE)
source_files = source_files[basename(source_files) != 'Fit_model_wrapper.R' &
                              basename(source_files) != 'Fuse_fitting_outputs.R']
invisible(lapply(source_files, function(x) source(x)))

# Get plot colors/linetypes
custom_guides = Get_plot_guides()
```

```{r}
data = Load_model_fits() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  .[, ':='(participant_id = as.factor(participant_id),
           group = as.factor(group),
           para = as.factor(para),
           model = as.factor(model))] %>%
  .[, real_iter := seq(.N),
      by = c('participant_id',
             'group',
             'model',
             'para')]
# Sort model levels by number of parameters
data$model = factor(data$model, levels = c('Rw',
                                           'Pedlr_simple',
                                           'Pedlr_simple_const',
                                           'Pedlr', 'Pedlr_step',
                                           'Pedlr_fixdep',
                                           'Pedlr_interdep'))

# get number of participants (to adjust figure height)
n_participants = length(unique(data$participant_id))
```

# Model comparison

```{r}
data_aic = data %>%
  .[,n_para := .N,
    by = c('participant_id', 'group', 'real_iter', 'model')] %>%
  data.table::dcast(.,
                    participant_id + group + real_iter + second_ll + model + n_para ~ para,
                    value.var = 'second_solution') %>%
  # Calculate AIC
  .[, AIC := (2*n_para) + (2*second_ll)]

# Get min aic model
data_best_fit = data_aic %>%
  # Reduce to core models (Rw and Pedlr_step)
  .[model %in% c('Rw', 'Pedlr_step')] %>%
  .[, best_fitting_model := model[which.min(AIC)],
    by = c('participant_id', 'group', 'real_iter')] %>%
  # Take only first iteration (no variability in fits)
  .[real_iter == 1,]

data_only_best_fit = data_best_fit[model == best_fitting_model,]
```

## Plot .{tabset}

### Rw & Pedlr_step

```{r}
data_plot = Prepare_data_for_plot(data_only_best_fit)
dodge_width = 0.2

p_fit = ggplot(data = data_plot,
       aes(x = model,
           color = group,
           fill = group)) +
  geom_bar(stat = 'count',
                 width = dodge_width,
                 position = position_dodge(width = dodge_width)) +
  scale_fill_manual(values = custom_guides) +
  theme(legend.position = 'top',
        axis.text.x = element_text(angle = -45, hjust = 0))
Neurocodify_plot(p_fit)
```

```{r}
# Count winning model
data_only_best_fit %>%
  .[, .(n_best_fit = .N),
    by = c('group', 'model')] %>%
  .[order(rank(group), rank(-n_best_fit)), ]
```

-   look at fit as function of behavioral results
    -   e.g. effect of rare outcome ==\> fit of pedlr

### Rw & Pedlr_step

```{r}
data_best_fit_all = data_aic %>%
  .[, best_fitting_model := model[which.min(AIC)],
    by = c('participant_id', 'group', 'real_iter')] %>%
  # Take only first iteration (no variability in fits)
  .[real_iter == 1,] %>%
  .[model == best_fitting_model,]
```


```{r}
data_plot = Prepare_data_for_plot(data_best_fit_all)
dodge_width = 0.2

p_fit = ggplot(data = data_plot,
       aes(x = model,
           color = group,
           fill = group)) +
  geom_bar(stat = 'count',
                 width = dodge_width,
                 position = position_dodge(width = dodge_width)) +
  scale_fill_manual(values = custom_guides) +
  theme(legend.position = 'top',
        axis.text.x = element_text(angle = -45, hjust = 0))
Neurocodify_plot(p_fit)
```

```{r}
# Count winning model
data_best_fit_all %>%
  .[, .(n_best_fit = .N),
    by = c('group', 'model')] %>%
  .[order(rank(group), rank(-n_best_fit)), ]
```


```{r}
data_pedlr_family = data_only_best_fit %>%
  .[model != 'Rw',] %>%
  data.table::melt(id.vars = c('participant_id', 'group', 'real_iter', 'second_ll', 'model', 'best_fitting_model', 'AIC'),
                   measure.vars = c('alpha1', 'temperature'),
                   variable.name = 'para')
```

```{r}
data_plot = Prepare_data_for_plot(data_pedlr_family)

dodge_width = 0.3

p_alpha1 = ggplot(data = data_plot[para == 'alpha1'],
       aes(x = group,
           y = log(value),
           color = group,
           fill = group)) +
  geom_point(position = position_jitterdodge(dodge.width = dodge_width,
                                             jitter.width = dodge_width,
                                             jitter.height = 0,
                                             seed = 666)) +
  geom_boxplot(width = dodge_width/2,
               outlier.shape = NA,
               color = 'black',
               position = position_dodge(width = dodge_width)) +
  gghalves::geom_half_violin(position = position_nudge(-dodge_width),
                             alpha = 0.5,
                             color = NA) +
  scale_color_manual(values = custom_guides) +
  scale_fill_manual(values = custom_guides)

Neurocodify_plot(p_alpha1)
```

-   Could maybe move model estimation to log space (so in the model its not a1 \* PE BUT e\^a1 \* PE) so 0.00004 and 0.0000000007 could actually be distinguished

```{r}
data_plot = Prepare_data_for_plot(data_pedlr_family)

dodge_width = 0.3

p_alpha1 = ggplot(data = data_plot[para == 'alpha1'],
       aes(x = group,
           y = value,
           color = group,
           fill = group)) +
  geom_point(position = position_jitterdodge(dodge.width = dodge_width,
                                             jitter.width = dodge_width,
                                             jitter.height = 0,
                                             seed = 666)) +
  geom_boxplot(width = dodge_width/2,
               outlier.shape = NA,
               color = 'black',
               position = position_dodge(width = dodge_width)) +
  gghalves::geom_half_violin(position = position_nudge(-dodge_width),
                             alpha = 0.5,
                             color = NA) +
  scale_color_manual(values = custom_guides) +
  scale_fill_manual(values = custom_guides)

Neurocodify_plot(p_alpha1)
```

## Sanity checks

### Correlation alpha1 & rare-influence in participants best fit by Pedlr-family

```{r}
data_model = data_best_fit %>%
  .[model == 'Pedlr_step',] %>%
  data.table::melt(id.vars = c('participant_id', 'group', 'real_iter', 'second_ll', 'model', 'best_fitting_model', 'AIC'),
                   measure.vars = c('alpha1', 'temperature'),
                   variable.name = 'para') %>%
  data.table::dcast(participant_id + group + real_iter + second_ll + model + best_fitting_model + AIC ~ para, value.var = 'value')

# Get influence of rare value
data_rare_inf = Load_data() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  Add_comp(.) %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  # Get correct answers
   .[trial_type == 'choice', correct := if(option_left > option_right) 'left' else 'right',
    by = c('participant_id', 'run', 'trial')] %>%
  .[trial_type == 'forced', correct := forced,
    by = c('participant_id', 'run', 'trial')] %>%
  # Exclude time-out trials
  .[!is.na(outcome), correct_choice := choice == correct,
    by = c('participant_id', 'run', 'trial')] %>%
  .[comp != '1v2' | trial_type != 'choice', correct_choice := NA,
    by = c('participant_id', 'run')] %>%
  # Get running averages
  .[, ':='(avg_1_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 1),
           avg_2_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 2),
           avg_3_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 3)),
    by = c('participant_id', 'run')]
  

# Allocate data holding +-5 trials from rare outcome of bandit 2
window_data = data.table()

#rep(NA, max(-z+wdsz, 0))

# Function to get data slice +-5 trials from rare outcome of bandit 2
Windowrize = function(data,
                      index_rare,
                      window_size){
  result = data[seq(max(index_rare - window_size + 1, 1),
                    index_rare + window_size), ]
  result$window_center = data$trial[index_rare]
  result$window_relative = seq(max(index_rare - window_size + 1, 1),
                               index_rare + window_size) - index_rare
  result[window_relative == 0]$correct_choice = NA
  
  result$window_center = as.factor(result$window_center)
  result$window_relative = as.factor(result$window_relative)
  return(result)
}

# i_id = '09RI1ZH'
# i_run = '1'

# For each participant & run
for(i_id in unique(data_rare_inf$participant_id)){
  for(i_run in unique(data_rare_inf$run)){
    
    # Select data
    temp_data = data_rare_inf[participant_id == i_id &
                      run == i_run]
    
    # Get all trials where rare outcomes were obtained
    idx_chosen_rare_outcome = which(temp_data$is_rare == 1 &
                                      temp_data$trial_type %in% c('choice', 'forced') &
                                      temp_data$option_choice == 2)
   # For each of the rare-outcome trials
   for(rare_count in seq(1,length(idx_chosen_rare_outcome))){
    # Get data slice
     temp = Windowrize(data = temp_data,
                       index_rare = idx_chosen_rare_outcome[rare_count],
                       window_size = 3) %>%
       .[, window_relative := factor(window_relative, levels = unique(sort(window_relative)))]
     # Add count of the rare outcome
     temp$i_rare = rare_count
     # Fuse data for each participant & run
     window_data = rbind(window_data, temp)
   }
  }
}

# Summarize data
window_data_run = window_data %>%
  # Eliminate windows which extended across trial boundaries (<1 or >240)
  .[, relative_trial := as.numeric(as.character(window_center)) + as.numeric(as.character(window_relative))] %>%
  .[!(relative_trial < 1 | relative_trial > 240),] %>%
  # Sort by relative window
  .[order(rank(group), rank(participant_id), rank(run), rank(window_relative)),] %>%
  # Get mean accuracy across all relative window positions (-2 to +3)
  .[, .(mean_accuracy = mean(correct_choice, na.rm = TRUE),
        n_data = sum(!is.na(correct_choice))),
    by = c('participant_id', 'group', 'run', 'window_relative')] %>%
  # Get difference pre/post rare outcome
  .[window_relative %in% c(-1,1),] %>%
  .[window_relative == '-1', window_relative := 'pre'] %>%
  .[window_relative == '1', window_relative := 'post'] %>%
  data.table::dcast(participant_id + group + run ~ paste0('prob_', window_relative),
                    value.var = 'mean_accuracy') %>%
  .[, preminuspost := prob_pre - prob_post] %>%
  .[, run := as.factor(run)] %>%
  # Average across run (because modeling was done across runs)
  .[, .(preminuspost = mean(preminuspost)),
    by = c('participant_id', 'group')]

# Fuse with modelling data
data_sanity = data.table::merge.data.table(data_model, window_data_run,
                                           by = c('participant_id', 'group'))
```

#### Plot

```{r}
ggplot(data = data_sanity,
       aes(x = alpha1,
           y = preminuspost,
           color = group)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = 'lm')
```

#### Stats

```{r}
bla = lm(preminuspost ~ alpha1 * group,
         data = data_sanity)
summary(bla)
```

### Correlation alpha1 & bandit 2 estimates in participants best fit by Pedlr-family

```{r}
# State melt columns (to align data types to avoid warnings)
measure_cols = c('est_1_reward',
                 'est_1_range',
                 'avg_1_running',
                 'est_2_reward',
                 'est_2_range',
                 'avg_2_running',
                 'est_3_reward',
                 'est_3_range',
                 'avg_3_running')

# Get estimation data
check_est = Load_data() %>%
  Add_comp(.) %>%
  # Exclude: estimation specific
  Apply_exclusion_criteria(., choice_based_exclusion = FALSE) %>%
  # Get running average of chosen rewards
  .[, ':='(avg_1_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 1),
           avg_2_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 2),
           avg_3_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 3)),
    by = c('participant_id', 'run')] %>%
  .[, forced_rare := as.numeric(as.logical(is_rare) & trial_type == 'forced' & (comp == '1v2' | comp == '2v3'))] %>%
  .[!is.na(est_1_reward),] %>%
  .[, est_trial := seq(.N), by = c('participant_id', 'run')] %>%
  # Unify data types of measure columns
  .[, (measure_cols) := lapply(.SD, as.double), .SDcols = measure_cols] %>%
  data.table::melt(.,
                   id.vars = c('participant_id',
                               'group',
                               'run',
                               'est_trial',
                               'forced_rare'),
                   measure.vars = measure_cols) %>%
  .[, est_stim := substr(variable, 5, 5)] %>%
  .[, type := substr(variable, 7, 9)] %>%
  .[type == 'rew', type := 'reward'] %>%
  .[type == 'ran', type := 'range'] %>%
  .[type == 'run', type := 'r_avg'] %>%
  data.table::dcast(., participant_id + group + run + est_trial + forced_rare + est_stim ~ type,
                    value.var = 'value')

# Merge true means with estimation
check_est_diff = check_est %>%
  # Get difference between estimation and true mean
  .[, diff_from_true := reward - r_avg]

# Get mean estimation accuracy across estimation trials
check_mean_est_diff = check_est_diff %>%
  # Apply exclusion criterion for rating trials
  Apply_exclusion_criteria(choice_based_exclusion = FALSE) %>%
  .[, half := rep(x = c(1,2), each = (max(est_trial)/2)),
    by = c('participant_id', 'group', 'run', 'est_stim')] %>%
  .[, .(mean_diff_from_true = mean(diff_from_true, na.rm = TRUE)),
    by = c('participant_id', 'group', 'run', 'half', 'est_stim')] %>%
  .[, ':='(participant_id = factor(participant_id),
           group = factor(group),
           run = factor(run),
           half = factor(half),
           est_stim = factor(est_stim))] %>%
  # Get estimation of 2nd bandit
  .[est_stim == '2', ] %>%
  # Focus only on 2nd half
  .[half == '2', ] %>%
  # Rename estimation column
  .[, mean_diff_from_true_2 := mean_diff_from_true] %>%
  # average across runs since modelling was done across runs
  .[, .(mean_diff_from_true_2 = mean(mean_diff_from_true_2)),
    by = c('participant_id', 'group', 'half', 'est_stim')]
```

```{r}
data_sanity_rat = data_sanity

data_sanity_rat = data.table::merge.data.table(data_sanity_rat,
                                               check_mean_est_diff,
                                               by = c('participant_id', 'group'))
```

#### Plot

```{r}
ggplot(data = data_sanity_rat,
       aes(x = alpha1,
           y = mean_diff_from_true_2,
           color = group)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = 'lm')
```

#### Stats

```{r}
bla = lm(mean_diff_from_true_2 ~ alpha1 * group,
         data = data_sanity_rat)
summary(bla)
```

---

# Model predictions

```{r}
# Get behavioral data (choices and estimates)
data_experiment = Load_data()
```

```{r}
# Make sure data used excludes participants with dubious ratings
data_rat = data

# Allocate data table that will hold model values and indivisual estimate
data_est_values = data.table()

# For each participant
for(i_id in unique(data_rat$participant_id)){
  
  # Select model parameters (not run sensitive, because models were fit across runs)
  data_fit = data_rat[real_iter == 1 & participant_id == i_id]
    
  #  For each run
  for(i_run in unique(data_experiment[participant_id == i_id]$run)){
    
    # Select behavioral data (important for individual estimates)
    temp = data_experiment[participant_id == i_id & run == i_run]
    temp = Prepare_data_for_fit(temp)
    
    # Fit RW model to choices
    data_para = data_fit[model == 'Rw']
    Rw = Fit_Rw(data = temp,
                params.alpha = data_para[para == 'alpha']$second_solution,
                params.temperature = data_para[para == 'temperature']$second_solution,
                params.reward_space_ub = 100,
                choice_policy = 'softmax',
                init_values = c(50,50,50))
    
    # Fit Pedler simple model to choices
    data_para = data_fit[model == 'Pedlr_simple']
    Pedlr_simple = Fit_Pedlr_simple(data = temp,
                                    params.alpha1 = data_para[para == 'alpha1']$second_solution,
                                    params.temperature = data_para[para == 'temperature']$second_solution,
                                    params.reward_space_ub = 100,
                                    choice_policy = 'softmax',
                                    init_values = c(50,50,50))
    
    # Fit Pedlr_simple_const model to choices
    data_para = data_fit[model == 'Pedlr_simple_const']
    Pedlr_simple_const = Fit_Pedlr_simple_const(data = temp,
                                                params.alpha1 = data_para[para == 'alpha1']$second_solution,
                                                params.temperature = data_para[para == 'temperature']$second_solution,
                                                params.reward_space_ub = 100,
                                                choice_policy = 'softmax',
                                                init_values = c(50,50,50))
    
    # Fit pedlr model to choices
    data_para = data_fit[model == 'Pedlr']
    Pedlr = Fit_Pedlr(data = temp,
                      params.alpha0 = data_para[para == 'alpha0']$second_solution,
                      params.alpha1 = data_para[para == 'alpha1']$second_solution,
                      params.temperature = data_para[para == 'temperature']$second_solution,
                      params.reward_space_ub = 100,
                      choice_policy = 'softmax',
                      init_values = c(50,50,50))
    
    # Fit Pedlr_step model to choices
    data_para = data_fit[model == 'Pedlr_step']
    Pedlr_step = Fit_Pedlr_step(data = temp,
                                params.alpha0 = data_para[para == 'alpha0']$second_solution,
                                params.alpha1 = data_para[para == 'alpha1']$second_solution,
                                params.temperature = data_para[para == 'temperature']$second_solution,
                                params.reward_space_ub = 100,
                                choice_policy = 'softmax',
                                init_values = c(50,50,50),
                                # Set PE boundary by 95th percentile of absolute RW PEs
                                pe_boundary_abs = Rw$pe_boundary_abs)
    
    # Fit pedlr_fixdep model to choices
    data_para = data_fit[model == 'Pedlr_fixdep']
    Pedlr_fixdep = Fit_Pedlr_fixdep(data = temp,
                                    params.alpha0 = data_para[para == 'alpha0']$second_solution,
                                    params.alpha1 = data_para[para == 'alpha1']$second_solution,
                                    params.temperature = data_para[para == 'temperature']$second_solution,
                                    params.reward_space_ub = 100,
                                    choice_policy = 'softmax',
                                    init_values = c(50,50,50))
    
    # Fit pedlr_interdep model to choices
    data_para = data_fit[model == 'Pedlr_interdep']
    Pedlr_interdep = Fit_Pedlr_interdep(data = temp,
                                        params.alpha0 = data_para[para == 'alpha0']$second_solution,
                                        params.alpha1 = data_para[para == 'alpha1']$second_solution,
                                        params.interdep = data_para[para == 'interdep']$second_solution,
                                        params.temperature = data_para[para == 'temperature']$second_solution,
                                        params.reward_space_ub = 100,
                                        choice_policy = 'softmax',
                                        init_values = c(50,50,50))
    
    # Add model values and PE for each option as column
    # RW
    temp$Rw_1_reward = Rw$values$stim_1
    temp$Rw_2_reward = Rw$values$stim_2
    temp$Rw_3_reward = Rw$values$stim_3
    temp$Rw_1_pe = Rw$PE$stim_1
    temp$Rw_2_pe = Rw$PE$stim_2
    temp$Rw_3_pe = Rw$PE$stim_3
    temp$pe_95th_Rw = Rw$pe_boundary_abs
    # Pedlr_simple
    temp$Pedlrsimple_1_reward = Pedlr_simple$values$stim_1
    temp$Pedlrsimple_2_reward = Pedlr_simple$values$stim_2
    temp$Pedlrsimple_3_reward = Pedlr_simple$values$stim_3
    temp$Pedlrsimple_1_pe = Pedlr_simple$PE$stim_1
    temp$Pedlrsimple_2_pe = Pedlr_simple$PE$stim_2
    temp$Pedlrsimple_3_pe = Pedlr_simple$PE$stim_3
    # Pedlr_simple_const
    temp$Pedlrsimpleconst_1_reward = Pedlr_simple_const$values$stim_1
    temp$Pedlrsimpleconst_2_reward = Pedlr_simple_const$values$stim_2
    temp$Pedlrsimpleconst_3_reward = Pedlr_simple_const$values$stim_3
    temp$Pedlrsimpleconst_1_pe = Pedlr_simple_const$PE$stim_1
    temp$Pedlrsimpleconst_2_pe = Pedlr_simple_const$PE$stim_2
    temp$Pedlrsimpleconst_3_pe = Pedlr_simple_const$PE$stim_3
    # Pedlr
    temp$Pedlr_1_reward = Pedlr$values$stim_1
    temp$Pedlr_2_reward = Pedlr$values$stim_2
    temp$Pedlr_3_reward = Pedlr$values$stim_3
    temp$Pedlr_1_pe = Pedlr$PE$stim_1
    temp$Pedlr_2_pe = Pedlr$PE$stim_2
    temp$Pedlr_3_pe = Pedlr$PE$stim_3
    # Pedlr_step
    temp$Pedlrstep_1_reward = Pedlr_step$values$stim_1
    temp$Pedlrstep_2_reward = Pedlr_step$values$stim_2
    temp$Pedlrstep_3_reward = Pedlr_step$values$stim_3
    temp$Pedlrstep_1_pe = Pedlr_step$PE$stim_1
    temp$Pedlrstep_2_pe = Pedlr_step$PE$stim_2
    temp$Pedlrstep_3_pe = Pedlr_step$PE$stim_3
    # Pedlr_fixdep
    temp$Pedlrfixdep_1_reward = Pedlr_fixdep$values$stim_1
    temp$Pedlrfixdep_2_reward = Pedlr_fixdep$values$stim_2
    temp$Pedlrfixdep_3_reward = Pedlr_fixdep$values$stim_3
    temp$Pedlrfixdep_1_pe = Pedlr_fixdep$PE$stim_1
    temp$Pedlrfixdep_2_pe = Pedlr_fixdep$PE$stim_2
    temp$Pedlrfixdep_3_pe = Pedlr_fixdep$PE$stim_3
    # Pedlr_interdep
    temp$Pedlrinterdep_1_reward = Pedlr_interdep$values$stim_1
    temp$Pedlrinterdep_2_reward = Pedlr_interdep$values$stim_2
    temp$Pedlrinterdep_3_reward = Pedlr_interdep$values$stim_3
    temp$Pedlrinterdep_1_pe = Pedlr_interdep$PE$stim_1
    temp$Pedlrinterdep_2_pe = Pedlr_interdep$PE$stim_2
    temp$Pedlrinterdep_3_pe = Pedlr_interdep$PE$stim_3
    
    # Bind run and participant specific data together
    data_est_values = rbind(data_est_values, temp)
  }
}

```

```{r}
# Get rating columns
est_columns = colnames(data_est_values)[grep('*_reward', colnames(data_est_values))]

# Get data structure comparing participant estimate to model values
data_values_comp = data_est_values[with_rating == TRUE] %>%
  Apply_exclusion_criteria(choice_based_exclusion = FALSE) %>%
  # Numerate estimation trials
  .[, est_trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  # Melt estimates
  .[, (est_columns) := lapply(.SD, as.numeric), .SDcols = est_columns] %>%
  data.table::melt(measure.vars = est_columns) %>%
  .[, ':='(model = unlist(strsplit(as.character(variable), '_'))[1],
           option = unlist(strsplit(as.character(variable), '_'))[2]),
    by = c('participant_id', 'run', 'est_trial', 'variable')] %>%
  # Standardize model names
  .[model == 'Pedlrsimple', model := 'Pedlr_simple'] %>%
  .[model == 'Pedlrsimpleconst', model := 'Pedlr_simple_const'] %>%
  .[model == 'Pedlrstep', model := 'Pedlr_step'] %>%
  .[model == 'Pedlrfixdep', model := 'Pedlr_fixdep'] %>%
  .[model == 'Pedlrinterdep', model := 'Pedlr_interdep'] %>%
  # Factorize estimate variable
  .[, model := factor(model, levels = c('est',
                                        'Rw',
                                        'Pedlr_simple',
                                        'Pedlr_simple_const',
                                        'Pedlr',
                                        'Pedlr_step',
                                        'Pedlr_fixdep',
                                        'Pedlr_interdep'))] %>%
  # Sort data table
  .[order(rank(participant_id), rank(run), rank(est_trial), rank(option), rank(model))] %>%
  # Create column duplicating individual estimate (for easier difference calculation)
  .[, estimate := value[model == 'est'],
    by = c('participant_id', 'run', 'est_trial', 'option')]
```

```{r, fig.height=n_participants}
# Plot estimation tracks and model values for each participant
p = ggplot(data = data_values_comp,
       aes(x = est_trial,
           y = value,
           color = model)) +
  geom_path(alpha = 0.5) +
  scale_color_viridis(option = 'D', discrete = TRUE) +
  facet_grid(participant_id ~ run + option) +
  theme(legend.position = 'top')

  Neurocodify_plot(p)
```

## Comparison: Model predictions vs. participant's estimation

```{r}
data_sse = data_values_comp %>%
  # Exclude real estimation value from models (was in there for easier plotting)
  .[model != 'est',] %>%
  # Get squared error (model prediction vs. participants estimation) for each estimation trial & model
  .[, error_squared := (value - estimate)^2] %>%
  # Sum of squared errors across all rating trials (also across runs)
  .[, .(sse = sum(error_squared),
        rmse = sqrt(mean(error_squared))),
    by = c('participant_id', 'group', 'model', 'option')] %>%
  .[, option := as.factor(option)]
```

```{r}
data_plot = Prepare_data_for_plot(data_sse)

dodge_width = 0.5

p = ggplot(data = data_plot,
           aes(x = model,
               y = rmse,
               color = group,
               fill = group)) +
  geom_point(position = position_jitterdodge(dodge.width = dodge_width,
                                             jitter.width = dodge_width/2,
                                             jitter.height = 0,
                                             seed = 666)) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               position = position_dodge(width = dodge_width),
               width = dodge_width) +
  scale_color_manual(values = custom_guides) +
  scale_fill_manual(values = custom_guides) +
  facet_grid(~option) +
  theme(axis.text.x = element_text(angle = 90))
Neurocodify_plot(p)
```

```{r}
# Rw vs. Pedlr_simple
t.test(data_sse[model == 'Rw' & option == '2', rmse],
       data_sse[model == 'Pedlr_simple' & option == '2', rmse],
       paired = TRUE)

# Rw vs. Pedlr_simple_const
t.test(data_sse[model == 'Rw' & option == '2', rmse],
       data_sse[model == 'Pedlr_simple_const' & option == '2', rmse],
       paired = TRUE)

# Rw vs Pedlr
t.test(data_sse[model == 'Rw' & option == '2', rmse],
       data_sse[model == 'Pedlr' & option == '2', rmse],
       paired = TRUE)

# Rw vs Pedlr_step
t.test(data_sse[model == 'Rw' & option == '2', rmse],
       data_sse[model == 'Pedlr_step' & option == '2', rmse],
       paired = TRUE)

# Rw vs. Pedlr_fixdep
t.test(data_sse[model == 'Rw' & option == '2', rmse],
       data_sse[model == 'Pedlr_fixdep' & option == '2', rmse],
       paired = TRUE)

# Rw vs. Pedlr_interdep
t.test(data_sse[model == 'Rw' & option == '2', rmse],
       data_sse[model == 'Pedlr_interdep' & option == '2', rmse],
       paired = TRUE)

lme = lme4::lmer(rmse ~ group * model + (1 | participant_id),
                 data = data_sse[option == '2'])
Anova(lme)
```

```{r}
emmeans::emmeans(lme,
                 pairwise ~ model)
```

## Correlation: Model predictions vs. participant's estimation

```{r}
data_est_val_cor = data_values_comp %>%
  # Exclude real estimation value from models (was in there for easier plotting)
  .[model != 'est',] %>%
  # Get correlation between estimate and model value
  .[, .(cor = cor(value, estimate)),
    by = c('participant_id', 'group', 'run', 'model', 'option')] %>%
  .[, option := as.factor(option)]
```

```{r}
dodge_width = 0.5

p = ggplot(data = Prepare_data_for_plot(data_est_val_cor),
           aes(x = model,
               y = cor,
               color = group,
               fill = group)) +
  geom_point(position = position_jitterdodge(dodge.width = dodge_width,
                                             jitter.width = dodge_width/2,
                                             jitter.height = 0,
                                             seed = 666)) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               position = position_dodge(width = dodge_width),
               width = dodge_width) +
  scale_color_manual(values = custom_guides) +
  scale_fill_manual(values = custom_guides) +
  facet_grid(~option) +
  theme(axis.text.x = element_text(angle = 90))
Neurocodify_plot(p)
```

```{r}
# Rw vs. Pedlr_simple
t.test(data_est_val_cor[model == 'Rw' & option == '2', cor],
       data_est_val_cor[model == 'Pedlr_simple' & option == '2', cor],
       paired = TRUE)

# Rw vs. Pedlr_simple_const
t.test(data_est_val_cor[model == 'Rw' & option == '2', cor],
       data_est_val_cor[model == 'Pedlr_simple_const' & option == '2', cor],
       paired = TRUE)

# Rw vs Pedlr
t.test(data_est_val_cor[model == 'Rw' & option == '2', cor],
       data_est_val_cor[model == 'Pedlr' & option == '2', cor],
       paired = TRUE)

# Rw vs Pedlr_step
t.test(data_est_val_cor[model == 'Rw' & option == '2', cor],
       data_est_val_cor[model == 'Pedlr_step' & option == '2', cor],
       paired = TRUE)

# Rw vs. Pedlr_fixdep
t.test(data_est_val_cor[model == 'Rw' & option == '2', cor],
       data_est_val_cor[model == 'Pedlr_fixdep' & option == '2', cor],
       paired = TRUE)

# Rw vs. Pedlr_interdep
t.test(data_est_val_cor[model == 'Rw' & option == '2', cor],
       data_est_val_cor[model == 'Pedlr_interdep' & option == '2', cor],
       paired = TRUE)

lme = lme4::lmer(cor ~ group * model + (1 | participant_id),
                 data = data_est_val_cor[option == '2'])
Anova(lme)
```

```{r}
emmeans::emmeans(lme,
                 pairwise ~ model)
```


### Correlation against 0

```{r}
# Rw
t.test(data_est_val_cor[model == 'Rw' & option == '2', cor],
       mu = 0)

# Pedlr_simple
t.test(data_est_val_cor[model == 'Pedlr_simple' & option == '2', cor],
       mu = 0)

# Pedlr_simple_const
t.test(data_est_val_cor[model == 'Pedlr_simple_const' & option == '2', cor],
       mu = 0)

# Pedlr
t.test(data_est_val_cor[model == 'Pedlr' & option == '2', cor],
       mu = 0)

# Pedlr_step
t.test(data_est_val_cor[model == 'Pedlr_step' & option == '2', cor],
       mu = 0)

# Pedlr_fixdep
t.test(data_est_val_cor[model == 'Pedlr_fixdep' & option == '2', cor],
       mu = 0)

# Pedlr_interdep
t.test(data_est_val_cor[model == 'Pedlr_interdep' & option == '2', cor],
       mu = 0)
```


---

# Pedlr_step: Difference alpha0 and alpha1

```{r}
data_alpha_ratio = data_best_fit %>%
  # Take pedlr step of each participant, even if not best fit
  .[model == 'Pedlr_step',] %>%
  # Get ratio between alpha0 and alpha1
  .[, ratio_1t0 := alpha1 / alpha0] %>%
  # Stratify to overweighters and underweighters
  .[ratio_1t0 > 1, weight_high_pe := 'overweight'] %>%
  .[ratio_1t0 < 1, weight_high_pe := 'underweight'] %>%
  # Also look at difference
  .[, diff_1m0 := alpha1 - alpha0]
```

### alpha1

```{r}
dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_alpha_ratio),
       aes(x = group,
           y = alpha1,
           color = group,
           fill = group)) +
  geom_hline(yintercept = 0,
             size = 0.5) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

```{r}
t.test(data_alpha_ratio[group == 'younger']$alpha1,
       data_alpha_ratio[group == 'older']$alpha1)
```


### Ratio a1/a0

```{r}
dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_alpha_ratio),
       aes(x = group,
           y = ratio_1t0,
           color = group,
           fill = group)) +
  geom_hline(yintercept = 0,
             size = 0.5) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
  
```

```{r}
t.test(data_alpha_ratio[group == 'younger' & !is.infinite(ratio_1t0)]$ratio_1t0,
       data_alpha_ratio[group == 'older']$ratio_1t0)
```

### Diff alpha1 - alpha0

```{r}
dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_alpha_ratio),
       aes(x = group,
           y = diff_1m0,
           color = group,
           fill = group)) +
  geom_hline(yintercept = 0,
             size = 0.5) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

```{r}
t.test(data_alpha_ratio[group == 'younger']$diff_1m0,
       data_alpha_ratio[group == 'older']$diff_1m0)
```

### Temperature

```{r}
dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_alpha_ratio),
       aes(x = group,
           y = temperature,
           color = group,
           fill = group)) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

```{r}
t.test(data_alpha_ratio[group == 'younger']$temperature,
       data_alpha_ratio[group == 'older']$temperature)
```

```{r}
Softmax_choice <- function(value_1, value_2, temperature){
  
  # Probabilistic model choice with choice probabilities controlled by soft-max function
  # Softmax function depends on temperature parameter (sigmoid steepness)
  choice_prob_1 = (
    exp(value_1/temperature) / (exp(value_1/temperature) + exp(value_2/temperature))
  )
  
 return(choice_prob_1)
}


data = data.table()
template = data.table(val_1 = seq(-50, 50),
                  val_2 = 0)
for(temp in seq(1,30)){
  bla = template
  bla$temperature = temp
  data = rbind(data, bla)
}

data = data %>%
  .[, prob_1 := Softmax_choice(val_1,
                               val_2,
                               temperature),
    by = c('temperature')] %>%
  .[, temperature := as.factor(temperature)]

ggplot(data,
       aes(x = val_1 - val_2,
           y = prob_1,
           color = temperature,
           group = temperature)) +
  geom_line()

```


---

# Sanity checks in participants with different alpha0/alpha1 relationship

```{r}
data_sanity_split = data_sanity[best_fitting_model == 'Pedlr_step'] %>%
  data.table::merge.data.table(., data_alpha_ratio,
                               by = c('participant_id', 'group', 'real_iter', 'model',
                                      'AIC', 'second_ll', 'alpha1', 'temperature', 'best_fitting_model')) %>%
  .[,weight_high_pe := factor(weight_high_pe)]
```


```{r}
p = ggplot(data = Prepare_data_for_plot(data_sanity_split),
       aes(x = weight_high_pe,
           color = group,
           fill = group)) +
  geom_bar() +
  facet_wrap(~group) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides)
Neurocodify_plot(p)
```

### Sanity check: Overweighters = higher influence of rare events?

```{r}
dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_sanity_split),
       aes(x = weight_high_pe,
           y = preminuspost,
           color = weight_high_pe,
           fill = weight_high_pe)) +
  geom_hline(yintercept = 0,
             size = 0.5) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

```{r}
t.test(data_sanity_split[weight_high_pe == 'overweight']$preminuspost,
       data_sanity_split[weight_high_pe == 'underweight']$preminuspost)
```

```{r}
lm = lm(preminuspost ~ group * weight_high_pe,
                 data = data_sanity_split)
Anova(lm)
```

### Sanity check: Overweighters = stronger underestimation of 2nd bandit?

```{r}
data_sanity_rat_split = data_sanity_rat %>%
  data.table::merge.data.table(., data_alpha_ratio,
                               by = c('participant_id', 'group', 'real_iter', 'model',
                                      'AIC', 'second_ll', 'alpha1', 'temperature', 'best_fitting_model')) %>%
  .[ratio_1t0 > 1, weight_high_pe := 'overweight'] %>%
  .[ratio_1t0 < 0, weight_high_pe := 'underweight'] %>%
  .[,weight_high_pe := factor(weight_high_pe)]
```

```{r}
dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_sanity_rat_split),
       aes(x = weight_high_pe,
           y = mean_diff_from_true_2,
           color = weight_high_pe,
           fill = weight_high_pe)) +
  geom_hline(yintercept = 0,
             size = 0.5) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

```{r}
lm = lm(mean_diff_from_true_2 ~ group * weight_high_pe,
        data = data_sanity_rat_split)
Anova(lm)
```


---

# Distribution RW LRs

```{r}
data_rw_lr = data_best_fit %>%
  # Only look at data from participants best fitted by RW
  .[best_fitting_model == 'Rw' &
      model == 'Rw',]

dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_rw_lr),
       aes(x = group,
           y = alpha,
           color = group,
           fill = group)) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
Neurocodify_plot(p)

```

```{r}
data_rw_lr[order(rank(alpha))]
```


---

# RW: distribution of PE

```{r}
est_columns = colnames(data_est_values)[grep('*_pe', colnames(data_est_values))]

# Get data structure comparing participant estimate to model values
data_pe_comp = data_est_values %>%
  Apply_exclusion_criteria(choice_based_exclusion = TRUE) %>%
  # Numerate estimation trials
  .[, trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  # Get value of next trial for each option (Pedlr_step model)
  .[, nextTrial_Pedlrstep_1_value := binhf::shift(Pedlrstep_1_reward, -1),
    by = c('participant_id', 'run')] %>%
  .[, nextTrial_Pedlrstep_2_value := binhf::shift(Pedlrstep_2_reward, -1),
    by = c('participant_id', 'run')] %>%
  .[, nextTrial_Pedlrstep_3_value := binhf::shift(Pedlrstep_3_reward, -1),
    by = c('participant_id', 'run')] %>%
  # Get distance between bandit 1 and 2 after update
  .[, nextTrial_Pedlrstep_2m1_value := nextTrial_Pedlrstep_2_value - nextTrial_Pedlrstep_1_value] %>%
  # Delete last entry of next trial columns, because there is no update in the last trial
  .[trial == max(trial), ':='(nextTrial_Pedlrstep_1_value = NA,
                              nextTrial_Pedlrstep_2_value = NA,
                              nextTrial_Pedlrstep_3_value = NA,
                              nextTrial_Pedlrstep_2m1_value = NA)] %>%
  # Melt estimates
  .[, (est_columns) := lapply(.SD, as.numeric), .SDcols = est_columns] %>%
  data.table::melt(measure.vars = est_columns) %>%
  .[, ':='(model = unlist(strsplit(as.character(variable), '_'))[1],
           option = unlist(strsplit(as.character(variable), '_'))[2]),
    by = c('participant_id', 'run', 'trial', 'variable')] %>%
  # Standardize model names
  .[model == 'Pedlrsimple', model := 'Pedlr_simple'] %>%
  .[model == 'Pedlrsimpleconst', model := 'Pedlr_simple_const'] %>%
  .[model == 'Pedlrstep', model := 'Pedlr_step'] %>%
  .[model == 'Pedlrfixdep', model := 'Pedlr_fixdep'] %>%
  .[model == 'Pedlrinterdep', model := 'Pedlr_interdep'] %>%
  # Factorize estimate variable
  .[, model := factor(model, levels = c('Rw',
                                        'Pedlr_simple',
                                        'Pedlr_simple_const',
                                        'Pedlr',
                                        'Pedlr_step',
                                        'Pedlr_fixdep',
                                        'Pedlr_interdep'))] %>%
  # Factorize other important variables
  .[, ':='(participant_id = factor(participant_id),
           group = factor(group),
           run = factor(run))] %>%
  # Sort data table
  .[order(rank(participant_id), rank(run), rank(trial), rank(option), rank(model))]
```


```{r}
data_rw_pe = data_pe_comp %>%
  .[model == 'Rw',] %>%
  .[, .(perc_95 = quantile(abs(value), 0.95, na.rm = TRUE)),
    by = c('participant_id', 'group', 'run')]

dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_rw_pe),
       aes(x = group,
           y = perc_95,
           color = group,
           fill = group)) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

```{r, fig.height=15}
data_pe_dist = data_pe_comp %>%
  .[model == 'Rw',] %>%
  .[!is.na(value), ] %>%
  .[, pe_abs := abs(value)]

p = ggplot(data = Prepare_data_for_plot(data_pe_dist),
           aes(x = pe_abs,
               y = participant_id)) +
  ggridges::geom_density_ridges(stat = 'binline',
                                binwidth = 1,
                                color = 'black',
                                size = 0.3) +
  geom_point(data = Prepare_data_for_plot(data_rw_pe),
             aes(x = perc_95)) +
  facet_wrap(~run)
Neurocodify_plot(p)
```

# Average update given rare events

```{r}
data_update = data_pe_comp %>%
  # Cut down to trials in which Pedlr_step PE higher than "rare" boundary given by RW
  .[model == 'Pedlr_step' & option == 2,] %>%
  .[!is.na(value),] %>%
  .[, rare_pe := (abs(value) > pe_95th_Rw)] %>%
  # Rare given by individual RW PE
   .[rare_pe == TRUE, ] %>%
  # Rare given by design
  #.[is_rare == TRUE, ] %>%
  # Merge with table giving LRs
  data.table::merge.data.table(., data_best_fit[model == 'Pedlr_step'],
                               by = c('participant_id', 'group', 'model')) %>%
    # get over vs underweighter
  .[alpha1 > alpha0, weight_rare_pe := 'overweight'] %>%
  .[alpha1 < alpha0, weight_rare_pe := 'underweight'] %>%
  .[, weight_rare_pe := factor(weight_rare_pe)] %>%
  # Get trialwise absolute update, expressed as fraction of current value
  .[, abs_relative_update := (abs(value) * alpha1) / Pedlrstep_2_reward,
    by = c('participant_id', 'group', 'run', 'trial')]

data_avg_update = data_update %>%
  # Mean asolute update in case of rare PE
  .[, .(mean_abs_relative_update = mean(abs_relative_update),
        sd_abs_relative_update = sd(abs_relative_update),
        mean_nextTrial_Pedlrstep_2m1_value = mean(nextTrial_Pedlrstep_2m1_value)),
    by = c('participant_id', 'group', 'run', 'model', 'best_fitting_model', 'weight_rare_pe')]
```

### Modeled distance between 2 and 1 in next trial after rare outcome

```{r}
p = ggplot(data = Prepare_data_for_plot(data_avg_update),
           aes(x = group,
               y = mean_nextTrial_Pedlrstep_2m1_value,
               fill = group,
               color = group)) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
  #facet_grid(~weight_rare_pe)
Neurocodify_plot(p)
```

### Average update following rare outcomes

```{r}
dodge_width = 0.075
jitter_width = dodge_width/2

p = ggplot(data = Prepare_data_for_plot(data_avg_update),
       aes(x = group,
           y = mean_abs_relative_update,
           color = group,
           fill = group)) +
  geom_point(position = position_jitternudge(jitter.width = jitter_width,
                                             jitter.height = 0,
                                             nudge.x = dodge_width + jitter_width/2,
                                             seed = 666),
             alpha = 0.5) +
  geom_boxplot(outlier.shape = NA,
               color = 'black',
               width = dodge_width/2) +
  gghalves::geom_half_violin(color = NA,
                             position = position_nudge(x = -dodge_width)) +
  stat_summary(fun = 'mean',
               geom = 'point',
               na.rm = TRUE,
               shape = 23,
               fill = 'white',
               size = 3,
               stroke = 0.5) +
  scale_fill_manual(values = custom_guides) +
  scale_color_manual(values = custom_guides) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

### Correlation between alpha 1 and influence of rare events

```{r}
p = ggplot(data = data_sanity_split[ratio_1t0 < 50],
       aes(x = ratio_1t0,
           y = preminuspost,
           color = group)) +
  geom_hline(yintercept = 0,
             size = 0.5) +
  geom_point() +
  geom_smooth(method = 'lm',
              formular = y ~ x)
Neurocodify_plot(p)
```

```{r}
lm = lm(data = data_sanity_split[ratio_1t0 < 50],
        preminuspost ~ ratio_1t0 * group)
summary(lm)
```


---

# Nico

```{r}
data = Load_model_fits_nico() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  .[, ':='(participant_id = as.factor(participant_id),
           group = as.factor(group),
           run = as.factor(run),
           para = as.factor(para),
           model = as.factor(model))]

# Sort model levels by number of parameters
data$model = factor(data$model, levels = c('Rw', 'Pedlr_simple', 'Pedlr', 'Pedlr_fixdep'))
```

```{r}
data_aic = data %>%
  .[,n_para := .N,
    by = c('participant_id', 'group', 'run', 'iter', 'model')] %>%
  data.table::dcast(.,
                    participant_id + group + run + iter + ll + model + n_para ~ para,
                    value.var = 'solution') %>%
  # Calculate AIC
  .[, AIC := (2*n_para) + (2*ll)]

# Get min aic model
data_best_fit = data_aic %>%
  .[, .SD[which.min(AIC)],
    by = c('participant_id', 'group', 'run', 'iter')] %>%
  # Take only first iteration (no variability in fits)
  .[iter == 1,]
```

```{r}
data_plot = Prepare_data_for_plot(data_best_fit)
dodge_width = 0.2

p_fit_nico = ggplot(data = data_plot,
       aes(x = model,
           color = group,
           fill = group)) +
  geom_bar(stat = 'count',
                 width = dodge_width,
                 position = position_dodge(width = dodge_width)) +
  scale_fill_manual(values = custom_guides) +
  facet_wrap(~run)
Neurocodify_plot(p_fit_nico)
```

# Two main things to put in:

-   Relating behavioral results to model fits (alpha 1)

# Look at difference in behavioral predicitin between modles

-   how many trials get predicted for different choices between models
