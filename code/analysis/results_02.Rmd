---
title: "Results 02"
output:
  html_document:
    toc: yes
    embed-resources: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(data.table)
library(here)
library(magrittr)
library(ggplot2)
library(viridis)
library(binhf)
library(pwr)
library(knitr)
library(kableExtra)
library(sdamr)
library(gghalves)
library(lme4)
library(emmeans)
library(papeR)
library(ggridges)
library(bmsR)
library(fabricatr)
library(mlisi)
```

```{r, message=FALSE}
# Get directory of repository
base_path = here::here()

# Load pre-written functions
source_path = file.path(base_path, 'code', 'utils',
                        fsep = .Platform$file.sep)
source_files = list.files(source_path, pattern = "[.][rR]$",
                          full.names = TRUE, recursive = TRUE)
invisible(lapply(source_files, function(x) source(x)))

source_path = file.path(base_path, 'code', 'model_fitting', 'LRfunction.R',
                        fsep = .Platform$file.sep)
source(source_path)


# Get plot colors/linetypes for plots
custom_guides = Get_plot_guides()
```

# Setup

```{r}
# Load modelling results
data = Load_model_fits_new() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  .[, ':='(participant_id = as.factor(participant_id),
           group = as.factor(group),
           sex = as.factor(sex),
           starting_values = as.factor(starting_values))]
# Sort model levels by number of parameters
data$model = factor(data$model, levels = c('rw',
                                           'uncertainty',
                                           'seplr',
                                           'uncertainty_seplr',
                                           'surprise',
                                           'uncertainty_surprise'))

# Load choice data
data_behav = Load_data() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  Add_comp(.) %>%
  .[,run := as.factor(run)]

# Percentage of optimal choices (bandit 1v2)
check_noc = data_behav %>%
  Add_comp(.) %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  .[trial_type == 'choice',] %>%
  .[, correct_choice := if(option_left > option_right) 'left' else 'right',
    by = c('participant_id', 'run', 'trial')] %>%
  .[, correct := correct_choice == choice] %>%
  # Get percentage of correct choices for each bandit comparison (exclude timeouts from overall trials)
  .[, .(perc_correct = sum(as.numeric(correct), na.rm = TRUE) / length(which(!is.na(as.numeric(correct))))),
    by = c('participant_id', 'group', 'comp')] %>%
  .[comp == '1v2'] %>%
  data.table::dcast(participant_id + group ~ paste0('perc_correct_', as.character(comp)), value.var = 'perc_correct')

# Focus analysis on random starting values
data = data[starting_values == 'random']
```

# Descriptives

## Betas

```{r}
data_betas = data %>%
  .[variable == 'coefs',] %>%
  .[x %in% c('(Intercept)', 'V1', 'V2', 'V1u', 'V2u'), ] %>%
  .[, ('x') := lapply(.SD,
                      factor,
                      levels = c('(Intercept)', 'V1', 'V2', 'V1u', 'V2u'),
                      labels= c('b_0', 'b_1', 'b_2', 'b_3', 'b_4')),
    .SDcols = 'x']

table_betas_all = data_betas %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        n = .N),
    by = c('model', 'x')] %>%
  .[, group := 'all']
table_betas_group = data_betas %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        n = .N),
    by = c('model', 'group', 'x')]
table_betas = rbind(table_betas_all,table_betas_group) %>%
  data.table::setcolorder(c('model', 'x', 'group')) %>%
  data.table::setorder(model, x, group)

table_betas = Collapse_repeated_rows(x = table_betas,
                                     columns = c(1,2),
                                     replace = '')
knitr::kable(table_betas, "html", align = 'l') %>%
  kableExtra::kable_paper(full_width = FALSE) %>%
  column_spec(1,
              bold = TRUE) %>%
  kableExtra::kable_styling()
```

```{r}
data_plot = data_betas

p_3betas = ggplot(data = data_plot[model %in% c('rw', 'surprise', 'seplr')],
           aes(x = x,
               y = value,
               fill = model)) +
  geom_hline(yintercept = 0,
             linewidth = 0.1) +
  geom_point(alpha = 0.3,
             size = 0.5,
             position = sdamr::position_jitternudge(jitter.width = 0.05,
                                                    jitter.height = 0,
                                                    nudge.x = -0.1,
                                                    nudge.y = 0)) +
  geom_boxplot(width = 0.1,
               outlier.shape = NA) +
  gghalves::geom_half_violin(side = 'r',
                             color = NA,
                             alpha = 0.8,
                             position = position_nudge(x = 0.1,
                                                       y = 0)) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous(limits = c(min(data_plot$value), max(data_plot$value))) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'none')
p_3betas = Neurocodify_plot(p_3betas)

p_5betas = ggplot(data = data_plot[model %in% c('uncertainty', 'uncertainty_surprise', 'uncertainty_seplr')],
           aes(x = x,
               y = value,
               fill = model)) +
  geom_hline(yintercept = 0,
             linewidth = 0.1) +
  geom_point(alpha = 0.3,
             size = 0.5,
             position = sdamr::position_jitternudge(jitter.width = 0.05,
                                                    jitter.height = 0,
                                                    nudge.x = -0.1,
                                                    nudge.y = 0)) +
  geom_boxplot(width = 0.1,
               outlier.shape = NA) +
  gghalves::geom_half_violin(side = 'r',
                             color = NA,
                             alpha = 0.8,
                             position = position_nudge(x = 0.1,
                                                       y = 0)) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous(limits = c(min(data_plot$value), max(data_plot$value))) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'none')
p_5betas = Neurocodify_plot(p_5betas)

p = cowplot::plot_grid(p_3betas, p_5betas,
                       ncol = 2,
                       rel_widths = c(3,5),
                       axis = 'tb',
                       align = 'h')
p
```

# RW model: RL-like behavior

```{r}
# Get statistics of beta 1 and 2 
data_rw_beta = data_betas %>%
  .[model == 'rw',] %>%
  data.table::dcast(participant_id + group + model + AICc ~ x, value.var = 'value') %>%
  data.table::merge.data.table(., check_noc, by = c('participant_id', 'group'))
```

## Left bandit (beta_2)

```{r}
# Left bandit
# Younger
b_left_ya = t.test(data_rw_beta[group == 'younger']$b_2,
       mu = 0)
# Older
b_left_oa = t.test(data_rw_beta[group == 'older']$b_2,
       mu = 0)

b_left_ya
b_left_oa
```

## Right bandit (beta_1)

```{r}
# Right bandit
# Younger
b_right_ya = t.test(data_rw_beta[group == 'younger']$b_1,
       mu = 0)
# Older
b_right_oa = t.test(data_rw_beta[group == 'older']$b_1,
       mu = 0)

b_right_ya
b_right_oa
```

## Correlation of betas with correct choice in low-mid

```{r}
# Get correlation of betas with percentage correct (high betas mean less random behavior)
# Left bandit
c_b_left = cor.test(data_rw_beta$b_2,
                    data_rw_beta$perc_correct_1v2)

# Right bandit
c_b_right = cor.test(data_rw_beta$b_1,
                     data_rw_beta$perc_correct_1v2)

# Adjust for multiple testing
p_adjusted = p.adjust(c(c_b_left$p.value, c_b_right$p.value), method = 'holm')

c_b_left
c_b_right
p_adjusted
```

------------------------------------------------------------------------

# Winning model

```{r}
# Get AICc per model
data_model_comp = data %>%
  .[iter == 1] %>%
  .[, .(AICc = mean(AICc),
        sd_AICc = sd(AICc),
        n_params = sum(variable == 'coefs')),
    by = c('participant_id', 'group', 'model')]

# Summarize AICc across groups
data_model_comp_all = data_model_comp %>%
  .[, .(AICc = mean(AICc)),
    by = 'model'] %>%
  # Sort by lowest AIC
  .[order(rank(AICc))]

# And within groups
data_model_comp_age = data_model_comp %>%
  .[, .(AICc = mean(AICc)),
    by = c('group', 'model')] %>%
  # Sort by age and lowest AIC
  .[order(group, AICc)]

# Get winning model within each participant
data_counts = data_model_comp %>%
  data.table::melt(id.vars = c('participant_id', 'group', 'model'),
                   measure.vars = c('AICc')) %>%
  .[, ':='(lowest = min(value),
           loc_winning = value == min(value),
           # Name of winning model
           winning_model = model[value == min(value)]),
    by = c('participant_id', 'variable')] %>%
  # Only keep winning model
  .[loc_winning == TRUE]
```

**Across groups**

```{r}
# Count winning models across groups
data_counts_all = data_counts %>%
  .[, .(n_winning = .N),
    by = c('variable', 'model')]

table_counts_all = Collapse_repeated_rows(data_counts_all,
                                          columns = 1,
                                          replace = '')
knitr::kable(table_counts_all) %>%
  kableExtra::kable_styling()
```

```{r}
# Plot
p = ggplot(data = data_counts_all,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~variable, ncol = 2)
Neurocodify_plot(p)
```

**Within groups**

```{r}
# Count winning models within age-groups
data_counts_age = data_counts %>%
  .[, .(n_winning = .N),
    by = c('variable', 'group', 'model')] %>%
  .[order(group, -rank(model))]

table_counts_age = Collapse_repeated_rows(data_counts_age,
                                          columns = c(1,2),
                                          replace = '')

knitr::kable(table_counts_age) %>%
  kableExtra::kable_styling()
```

```{r}
# Plot
p = ggplot(data = data_counts_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(group~variable, ncol = 2)
Neurocodify_plot(p)
```

$\chi^2$-test of winning frequencies

```{r}
# Isolate winning model counts
count_win_oa = data_counts_age[group == 'older' & variable == 'AICc']$n_winning
count_win_ya = data_counts_age[group == 'younger' & variable == 'AICc']$n_winning
# Chi-Sq of frequencies (models X age groups)
cs_test = chisq.test(cbind(count_win_oa,
                           count_win_ya))

cs_test
```

------------------------------------------------------------------------

# Protected exceedance probability

```{r}
# Exceedance probs
data_ep = data_model_comp %>%
  .[, ':='(nAICc = -(AICc))] %>%
  data.table::dcast(participant_id + group ~ model, value.var = 'nAICc')

# Calculate exceedance probs using Bayes Model Selection in R
bmsR::VB_bms(cbind(data_ep$rw,
                   data_ep$uncertainty,
                   data_ep$seplr,
                   data_ep$uncertainty_seplr,
                   data_ep$surprise,
                   data_ep$uncertainty_surprise),
             n_samples = 100000)
```

------------------------------------------------------------------------

# Parametric AICc comparison

```{r}
# Relative AICc to RW model
data_model_comp_rw = data_model_comp %>%
  data.table::dcast(participant_id + group ~ paste0('AICc_', model),
                    value.var = 'AICc') %>%
  .[, ':='(AICc_SmRW = AICc_surprise - AICc_rw,
           AICc_SEPLRmRW = AICc_seplr - AICc_rw,
           AICc_UmRW = AICc_uncertainty - AICc_rw,
           AICc_USEPLRmRW = AICc_uncertainty_seplr - AICc_rw,
           AICc_USmRW = AICc_uncertainty_surprise - AICc_rw),
    by = c('participant_id', 'group')] %>%
  data.table::melt(id.vars = c('participant_id', 'group'),
                   measure.vars = c('AICc_SmRW',
                                    'AICc_SEPLRmRW',
                                    'AICc_UmRW',
                                    'AICc_USEPLRmRW',
                                    'AICc_USmRW'))

# Groups
test_model_comp_rw = data_model_comp_rw %>%
  .[, .(t = t.test(value, mu = 0)$statistic,
        df = t.test(value, mu = 0)$parameter,
        p = t.test(value, mu = 0)$p.value),
    by = c('group', 'variable')] %>%
  .[, p_adj := p.adjust(p, method = 'holm')] %>%
  .[order(group, variable)] %>%
  Prepare_data_for_plot(.)

table_test_model_comp_rw = test_model_comp_rw %>%
  Collapse_repeated_rows(.,
                         columns = 1,
                         replace = '')
knitr::kable(table_test_model_comp_rw) %>%
  kableExtra::kable_styling()
```

**Plot**

```{r}
data_model_comp_rw_mean = data_model_comp_rw %>%
  .[, .(value = mean(value),
        sd_value = sd(value),
        n = .N,
        sem_value = sd(value)/sqrt(.N)),
    by = 'variable']

p = ggplot(data = data_model_comp_rw,
           aes(x = variable,
               y = value,
               fill = variable)) +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_col(data = data_model_comp_rw_mean)
Neurocodify_plot(p)
```

------------------------------------------------------------------------

# Surprise model

```{r}
# Data for correlation between RW betas and performance
data_corr = data %>%
  .[variable == 'coefs' & model == 'surprise' & iter == 1] %>%
  data.table::dcast(participant_id + group + AIC ~ x, value.var = 'value') %>%
  data.table::merge.data.table(., check_noc, by = c('participant_id', 'group'))
```

**Correlation between betas and correct 1v2 choices**

```{r}
# V1 ~ performance
m_right = cor.test(data_corr$V1, data_corr$perc_correct_1v2)
m_right

# V2 ~ performance
m_left = cor.test(data_corr$V2, data_corr$perc_correct_1v2)
m_left


p.adjust(c(m_right$p.value,
           m_left$p.value),
         method = 'holm')
```

**Parameter details**

```{r}
# get coefficients of winning model
data_surprise = data[model == 'surprise' & variable == 'coefs']

data_param_overall = data_surprise %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        iqr = IQR(value)),
    by = c('x')] %>%
  .[, group := 'all']

data_param_group = data_surprise %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        iqr = IQR(value)),
    by = c('group', 'x')]

data_param_summary = rbind(data_param_overall,
                           data_param_group) %>%
  Prepare_data_for_plot(.)

knitr::kable(data_param_summary) %>%
  kableExtra::kable_styling()
```

**Correlation matrix**

```{r}
# Correlations between parameters
data_surprise[x == '(Intercept)']$x = 'intercept'
data_param_corr = data_surprise %>%
  data.table::dcast(participant_id + group + model + AIC + AICc ~ paste0('param_', x),
                    value.var = 'value')

# Corr mat
cor_mat = cor(data_param_corr[, .SD, .SDcols = c('param_intercept',
                                       'param_V1',
                                       'param_V2',
                                       'param_l',
                                       'param_s',
                                       'param_u')])
cor_mat
```

**Individual LR functions**

```{r}
# Get parameter difference
data_surprise_param = data_param_corr %>%
  .[, param_uml := param_u - param_l] %>%
  .[, param_uml_dicho := param_u > param_l]
```

```{r}
# Get individual LR functions for surprise model
data_lrs = data[model == 'surprise' & variable == 'LRs' & !is.na(value)] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'model', 'AICc', 'variable',
                       'x', 'value')] %>%
  # Fuse encountered LRs with uml
  data.table::merge.data.table(., data_surprise_param,
                               by = c('participant_id', 'group', 'model', 'AICc'))

p = ggplot(data = data_lrs,
           aes(x = as.numeric(x),
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  labs(x = '|PE|',
        y = 'alpha*') +
  facet_wrap(param_uml_dicho~group)
p
```

$\chi^2$-test of dichotomized $u-l$-values: Rising functions vs. Falling functions

```{r}
# Chi-Sq with rising and falling
data_rising = data_surprise_param %>%
  .[, .(n_rising = sum(param_uml_dicho),
        n = .N),
    by = c('group')] %>%
  .[, n_falling := n - n_rising] %>%
  .[, .SD, .SDcols = c('group', 'n_rising', 'n_falling')] %>%
  data.table::melt(measure.vars = c('n_rising', 'n_falling'))

# Test
c_sq = chisq.test(cbind(data_rising[group == 'younger']$value,
                        data_rising[group == 'older']$value))
c_sq
```

**Parametric** $u-l$ analysis against 0

```{r}
# t-test vs. 0
# Younger
ty = t.test(data_surprise_param[group == 'younger']$param_uml,
       mu = 0)
ty
# Older
to = t.test(data_surprise_param[group == 'older']$param_uml,
       mu = 0)
to

p.adjust(c(ty$p.value, to$p.value),
         method = 'holm')
```

**Parametric** $u-l$ analysis between age groups

```{r}
# t-test between groups
t.test(data_surprise_param[group == 'younger']$param_uml,
       data_surprise_param[group == 'older']$param_uml)
```

```{r}
# Continuous comparison
p = ggplot(data = data_surprise_param,
       aes(x = group,
           y = param_uml,
           fill = group,
           color = group)) +
  geom_hline(yintercept = 0,
             size = 0.5,
             color = 'black') +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_half_violin(data = data_surprise_param[group == 'younger'],
                   side = 'r',
                   position = position_nudge(x=-0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  geom_half_violin(data = data_surprise_param[group == 'older'],
                   side = 'l',
                   position = position_nudge(x=0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA)
Neurocodify_plot(p)
```

**Parametric** $u$ analysis between age groups

```{r}
# t-test between groups
t.test(data_surprise_param[group == 'younger']$param_u,
       data_surprise_param[group == 'older']$param_u)
```

```{r}
# Continuous comparison
p = ggplot(data = data_surprise_param,
       aes(x = group,
           y = param_u,
           fill = group,
           color = group)) +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_half_violin(data = data_surprise_param[group == 'younger'],
                   side = 'r',
                   position = position_nudge(x=-0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  geom_half_violin(data = data_surprise_param[group == 'older'],
                   side = 'l',
                   position = position_nudge(x=0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  theme(legend.position = 'none')
Neurocodify_plot(p)

# log space
p = ggplot(data = data_surprise_param,
       aes(x = group,
           y = log(param_u),
           fill = group,
           color = group)) +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_half_violin(data = data_surprise_param[group == 'younger'],
                   side = 'r',
                   position = position_nudge(x=-0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  geom_half_violin(data = data_surprise_param[group == 'older'],
                   side = 'l',
                   position = position_nudge(x=0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  theme(legend.position = 'none')
Neurocodify_plot(p)
```

**Slope parameter** $s$ between age groups

```{r}
# Slope parameter
t.test(data_surprise_param[group == 'younger']$param_s,
       data_surprise_param[group == 'older']$param_s)
```

```{r}
# Continuous comparison
p = ggplot(data = data_surprise_param,
       aes(x = group,
           y = param_s,
           fill = group,
           color = group)) +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_half_violin(data = data_surprise_param[group == 'younger'],
                   side = 'r',
                   position = position_nudge(x=-0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  geom_half_violin(data = data_surprise_param[group == 'older'],
                   side = 'l',
                   position = position_nudge(x=0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA)
Neurocodify_plot(p)
```

$u-l$ vs. immediate influence of surprising outcomes

```{r}
# Load data
data_behav = Load_data() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  Add_comp(.) %>%
  .[, run := as.factor(run)]

# Prepare behavioral data for analysis
data_ii = data_behav %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  .[trial_type == 'choice', correct := if(option_left > option_right) 'left' else 'right',
    by = c('participant_id', 'run', 'trial')] %>%
  .[trial_type == 'forced', correct := forced,
    by = c('participant_id', 'run', 'trial')] %>%
  # Exclude time-out trials
  .[!is.na(outcome), correct_choice := choice == correct,
    by = c('participant_id', 'run', 'trial')] %>%
  # Make sure we only include choices in 1v2 comps when calculating prob of choosing 2
  .[comp != '1v2' | trial_type != 'choice', correct_choice := NA,
    by = c('participant_id', 'run')] %>%
  # Get running averages
  .[, ':='(avg_1_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 1),
           avg_2_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 2),
           avg_3_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 3)),
    by = c('participant_id', 'run')]

# Function to get data slice +-5 trials from rare outcome of bandit 2
Windowrize = function(data,
                      index_rare,
                      window_size){
  result = data[seq(max(index_rare - window_size + 1, 1),
                    index_rare + window_size), ]
  result$window_center = data$trial[index_rare]
  result$window_relative = seq(max(index_rare - window_size + 1, 1),
                               index_rare + window_size) - index_rare
  result[window_relative == 0]$correct_choice = NA
  
  result$window_center = as.factor(result$window_center)
  result$window_relative = as.factor(result$window_relative)
  return(result)
}

# Allocate data holding +-5 trials from rare outcome of bandit 2
window_data = data.table()

# For each participant & run
for(i_id in unique(data_ii$participant_id)){
  for(i_run in unique(data_ii$run)){
    
    # Select data
    temp_data = data_ii[participant_id == i_id &
                      run == i_run]
    
    # Get all trials where rare outcomes were obtained
    idx_chosen_rare_outcome = which(temp_data$is_rare == 1 &
                                      # CHANGE: I also allow rare outcomes that came from forced choices
                                      temp_data$trial_type %in% c('choice', 'forced') &
                                      temp_data$option_choice == 2)
   # For each of the rare-outcome trials
   for(rare_count in seq(1,length(idx_chosen_rare_outcome))){
    # Get data slice
     temp = Windowrize(data = temp_data,
                       index_rare = idx_chosen_rare_outcome[rare_count],
                       window_size = 3) %>%
       .[, window_relative := factor(window_relative, levels = unique(sort(window_relative)))]
     # Add count of the rare outcome
     temp$i_rare = rare_count
     # Fuse data for each participant & run
     window_data = rbind(window_data, temp)
   }
  }
}


# Summarize data
window_data_run = window_data %>%
  # Eliminate windows which extended across trial boundaries (<1 or >240)
  .[, relative_trial := as.numeric(as.character(window_center)) + as.numeric(as.character(window_relative))] %>%
  .[!(relative_trial < 1 | relative_trial > 240),] %>%
  # Sort by relative window
  .[order(rank(group), rank(participant_id), rank(run), rank(window_relative)),] %>%
  # Get mean accuracy across all relative window positions (-2 to +3)
  .[, .(mean_accuracy = mean(correct_choice, na.rm = TRUE),
        n_data = sum(!is.na(correct_choice))),
    by = c('participant_id', 'group', 'run', 'window_relative')]

# Get mean across runs
window_data_participant = window_data_run %>%
  .[, .(accuracy = mean(mean_accuracy, na.rm = TRUE)),
    by = c('participant_id', 'group', 'window_relative')]

# Get difference after - before (critical behavioral effect)
data_cbe = window_data_participant %>%
  data.table::dcast(participant_id + group ~ paste0('rel_', window_relative), value.var = 'accuracy')
colnames(data_cbe)[colnames(data_cbe) == "rel_-1"] = 'rel_m1'
colnames(data_cbe)[colnames(data_cbe) == "rel_-2"] = 'rel_m2'
data_cbe = data_cbe %>%
  .[, cbe := rel_1 - rel_m1] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'cbe')]
```

```{r}
# Correlate effects of rare outcomes with parameters
data_cbe_fit = data.table::merge.data.table(data_surprise_param,
                                            data_cbe,
                                            by = c('participant_id', 'group'))
# model over/underweighting (continuous) and group
m1 = lm(cbe ~ param_uml * group,
        data = data_cbe_fit)
summary(m1)
```

$u$ vs. immediate influence of surprising outcomes

```{r}
# model u (continuous) and group
m1 = lm(cbe ~ param_u * group,
        data = data_cbe_fit)
summary(m1)
```

```{r}
p = ggplot(data = data_cbe_fit,
       aes(x = param_u,
           y = cbe,
           color = group)) +
  geom_point() +
  facet_wrap(~group)
Neurocodify_plot(p)
```

## Focus on participants in which suprise model fits best

```{r}
data_winning_AICc = data_counts %>%
  .[, c('participant_id', 'group', 'winning_model')]

# Add winning model to critical behavioral effect data
data_cbe_fit_surprise = data.table::merge.data.table(data_cbe_fit,
                                                     data_winning_AICc) %>%
  # Only look at participants in which surprise model won
  .[winning_model == 'surprise',]
```

```{r}
p = ggplot(data = data_cbe_fit_surprise,
       aes(x = param_u,
           y = cbe,
           color = group)) +
  geom_point()
Neurocodify_plot(p)

p = ggplot(data = data_cbe_fit_surprise,
       aes(x = log(param_u),
           y = cbe,
           color = group)) +
  geom_point()
Neurocodify_plot(p)
```
