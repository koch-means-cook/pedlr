---
title: "Results 02"
output:
  html_document:
    toc: yes
    embed-resources: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(data.table)
library(here)
library(magrittr)
library(ggplot2)
library(viridis)
library(binhf)
library(pwr)
library(knitr)
library(kableExtra)
library(sdamr)
library(gghalves)
library(lme4)
library(emmeans)
library(papeR)
library(ggridges)
library(bmsR)
library(fabricatr)
library(mlisi)
```

```{r, message=FALSE}
# Get directory of repository
base_path = here::here()

# Load pre-written functions
source_path = file.path(base_path, 'code', 'utils',
                        fsep = .Platform$file.sep)
source_files = list.files(source_path, pattern = "[.][rR]$",
                          full.names = TRUE, recursive = TRUE)
invisible(lapply(source_files, function(x) source(x)))

source_path = file.path(base_path, 'code', 'model_fitting', 'LRfunction.R',
                        fsep = .Platform$file.sep)
source(source_path)


# Get plot colors/linetypes for plots
custom_guides = Get_plot_guides()
```

# Setup

```{r}
# Load modelling results
data = Load_model_fits_new() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  .[, ':='(participant_id = as.factor(participant_id),
           group = as.factor(group),
           sex = as.factor(sex),
           starting_values = as.factor(starting_values))]
# Sort model levels by number of parameters
data$model = factor(data$model, levels = c('rw',
                                           'uncertainty',
                                           'seplr',
                                           'uncertainty_seplr',
                                           'surprise',
                                           'uncertainty_surprise',
                                           'seplr_surprise'))

# Load choice data
data_behav = Load_data() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  Add_comp(.) %>%
  .[,run := as.factor(run)]

# Percentage of optimal choices (bandit 1v2)
check_noc = data_behav %>%
  Add_comp(.) %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  .[trial_type == 'choice',] %>%
  .[, correct_choice := if(option_left > option_right) 'left' else 'right',
    by = c('participant_id', 'run', 'trial')] %>%
  .[, correct := correct_choice == choice] %>%
  # Get percentage of correct choices for each bandit comparison (exclude timeouts from overall trials)
  .[, .(perc_correct = sum(as.numeric(correct), na.rm = TRUE) / length(which(!is.na(as.numeric(correct))))),
    by = c('participant_id', 'group', 'comp')] %>%
  .[comp == '1v2'] %>%
  data.table::dcast(participant_id + group ~ paste0('perc_correct_', as.character(comp)), value.var = 'perc_correct')

# Focus analysis on random starting values
data = data[starting_values == 'random']
```

# Descriptives

## Betas {.tabset}

$$p(k|V_{.,t}) = \sigma(\boldsymbol{\beta_0} + \overbrace{\boldsymbol{\beta_1} V_{l,t}, + \boldsymbol{\beta_2} V_{k,t}}^{\textrm{Influence of Bandit Values}} + \overbrace{\boldsymbol{\beta_3} U_{l,t} + \boldsymbol{\beta_4} U_{k,t}}^{\textrm{Influence of Bandit Uncertainty}})$$

$\boldsymbol{\beta_3}$ and $\boldsymbol{\beta_4}$ are not included in the logistic regression in case the model did not take uncertainty into account.

### Using raw predictors

```{r}
# Get data for betas
data_betas = data %>%
  .[variable == 'coefs',] %>%
  .[x %in% c('(Intercept)', 'V1', 'V2', 'V1u', 'V2u'), ] %>%
  .[, ('x') := lapply(.SD,
                      factor,
                      levels = c('(Intercept)', 'V1', 'V2', 'V1u', 'V2u'),
                      labels= c('b_0', 'b_1', 'b_2', 'b_3', 'b_4')),
    .SDcols = 'x']
```

#### Plot

```{r}
data_plot = data_betas

p_3betas = ggplot(data = data_plot[model %in% c('rw', 'surprise', 'seplr', 'seplr_surprise')],
           aes(x = x,
               y = value,
               fill = model)) +
  geom_hline(yintercept = 0,
             linewidth = 0.1) +
  geom_point(alpha = 0.3,
             size = 0.5,
             position = sdamr::position_jitternudge(jitter.width = 0.05,
                                                    jitter.height = 0,
                                                    nudge.x = -0.1,
                                                    nudge.y = 0)) +
  geom_boxplot(width = 0.1,
               outlier.shape = NA) +
  gghalves::geom_half_violin(side = 'r',
                             color = NA,
                             alpha = 0.8,
                             position = position_nudge(x = 0.1,
                                                       y = 0)) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous(limits = c(min(data_plot$value), max(data_plot$value))) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'none')
p_3betas = Neurocodify_plot(p_3betas)

p_5betas = ggplot(data = data_plot[model %in% c('uncertainty', 'uncertainty_surprise', 'uncertainty_seplr')],
           aes(x = x,
               y = value,
               fill = model)) +
  geom_hline(yintercept = 0,
             linewidth = 0.1) +
  geom_point(alpha = 0.3,
             size = 0.5,
             position = sdamr::position_jitternudge(jitter.width = 0.05,
                                                    jitter.height = 0,
                                                    nudge.x = -0.1,
                                                    nudge.y = 0)) +
  geom_boxplot(width = 0.1,
               outlier.shape = NA) +
  gghalves::geom_half_violin(side = 'r',
                             color = NA,
                             alpha = 0.8,
                             position = position_nudge(x = 0.1,
                                                       y = 0)) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous(limits = c(min(data_plot$value), max(data_plot$value))) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'none')
p_5betas = Neurocodify_plot(p_5betas)

p = cowplot::plot_grid(p_3betas, p_5betas,
                       ncol = 2,
                       rel_widths = c(3,5),
                       axis = 'tb',
                       align = 'h')
p
```

#### Across groups

```{r}
# Prepare table
table_betas_all = data_betas %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        n = .N),
    by = c('model', 'x')]

# Display table
table_betas_all = table_betas_all %>%
  data.table::setcolorder(c('model', 'x')) %>%
  data.table::setorder(model, x) %>%
  .[, c('mean', 'sd') := lapply(.SD, round, 4), .SDcols = c('mean', 'sd')] %>%
  Collapse_repeated_rows(x = .,
                         columns = 1,
                         replace = '') %>%
  kableExtra::kbl("html", align = 'l') %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  kableExtra::column_spec(1,
              bold = TRUE) %>%
  kableExtra::row_spec(row = 0,
                       bold = TRUE)
table_betas_all
```

#### Within groups

```{r}
table_betas_group = data_betas %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        n = .N),
    by = c('model', 'group', 'x')]

# Display table
table_betas_group = table_betas_group %>%
  data.table::setcolorder(c('group', 'model', 'x')) %>%
  data.table::setorder(group, model, x) %>%
  .[, c('mean', 'sd') := lapply(.SD, round, 4), .SDcols = c('mean', 'sd')] %>%
  Collapse_repeated_rows(x = .,
                         columns = c(1,2),
                         replace = '') %>%
  kableExtra::kbl("html", align = 'l') %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  kableExtra::column_spec(1,
              bold = TRUE) %>%
  kableExtra::row_spec(row = 0,
                       bold = TRUE)
table_betas_group
```

### Using z-scored predictors

```{r}
# Get data for betas
data_betas_z = data %>%
  .[variable == 'coefs',] %>%
  .[x %in% c('z_(Intercept)', 'z_V1', 'z_V2', 'z_V1u', 'z_V2u'), ] %>%
  .[, ('x') := lapply(.SD,
                      factor,
                      levels = c('z_(Intercept)', 'z_V1', 'z_V2', 'z_V1u', 'z_V2u'),
                      labels= c('b_0', 'b_1', 'b_2', 'b_3', 'b_4')),
    .SDcols = 'x']
```

#### Plot

```{r}
data_plot = data_betas_z

p_3betas = ggplot(data = data_plot[model %in% c('rw', 'surprise', 'seplr', 'seplr_surprise')],
           aes(x = x,
               y = value,
               fill = model)) +
  geom_hline(yintercept = 0,
             linewidth = 0.1) +
  geom_point(alpha = 0.3,
             size = 0.5,
             position = sdamr::position_jitternudge(jitter.width = 0.05,
                                                    jitter.height = 0,
                                                    nudge.x = -0.1,
                                                    nudge.y = 0)) +
  geom_boxplot(width = 0.1,
               outlier.shape = NA) +
  gghalves::geom_half_violin(side = 'r',
                             color = NA,
                             alpha = 0.8,
                             position = position_nudge(x = 0.1,
                                                       y = 0)) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous(limits = c(min(data_plot$value), max(data_plot$value))) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'none')
p_3betas = Neurocodify_plot(p_3betas)

p_5betas = ggplot(data = data_plot[model %in% c('uncertainty', 'uncertainty_surprise', 'uncertainty_seplr')],
           aes(x = x,
               y = value,
               fill = model)) +
  geom_hline(yintercept = 0,
             linewidth = 0.1) +
  geom_point(alpha = 0.3,
             size = 0.5,
             position = sdamr::position_jitternudge(jitter.width = 0.05,
                                                    jitter.height = 0,
                                                    nudge.x = -0.1,
                                                    nudge.y = 0)) +
  geom_boxplot(width = 0.1,
               outlier.shape = NA) +
  gghalves::geom_half_violin(side = 'r',
                             color = NA,
                             alpha = 0.8,
                             position = position_nudge(x = 0.1,
                                                       y = 0)) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous(limits = c(min(data_plot$value), max(data_plot$value))) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.title.x = element_blank(),
        legend.position = 'none')
p_5betas = Neurocodify_plot(p_5betas)

p = cowplot::plot_grid(p_3betas, p_5betas,
                       ncol = 2,
                       rel_widths = c(3,5),
                       axis = 'tb',
                       align = 'h')
p
```

#### Across groups

```{r}
# Prepare table
table_betas_all_z = data_betas_z %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        n = .N),
    by = c('model', 'x')]

# Display table
table_betas_all_z = table_betas_all_z %>%
  data.table::setcolorder(c('model', 'x')) %>%
  data.table::setorder(model, x) %>%
  .[, c('mean', 'sd') := lapply(.SD, round, 4), .SDcols = c('mean', 'sd')] %>%
  Collapse_repeated_rows(x = .,
                         columns = 1,
                         replace = '') %>%
  kableExtra::kbl("html", align = 'l') %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  kableExtra::column_spec(1,
              bold = TRUE) %>%
  kableExtra::row_spec(row = 0,
                       bold = TRUE)
table_betas_all_z
```

#### Within groups

```{r}
table_betas_group_z = data_betas_z %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        n = .N),
    by = c('model', 'group', 'x')]

# Display table
table_betas_group_z = table_betas_group_z %>%
  data.table::setcolorder(c('group', 'model', 'x')) %>%
  data.table::setorder(group, model, x) %>%
  .[, c('mean', 'sd') := lapply(.SD, round, 4), .SDcols = c('mean', 'sd')] %>%
  Collapse_repeated_rows(x = .,
                         columns = c(1,2),
                         replace = '') %>%
  kableExtra::kbl("html", align = 'l') %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  kableExtra::column_spec(1,
              bold = TRUE) %>%
  kableExtra::row_spec(row = 0,
                       bold = TRUE)
table_betas_group_z
```

------------------------------------------------------------------------

# RW model: RL-like behavior

```{r}
# Get statistics of beta 1 and 2 
data_rw_beta = data_betas %>%
  .[model == 'rw',] %>%
  data.table::dcast(participant_id + group + model + AICc ~ x, value.var = 'value') %>%
  data.table::merge.data.table(., check_noc, by = c('participant_id', 'group'))
```

## right bandit (beta_2)

Manuscript ll. 402-403

```{r}
# right bandit
# Younger
b_right_ya = t.test(data_rw_beta[group == 'younger']$b_2,
       mu = 0)
# Older
b_right_oa = t.test(data_rw_beta[group == 'older']$b_2,
       mu = 0)

b_right_ya
b_right_oa
```

## left bandit (beta_1)

Manuscrip l. 404

```{r}
# left bandit
# Younger
b_left_ya = t.test(data_rw_beta[group == 'younger']$b_1,
       mu = 0)
# Older
b_left_oa = t.test(data_rw_beta[group == 'older']$b_1,
       mu = 0)

b_left_ya
b_left_oa
```

## Correlation of betas with correct choice in low-mid

Manuscript ll. 406-407

```{r}
# Get correlation of betas with percentage correct (high betas mean less random behavior)
# Right bandit
c_b_right = cor.test(data_rw_beta$b_2,
                    data_rw_beta$perc_correct_1v2)

# Left bandit
c_b_left = cor.test(data_rw_beta$b_1,
                     data_rw_beta$perc_correct_1v2)

# Adjust for multiple testing
p_adjusted = p.adjust(c(c_b_left$p.value, c_b_right$p.value), method = 'holm')

c_b_left
c_b_right

# Display p_values
print(paste0('p-values (corr.): ' , p_adjusted))
```

------------------------------------------------------------------------

# Winning model

```{r}
# Get AICc per model
data_model_comp = data %>%
  .[iter == 1] %>%
  .[, .(AICc = mean(AICc),
        sd_AICc = sd(AICc),
        n_params = sum(variable == 'coefs')),
    by = c('participant_id', 'group', 'model')]

# Summarize AICc across groups
data_model_comp_all = data_model_comp %>%
  .[, .(AICc = mean(AICc)),
    by = 'model'] %>%
  # Sort by lowest AIC
  .[order(rank(AICc))]

# And within groups
data_model_comp_age = data_model_comp %>%
  .[, .(AICc = mean(AICc)),
    by = c('group', 'model')] %>%
  # Sort by age and lowest AIC
  .[order(group, AICc)]

# Get winning model within each participant
data_counts = data_model_comp %>%
  data.table::melt(id.vars = c('participant_id', 'group', 'model'),
                   measure.vars = c('AICc')) %>%
  .[, ':='(lowest = min(value),
           loc_winning = value == min(value),
           # Name of winning model
           winning_model = model[value == min(value)]),
    by = c('participant_id', 'variable')] %>%
  # Only keep winning model
  .[loc_winning == TRUE]
```

## Across groups

Manuscript ll. 422-425

```{r}
# Count winning models across groups
data_counts_all = data_counts %>%
  .[, .(n_winning = .N),
    by = c('variable', 'model')] %>%
  # Sort by winning counts
  .[order(-n_winning),] %>%
  # set factor order by win counts
  .[, model := factor(model, levels = model)]

# Display table
table_counts_all = Collapse_repeated_rows(data_counts_all,
                                          columns = 1,
                                          replace = '') %>%
  kableExtra::kbl("html", align = 'l') %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  column_spec(1,
              bold = TRUE) %>%
  row_spec(row = 0,
           bold = TRUE)
table_counts_all
```

```{r}
data_plot = Prepare_data_for_plot(data_counts_all)

# Plot
p = ggplot(data = data_plot,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~variable, ncol = 2)
p = Neurocodify_plot(p) +
  theme(legend.position = 'none',
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45,
                                   hjust = 1,
                                   vjust = 1),
        axis.title.x = element_blank())
p
```

## Within groups

```{r}
# Count winning models within age-groups
data_counts_age = data_counts %>%
  .[, .(n_winning = .N),
    by = c('variable', 'group', 'model')] %>%
  .[order(group, -n_winning)] %>%
  # set factor order by win counts
  .[, model := factor(model, levels = data_counts_all$model)]

# Display table
table_counts_age = Collapse_repeated_rows(data_counts_age,
                                          columns = c(1,2),
                                          replace = '') %>%
kableExtra::kbl("html", align = 'l') %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  column_spec(1,
              bold = TRUE) %>%
  row_spec(row = 0,
           bold = TRUE)
table_counts_age
```

```{r}
# Plot
p = ggplot(data = data_counts_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(group~variable, ncol = 2)
p = Neurocodify_plot(p) +
  theme(legend.position = 'none',
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45,
                                   hjust = 1,
                                   vjust = 1),
        axis.title.x = element_blank())
p
```

------------------------------------------------------------------------

# Protected exceedance probability

Manuscript ll. 412-416

```{r}
# Set seed for sampling (for reproducibility)
set.seed(666)

# Exceedance probs
data_ep = data_model_comp %>%
  # Set order of models after winning models
  .[, model := factor(model, levels = data_counts_all$model)] %>%
  .[, ':='(nAICc = -(AICc))] %>%
  data.table::dcast(participant_id + group ~ model, value.var = 'nAICc')

ep = bmsR::VB_bms(as.matrix(data_ep[, .SD, .SDcols = levels(data_counts_all$model)]),
             n_samples = 100000)
data_pxp = data.table::data.table('model' = levels(data_counts_all$model),
                             'pxp' = ep$pxp) %>%
  .[, model := factor(model, levels = model)]

# Re-initialize seed
set.seed(NULL)

# display pxp
data_pxp
```

```{r}
# Plot
p = ggplot(data = data_pxp,
           aes(x = model,
               y = pxp,
               fill = model)) +
  geom_col() +
  scale_y_continuous(limits = c(0,1))

p = Neurocodify_plot(p) +
  theme(legend.position = 'none',
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45,
                                   hjust = 1,
                                   vjust = 1),
        axis.title.x = element_blank())
p
```


------------------------------------------------------------------------

# Average AICc across models

## Across age groups

Manuscript ll. 409-412

```{r}
# Across all participants
data_AICc = data_model_comp %>%
  .[, .(mean_AICc = mean(AICc),
        sd_AICc = sd(AICc),
        sem_AICc = sd(AICc)/sqrt(.N)),
    by = c('model')]

# Display table
table_AICc_all = data_AICc %>%
  .[, c('mean_AICc', 'sd_AICc', 'sem_AICc') := lapply(.SD, round, 2),
    .SDcols = c('mean_AICc', 'sd_AICc', 'sem_AICc')] %>%
  kableExtra::kbl("html", align = 'l') %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  column_spec(1,
              bold = TRUE) %>%
  row_spec(row = 0,
           bold = TRUE)
table_AICc_all
```

```{r}
# Plot
data_plot = data_model_comp
data_plot_mean = data_AICc

p1 = ggplot(data = data_plot,
           aes(x = model,
               y = AICc,
               fill = model,
               color = model)) +
  geom_col(data = data_plot_mean,
           inherit.aes = FALSE,
           aes(x = model,
               y = mean_AICc,
               fill = model),
           color = NA) + 
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0),
             size = 0.5,
             alpha = 0.5,
             color = 'black') +
  geom_errorbar(data = data_plot_mean,
                inherit.aes = FALSE,
                aes(x = model,
                    ymin = mean_AICc - sem_AICc,
                    ymax = mean_AICc + sem_AICc),
                color = 'black',
                width = 0.3)

p1 = Neurocodify_plot(p1) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        legend.position = 'none',
        axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45,
                                   hjust = 1,
                                   vjust = 1))

# Zoomed in version
p2 = ggplot(data = data_plot,
           aes(x = model,
               y = AICc,
               fill = model,
               color = model)) +
    geom_errorbar(data = data_plot_mean,
                inherit.aes = FALSE,
                aes(x = model,
                    ymin = mean_AICc - sem_AICc,
                    ymax = mean_AICc + sem_AICc),
                color = 'black',
                width = 0.3) +
  geom_point(data = data_plot_mean,
             inherit.aes = FALSE,
             aes(x = model,
                 y = mean_AICc,
                 fill = model),
             color = 'black',
             size = 4,
             shape = 23,
             stroke = 1) +
  scale_y_continuous(limits = c(110,125))


p2 = Neurocodify_plot(p2) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        legend.position = 'none',
        axis.title = element_blank(),
        axis.text.x = element_text(angle = 45,
                                   hjust = 1,
                                   vjust = 1))

p = cowplot::plot_grid(p1, p2,
                       ncol = 2,
                       nrow = 1,
                       axis = 'bt',
                       align = 'h',
                       rel_widths = c(2,1))
p
```


------------------------------------------------------------------------

# Parametric AICc comparison

Manuscript ll. 416-422

```{r}
# Relative AICc to RW model
data_model_comp_rw = data_model_comp %>%
  data.table::dcast(participant_id + group ~ paste0('AICc_', model),
                    value.var = 'AICc') %>%
  .[, ':='(SmRW = AICc_surprise - AICc_rw,
           SEPLRmRW = AICc_seplr - AICc_rw,
           UmRW = AICc_uncertainty - AICc_rw,
           USEPLRmRW = AICc_uncertainty_seplr - AICc_rw,
           USmRW = AICc_uncertainty_surprise - AICc_rw,
           SEPLRSmRW = AICc_seplr_surprise - AICc_rw),
    by = c('participant_id', 'group')] %>%
  data.table::melt(id.vars = c('participant_id', 'group'),
                   measure.vars = c('UmRW',
                                    'SEPLRmRW',
                                    'USEPLRmRW',
                                    'SmRW',
                                    'USmRW',
                                    'SEPLRSmRW'))

# Groups
rel_model_comp_group = data_model_comp_rw %>%
  .[, .(t = t.test(value, mu = 0)$statistic,
        df = t.test(value, mu = 0)$parameter,
        p = t.test(value, mu = 0)$p.value),
    by = c('group', 'variable')] %>%
  .[, p_adj := p.adjust(p, method = 'holm')] %>%
  .[order(group, variable)] %>%
  Prepare_data_for_plot(.) %>%
  data.table::setcolorder(c('group', 'variable'))

# Display table
table_rel_model_comp = rel_model_comp_group %>%
  .[, t := round(t, 2)] %>%
  .[, c('p', 'p_adj') := lapply(.SD, round, 3), .SDcols = c('p', 'p_adj')] %>%
  Collapse_repeated_rows(.,
                         columns = 1,
                         replace = '') %>%
  kableExtra::kbl(align = 'l', booktabs = TRUE) %>%
  kableExtra::kable_classic(full_width = FALSE) %>%
  kableExtra::column_spec(1,
                          bold = TRUE) %>%
  kableExtra::row_spec(row = length(unique(rel_model_comp_group$variable)),
                       extra_css = "border-bottom: 2px solid;") %>%
  kableExtra::row_spec(row = 0,
                       bold = TRUE)
table_rel_model_comp
```

```{r}
data_plot = data_model_comp_rw %>%
  .[, variable := factor(variable,
                         levels = c('UmRW',
                                    'SEPLRmRW',
                                    'USEPLRmRW',
                                    'SmRW',
                                    'USmRW',
                                    'SEPLRSmRW'))] %>%
  .[order(variable),]

data_plot_mean = data_model_comp_rw %>%
  .[, .(value = mean(value),
        sd_value = sd(value),
        n = .N,
        sem_value = sd(value)/sqrt(.N)),
    by = 'variable'] %>%
  .[, variable := factor(variable,
                         levels = c('UmRW',
                                    'SEPLRmRW',
                                    'USEPLRmRW',
                                    'SmRW',
                                    'USmRW',
                                    'SEPLRSmRW'))] %>%
  .[order(variable),]

p = ggplot(data = data_model_comp_rw,
           aes(x = variable,
               y = value,
               fill = variable)) +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_col(data = data_plot_mean)
Neurocodify_plot(p)
```

------------------------------------------------------------------------

# Surprise model

```{r}
# Data for correlation between betas and performance
data_corr = data %>%
  .[variable == 'coefs' & model == 'surprise' & iter == 1] %>%
  data.table::dcast(participant_id + group + AIC ~ x, value.var = 'value') %>%
  data.table::merge.data.table(., check_noc, by = c('participant_id', 'group'))
```

**Correlation between betas and correct 1v2 choices**

Manuscript ll. 448-449

```{r}
# V1 ~ performance
m_left = cor.test(data_corr$V1, data_corr$perc_correct_1v2)
m_left

# V2 ~ performance
m_right = cor.test(data_corr$V2, data_corr$perc_correct_1v2)
m_right


p.adjust(c(m_left$p.value,
           m_right$p.value),
         method = 'holm')
```

**Parameter details**

```{r}
# get coefficients of winning model
data_surprise = data[model == 'surprise' & variable == 'coefs']

data_param_overall = data_surprise %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        iqr = IQR(value)),
    by = c('x')] %>%
  .[, group := 'all']

data_param_group = data_surprise %>%
  .[, .(mean = mean(value),
        sd = sd(value),
        iqr = IQR(value)),
    by = c('group', 'x')]

data_param_summary = rbind(data_param_overall,
                           data_param_group) %>%
  Prepare_data_for_plot(.)

knitr::kable(data_param_summary) %>%
  kableExtra::kable_styling()
```

**Correlation matrix**

see figure 6, panel A (right side)

```{r}
# Correlations between parameters
data_surprise[x == '(Intercept)']$x = 'intercept'
data_param_corr = data_surprise %>%
  data.table::dcast(participant_id + group + model + AIC + AICc ~ paste0('param_', x),
                    value.var = 'value')

# Corr mat
cor_mat = cor(data_param_corr[, .SD, .SDcols = c('param_intercept',
                                       'param_V1',
                                       'param_V2',
                                       'param_l',
                                       'param_s',
                                       'param_u')])
cor_mat
```

**Individual LR functions**

see figure 6, panel B

```{r}
# Get parameter difference
data_surprise_param = data_param_corr %>%
  .[, param_uml := param_u - param_l] %>%
  .[, param_uml_dicho := param_u > param_l]
```

```{r}
# Get individual LR functions for surprise model
data_lrs = data[model == 'surprise' & variable == 'LRs' & !is.na(value)] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'model', 'AICc', 'variable',
                       'x', 'value')] %>%
  # Fuse encountered LRs with uml
  data.table::merge.data.table(., data_surprise_param,
                               by = c('participant_id', 'group', 'model', 'AICc'))

p = ggplot(data = data_lrs,
           aes(x = as.numeric(x),
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  labs(x = '|PE|',
        y = 'alpha*') +
  facet_wrap(param_uml_dicho~group)
p
```

**Number of participants with increasing LR function**

Manuscript ll. 462-463

```{r}
# Count participants within rising and faling group
data_rising = data_surprise_param %>%
  .[, .(n_rising = sum(param_uml_dicho),
        n = .N),
    by = c('group')] %>%
  .[, n_falling := n - n_rising] %>%
  .[, .SD, .SDcols = c('group', 'n_rising', 'n_falling')] %>%
  data.table::melt(measure.vars = c('n_rising', 'n_falling'))

# Display number of participants with rising LR-Functions
data_rising[variable == 'n_rising',]
```


$\chi^2$-test of dichotomized $u-l$-values: Rising functions vs. Falling functions

Manuscript ll. 463-465

```{r}
# Chi-Sq with rising
c_sq = chisq.test(cbind(data_rising[group == 'younger']$value,
                        data_rising[group == 'older']$value))
c_sq
```

**Parametric** $u-l$ analysis against 0

Manuscript ll. 457-459

```{r}
# t-test vs. 0
# Younger
ty = t.test(data_surprise_param[group == 'younger']$param_uml,
       mu = 0)
ty
# Older
to = t.test(data_surprise_param[group == 'older']$param_uml,
       mu = 0)
to

p.adjust(c(ty$p.value, to$p.value),
         method = 'holm')
```

**Parametric** $u-l$ analysis between age groups

Manuscript ll. 464-465

```{r}
# t-test between groups
t.test(data_surprise_param[group == 'younger']$param_uml,
       data_surprise_param[group == 'older']$param_uml)
```

```{r}
# Continuous comparison
p = ggplot(data = data_surprise_param,
       aes(x = group,
           y = param_uml,
           fill = group,
           color = group)) +
  geom_hline(yintercept = 0,
             size = 0.5,
             color = 'black') +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_half_violin(data = data_surprise_param[group == 'younger'],
                   side = 'r',
                   position = position_nudge(x=-0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  geom_half_violin(data = data_surprise_param[group == 'older'],
                   side = 'l',
                   position = position_nudge(x=0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA)
Neurocodify_plot(p)
```

**Slope parameter** $s$ between age groups

Manuscript ll. 465-466

```{r}
# Slope parameter
t.test(data_surprise_param[group == 'younger']$param_s,
       data_surprise_param[group == 'older']$param_s)
```

```{r}
# Continuous comparison
p = ggplot(data = data_surprise_param,
       aes(x = group,
           y = param_s,
           fill = group,
           color = group)) +
  geom_point(position = position_jitter(width = 0.1,
                                        height = 0,
                                        seed = 666)) +
  geom_half_violin(data = data_surprise_param[group == 'younger'],
                   side = 'r',
                   position = position_nudge(x=-0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA) +
  geom_half_violin(data = data_surprise_param[group == 'older'],
                   side = 'l',
                   position = position_nudge(x=0.49,
                                             y = 0),
                   width = 0.5,
                   alpha = 0.8,
                   color = NA)
Neurocodify_plot(p)
```

$u-l$ vs. immediate influence of surprising outcomes

```{r}
# Load data
data_behav = Load_data() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  Add_comp(.) %>%
  .[, run := as.factor(run)]

# Load windowrized data
file = file.path(base_path, 'derivatives', 'analysis', 'windowrize.tsv',
                 fsep = .Platform$file.sep)
window_data = data.table::fread(file = file, sep = '\t', na.strings = 'n/a')

# Summarize data
window_data_run = window_data %>%
  # Get mean accuracy across all relative window positions (-2 to +3)
  .[, .(mean_accuracy = mean(correct_choice, na.rm = TRUE),
        n_data = sum(!is.na(correct_choice))),
    by = c('participant_id', 'group', 'run', 'window_relative')]

# Get mean across runs
window_data_participant = window_data_run %>%
  .[, .(accuracy = mean(mean_accuracy, na.rm = TRUE)),
    by = c('participant_id', 'group', 'window_relative')]

# Get difference after - before (critical behavioral effect)
data_cbe = window_data_participant %>%
  data.table::dcast(participant_id + group ~ paste0('rel_', window_relative), value.var = 'accuracy')
colnames(data_cbe)[colnames(data_cbe) == "rel_-1"] = 'rel_m1'
colnames(data_cbe)[colnames(data_cbe) == "rel_-2"] = 'rel_m2'
data_cbe = data_cbe %>%
  .[, cbe := rel_1 - rel_m1] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'cbe')]
```

```{r}
# Correlate effects of rare outcomes with parameters
data_cbe_fit = data.table::merge.data.table(data_surprise_param,
                                            data_cbe,
                                            by = c('participant_id', 'group'))
# model over/underweighting (continuous) and group
m1 = lm(cbe ~ param_uml * group,
        data = data_cbe_fit)
summary(m1)
```

Manuscript ll. 454-456

```{r}
# Simple Pearson correlation
cor(x = data_cbe_fit$param_uml,
    y = data_cbe_fit$cbe,
    method = 'pearson')
```

---

# Valence + Surprise model

```{r}
# Get parameters and dicho uml for seplr+surprise model (pos and neg)
data_seplr_surprise = data[model == 'seplr_surprise' & variable == 'coefs'] %>%
  .[x %in% c('l_pos', 'u_pos', 's_pos', 'l_neg', 'u_neg', 's_neg')] %>%
  data.table::dcast(participant_id + group + model + AIC + AICc ~ paste0('param_', x),
                    value.var = 'value') %>%
  .[, ':='(uml_dicho_neg = param_u_neg > param_l_neg,
           uml_dicho_pos = param_u_pos > param_l_pos)] %>%
  .[, ':='(uml_dicho_neg = factor(uml_dicho_neg, labels = c('falling', 'rising')),
           uml_dicho_pos = factor(uml_dicho_pos, labels = c('falling', 'rising')))]

# Get individual LR functions for seplr+surprise model
data_lrs = data[model == 'seplr_surprise' & variable == 'LRs' & !is.na(value)] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'model', 'AICc', 'variable',
                       'x', 'value')] %>%
  # Fuse encountered LRs with uml
  data.table::merge.data.table(., data_seplr_surprise,
                               by = c('participant_id', 'group', 'model', 'AICc')) %>%
  .[, x := as.numeric(x)] %>%
  # Add variable of LR function for different PE valence
  .[x < 0, valence := 'neg'] %>%
  .[x > 0, valence := 'pos']
```

```{r}
# Neg
p = ggplot(data_lrs[valence == 'neg'],
           aes(x = x,
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  scale_x_continuous(limits = c(-(max(abs(data_lrs$x))), 0)) +
  labs(x = 'PE',
       y = 'alpha*') +
  facet_wrap(~group + uml_dicho_neg) +
  annotate('text', x = -50, y = 0.5, label = 'u_neg', size = 3, hjust = 0) +
  annotate('text', x = 0, y = 0.5, label = 'l_neg', size = 3, hjust = 1)
Neurocodify_plot(p)
```

```{r}
# pos
p = ggplot(data_lrs[valence == 'pos'],
           aes(x = x,
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  labs(x = 'PE',
       y = 'alpha*') +
  scale_x_continuous(limits = c(0, max(abs(data_lrs$x)))) +
  annotate('text', x = 0, y = 0.5, label = 'l_pos', size = 3, hjust = 0) +
  annotate('text', x = 50, y = 0.5, label = 'u_pos', size = 3, hjust = 1) +
  facet_wrap(~group + uml_dicho_pos)
Neurocodify_plot(p)
```
