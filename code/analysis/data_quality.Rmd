---
title: "Half-way data quality assessment"
output:
  html_document:
    toc: yes
    self_contained: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(data.table)
library(here)
library(magrittr)
library(ggplot2)
library(viridis)
library(binhf)
library(pwr)
library(knitr)
library(kableExtra)
library(cowplot)
library(ggbeeswarm)
```

```{r, message=FALSE}
# Get directory of repository
base_path = here::here()

# Load pre-written functions
source_path = file.path(base_path, 'code', 'utils',
                        fsep = .Platform$file.sep)
source_files = list.files(source_path, pattern = "[.][rR]$",
                          full.names = TRUE, recursive = TRUE)
invisible(lapply(source_files, function(x) source(x)))
```

```{r}
# Load data
data = Load_data() %>%
  Add_comp(.) %>%
  .[, run := as.factor(run)]

# get number of participants (to adjust figure height)
n_participants = length(unique(data$participant_id))

# allocate table to hold suspicious participants
sus_pool = data.table()
```

# Data quality

## Errors in forced choices

See if participants made a lot of errors in the forced choice trials.
Forced choice trials were very simple: Choose the option that had a frame around it.
A lot of errors in these trials indicate the participant did not pay attention to the task at hand.

```{r, out.width='100%'}
# Get correctness in forced choices
fce = data %>%
  .[trial_type == 'forced',] %>%
  .[, forced_error := error] %>%
  .[, .(n_forced = length(forced),
        n_error = sum(as.numeric(forced_error)),
        group = unique(group)), by = 'participant_id'] %>%
  # Percentage false
  .[, perc_forced_error := round((n_error/n_forced) * 100, 2)] %>%
  .[, count := seq(.N), by = c('n_error', 'group')] %>%
  # Get z-score
  .[, z_perc_forced_error := scale(perc_forced_error)] %>%
  # Apply excl criterion
  .[, excl := z_perc_forced_error >= 3]

# Fill pool with sus participants
sus_pool = fce[excl == TRUE, c('participant_id', 'group')]
sus_pool$run = NA
sus_pool$reason = 'Forced choice errors + 3 SDs'

data_plot = Prepare_data_for_plot(fce)
p = ggplot(data = data_plot,
           aes(x = group,
               y = perc_forced_error,
               color = excl)) +
  # Add line to mark target of correct answers (everything over line is not accurate enough)
  geom_hline(yintercept = 25,
             linetype = 'dashed') +
  ggbeeswarm::geom_beeswarm() +
  labs(y = '% of errors in forced choice trials') +
  theme(legend.position = 'top')
Neurocodify_plot(p)
```

**Above:** Amount of errors in 'guided' choice trials.
Dashed line shows 25% errors (75% accuracy).
Most participants stay way below this critical error rate.
Additionally, older adults don't seem much worse in this measure.

## Difference estimation bandit 1 and 3

```{r, warning=FALSE}
est = data %>%
  .[, ':='(avg_1_running = Get_running_avg(choice_option = option_choice,
                                           choice_outcome = outcome,
                                           stim = 1),
           avg_2_running = Get_running_avg(choice_option = option_choice,
                                           choice_outcome = outcome,
                                           stim = 2),
           avg_3_running = Get_running_avg(choice_option = option_choice,
                                           choice_outcome = outcome,
                                           stim = 3)),
    by = c('participant_id', 'run')] %>%
  .[, forced_rare := as.numeric(as.logical(is_rare) & trial_type == 'forced' & (comp == '1v2' | comp == '2v3'))] %>%
  .[!is.na(est_1_reward),] %>%
  .[, est_trial := seq(.N), by = c('participant_id', 'run')]
# Convert measure variables to same type (to avoid "melt" warning)
conv_cols = c('est_1_reward',
              'est_1_range',
              'avg_1_running',
              'est_2_reward',
              'est_2_range',
              'avg_2_running',
              'est_3_reward',
              'est_3_range',
              'avg_3_running')
out_cols = conv_cols
est = est %>%
  .[, c(out_cols) := lapply(.SD, as.double), .SDcols = conv_cols] %>%
  data.table::melt(.,
                   id.vars = c('participant_id',
                               'group',
                               'run',
                               'est_trial',
                               'forced_rare'),
                   measure.vars = conv_cols) %>%
  .[, est_stim := substr(variable, 5, 5)] %>%
  .[, type := substr(variable, 7, 9)] %>%
  .[type == 'rew', type := 'reward'] %>%
  .[type == 'ran', type := 'range'] %>%
  .[type == 'run', type := 'r_avg'] %>%
  data.table::dcast(., participant_id + group + run + est_trial + forced_rare + est_stim ~ type,
                    value.var = 'value')

# Compare estimates of bandit 1 and 3 with paired t-test within subject
est_1v3 = est %>%
  .[est_stim != 2,] %>%
  data.table::dcast(participant_id + group + run + est_trial ~ paste0('est_', est_stim),
                    value.var = 'reward') %>%
  .[, .(mean_1 = mean(est_1),
        mean_3 = mean(est_3),
        # Test for sig difference between estimate of bandit 1 and 3 (across 
        # task versions is okay because of paired)
        statistic = t.test(x = est_1,
                           y = est_3,
                           alternative = 'less',
                           paired = TRUE)$statistic,
        parameter = t.test(x = est_1,
                           y = est_3,
                           alternative = 'less',
                           paired = TRUE)$parameter,
        p.value = round(t.test(x = est_1,
                               y = est_3,
                               alternative = 'less',
                               paired = TRUE)$p.value, 3)),
    by = c('participant_id', 'group')]

# add suspicious participants
temp = est_1v3[p.value > 0.05,
               c('participant_id', 'group')]
temp$run = NA
temp$reason = 'est_1v3_not_different'
sus_pool = rbind(sus_pool, temp)
```

See if participants can't accurately estimate that bandit 1 and 3 are different.
The distributions of bandit 1 and 3 are very different and don't touch.
Not being able to estimate a difference between them indicates the participant did not pay attention to the task at hand.

```{r}
data_plot = Prepare_data_for_plot(est[est_stim != 2]) %>%
  # data.table::melt(id.vars = c('participant_id', 'group', 'run', 'est_trial', 'est_stim'),
  #                  measure.vars = c('r_avg',
  #                                   'reward')) %>%
  .[participant_id %in% est_1v3[p.value > 0.05]$participant_id, excl := TRUE] %>%
  .[!participant_id %in% est_1v3[p.value > 0.05]$participant_id, excl := FALSE]

p = ggplot(data = data_plot,
               aes(x = est_trial,
                   y = reward,
                   color = est_stim)) +
  geom_path(aes(y = r_avg),
             linetype = 'dashed',
            alpha = 0.5) +
  geom_point(size = 0.8) +
  geom_path() +
  labs(x = 'Estimation Trial',
       y = 'Estimate') +
  facet_grid(participant_id ~ excl + run) +
  scale_color_viridis(option = 'D', discrete = TRUE) +
  theme(legend.position = 'top')
p = Neurocodify_plot(p)
```

```{r}
# Add accuracy of 1v3 choices
check_corr_1v3 = data %>%
  .[comp == '1v3' & trial_type == 'choice',] %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'group', 'run')] %>%
  .[, corr_outcome := max(c(reward_stim_1, reward_stim_2)),
    by = c('participant_id', 'group', 'run', 'trial')] %>%
  .[, corr := corr_outcome == outcome,
    by = c('participant_id', 'group', 'run', 'trial')] %>%
  # get accuracy for valid 1v3 trials (timeouts and forced choices excluded)
  .[!is.na(corr), .(corr_1v3 = sum(corr) / length(corr)),
    by = c('participant_id', 'group', 'run')]

p_ext = ggplot(data = check_corr_1v3,
               aes(x = run,
                   y = corr_1v3)) +
  geom_hline(yintercept = 0.5,
             linetype = 'dashed') +
  geom_point() +
  facet_grid(participant_id ~ 'corr_1v3')
p_ext = Neurocodify_plot(p_ext)
```

```{r, out.width='100%', fig.height=n_participants}
p_full = cowplot::plot_grid(p_ext, p,
                            rel_widths = c(0.3,1),
                            ncol = 2,
                            axis = 'bt',
                            align = 'h')
p_full
```

**Above:** Suspicious and valid estimation patterns of participants.
We did a paired t-test across ALL estimation trials (also across task runs) between estimations of bandit 1 and 3.
Participants where we cannot find a significant difference between these two bandits using this test are listed in the leftmost two columns of the right plot.
These participants estimated similar means for two separate, not-crossing Gaussian distributions.
This makes it likely they did not pay attention to the estimation trials.
The left plot shows accuracy in comparisons between bandit 1 and 3 in choice trials.

## Overall performance

```{r}
perf_ov = data %>%
  .[, trial := seq(.N),
    by = participant_id] %>%
  # Select only choice trials without timeouts
  .[trial_type == 'choice' & !is.na(outcome),] %>%
  # Get correctness of each trial (choice of stimulus with higher mean)
  .[, corr_choice := (max(option_left, option_right) == option_choice),
    by = c('participant_id', 'trial')] %>%
  # Get percentage of overall correct choices
  .[, .(perc_correct = sum(corr_choice) / length(corr_choice),
        # binom test to see difference from chance
        p_binom = binom.test(x = sum(corr_choice),
                             n = length(corr_choice),
                             p = 0.5,
                             alternative = 'greater')$p.value),
    by = c('participant_id', 'group')] %>%
  # Get z-score of performance
  .[, z_perc_correct := scale(perc_correct)] %>%
  .[, excl := p_binom > 0.05]

# Exclude participants with less than 55% overall accuracy (probably guessing)
temp = perf_ov[p_binom > 0.05, c('participant_id', 'group')]
temp$run = NA
temp$reason = 'Overall performance not different from chance (binom test)'
sus_pool = rbind(sus_pool, temp)
```

```{r}
p = ggplot(data = perf_ov,
         aes(x = group,
             y = perc_correct,
             color = excl)) +
    geom_point() +
    geom_hline(yintercept = 0.5) +
    geom_hline(yintercept = 0.55,
               linetype = 'dashed')
Neurocodify_plot(p)
```


## Performance 1v3

```{r}
perf_1v3 = data %>%
  .[comp == '1v3' & trial_type == 'choice',] %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'group')] %>%
  .[, corr_outcome := max(c(reward_stim_1, reward_stim_2)),
    by = c('participant_id', 'group', 'trial')] %>%
  .[, corr := corr_outcome == outcome,
    by = c('participant_id', 'group', 'trial')] %>%
  # Get accuracy for valid 1v3 trials (timeouts and forced choices excluded)
  .[!is.na(corr), .(corr_1v3 = sum(corr) / length(corr),
                    # binom test to see difference from chance
                    p_binom = binom.test(x = sum(corr),
                                         n = length(corr),
                                         p = 0.5,
                                         alternative = 'greater')$p.value),
    by = c('participant_id', 'group')] %>%
  .[, excl := p_binom > 0.05]

# Exclude participants with less than 55% overall accuracy (probably guessing)
temp = perf_1v3[excl == TRUE, c('participant_id', 'group')]
temp$run = NA
temp$reason = 'Performance 1v3 not diferent from chance (binom test)'
sus_pool = rbind(sus_pool, temp)
```

```{r}
p = ggplot(data = perf_1v3,
         aes(x = group,
             y = corr_1v3,
             color = excl)) +
    geom_point() +
    geom_hline(yintercept = 0.5) +
    geom_hline(yintercept = 0.55,
               linetype = 'dashed')
Neurocodify_plot(p)
```


# Percentage of suspicious participants

```{r}
perc_sus = length(unique(sus_pool$participant_id)) / length(unique(data$participant_id)) * 100
```

Roughly `r round(perc_sus,2)`% of the `r n_participants` participants get flagged as suspicious (probably did not pay attention).

```{r}
sus_pool = Get_exclusions()
perc_sus = length(unique(sus_pool$participant_id)) / length(unique(data$participant_id)) * 100
```

## Exclusion reasons

### Binom test: Overall performance vs. chance

```{r}
excl = sus_pool[reason == 'Overall performance not different from chance (binom test)']
ids = excl$participant_id
rem = sus_pool[!participant_id %in% ids, ]
excl
```

### Consistently similar estimate for bandit 1 and 3

```{r}
excl = rem[reason == 'est_1v3_not_different']
ids = excl$participant_id
rem = rem[!participant_id %in% ids, ]
excl
```

### Binom test: 1v3 performance vs. chance

```{r}
excl = rem[reason == 'Performance 1v3 not diferent from chance (binom test)']
ids = excl$participant_id
rem = rem[!participant_id %in% ids, ]
excl
```

### Over 3SD for forced choices

```{r}
excl = rem[reason == 'Forced choice errors + 3 SDs']
ids = excl$participant_id
rem = rem[!participant_id %in% ids, ]
excl
```



