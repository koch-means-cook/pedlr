---
title: "Power analysis"
output:
  html_document:
    toc: yes
    self_contained: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

---

# NOTE

Note that this analysis is only valid on the pilot data!
This analysis can be reproduced by using the pilot data, only!

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(data.table)
library(here)
library(magrittr)
library(ggplot2)
library(viridis)
library(binhf)
library(pwr)
library(knitr)
library(kableExtra)
```

```{r}
# Get directory of repository
base_path = here::here()

# Load pre-written functions
source_path = file.path(base_path, 'code', 'utils',
                        fsep = .Platform$file.sep)
source_files = list.files(source_path, pattern = "[.][rR]$",
                          full.names = TRUE, recursive = TRUE)
invisible(lapply(source_files, function(x) source(x)))
```

```{r}
# Load data
data = Load_data() %>%
  Add_comp(.) %>%
  .[, run := as.factor(run)]

# get number of participants (to adjust figure height)
n_participants = length(unique(data$participant_id))

# Summary: Mistakes on forced choices
check_fce = data %>%
  .[trial_type == 'forced',] %>%
  .[, forced_error := error] %>%
  .[, .(n_forced = length(forced),
        n_error = sum(as.numeric(forced_error)),
        group = unique(group)), by = 'participant_id'] %>%
  .[, perc_forced_error := round((n_error/n_forced) * 100, 2)] %>%
  .[, count := seq(.N), by = c('n_error', 'group')]

# Number of time out trials
check_tot = data %>%
  .[trial_type %in% c('choice', 'forced'),] %>%
  .[, .(n_timeout = sum(as.logical(timeout)),
        group = unique(group)),
        by = 'participant_id'] %>%
  .[, count := seq(.N), by = c('n_timeout', 'group')]

# Function to get running average of rewards per stimulus
Get_running_avg = function(choice_option, choice_outcome, stim){
  # Initialize array keeping running average for all trials
  running_avg = c()
  
  # Loop over trials
  for(i in seq(length(choice_option))){
    # Find all choices of option so far
    idx = choice_option[1:i] == stim
    # Create mean over all stim choices so far
    trialwise_mean = mean(choice_outcome[1:i][idx], na.rm = TRUE)
    # Append running average for each trial
    running_avg = c(running_avg, trialwise_mean)
  }
  
  # Set all trials in which option has not been chosen so far to NA
  running_avg[is.nan(running_avg)] = NA
  return(running_avg)
}

# Estimation summary
check_est = data %>%
  .[, ':='(avg_1_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 1),
           avg_2_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 2),
           avg_3_running = Get_running_avg(choice_option = option_choice,
                                     choice_outcome = outcome,
                                     stim = 3)),
    by = c('participant_id', 'run')] %>%
  .[, forced_rare := as.numeric(as.logical(is_rare) & trial_type == 'forced' & (comp == '1v2' | comp == '2v3'))] %>%
  .[!is.na(est_1_reward),] %>%
  .[, est_trial := seq(.N), by = c('participant_id', 'run')] %>%
  data.table::melt(.,
                   id.vars = c('participant_id',
                               'group',
                               'run',
                               'est_trial',
                               'forced_rare'),
                   measure.vars = c('est_1_reward',
                                    'est_1_range',
                                    'avg_1_running',
                                    'est_2_reward',
                                    'est_2_range',
                                    'avg_2_running',
                                    'est_3_reward',
                                    'est_3_range',
                                    'avg_3_running')) %>%
  .[, est_stim := substr(variable, 5, 5)] %>%
  .[, type := substr(variable, 7, 9)] %>%
  .[type == 'rew', type := 'reward'] %>%
  .[type == 'ran', type := 'range'] %>%
  .[type == 'run', type := 'r_avg'] %>%
  data.table::dcast(., participant_id + group + run + est_trial + forced_rare + est_stim ~ type,
                    value.var = 'value')

# Merge true means with estimation to get difference from truth
check_est_diff = check_est %>%
  # Get difference between estimation and true mean
  .[, diff_from_true := reward - r_avg]
```

# Power analyses

## Estimation

The power analysis is highly depending on the participants included (due to small sample size).
So this is only a rough estimate.
Two participants highly underestimate the 3rd bandit (`1LKX8E3`, `T0WGDM7`).
If these participants are dropped the required sample size to detect that the estimate of the 2nd bandit is lower than the 3rd bandit drops from ca. 3000 to 20.

### Difference from running avg

#### Within run

```{r}
pwr_within = check_est_diff %>%
  # Get mean estimation for each participant per run
  .[, .(mean_est = mean(diff_from_true, na.rm = TRUE),
        sd_est = sd(diff_from_true, na.rm = TRUE)),
    by = c('participant_id', 'run', 'est_stim')] %>%
  # Get mean for whole sample
  .[, .(mean_sample = mean(mean_est),
        sd_est = sd(mean_est),
        sample_size = .N),
    by = c('run', 'est_stim')] %>%
  .[, cohens_d := mean_sample/sd_est] %>%
  .[, suggested_sample := pwr::pwr.t.test(d = cohens_d,
                                          sig.level = 0.05,
                                          power = 0.9,
                                          type = 'one.sample',
                                          alternative = 'two.sided')$n,
    by = c('run', 'est_stim')]
knitr::kable(pwr_within) %>%
  kableExtra::kable_styling()
```

#### Across run

```{r}
pwr_across = check_est_diff %>%
  # Get mean estimation for each participant
  .[, .(mean_est = mean(diff_from_true, na.rm = TRUE),
        sd_est = sd(diff_from_true, na.rm = TRUE)),
    by = c('participant_id', 'est_stim')] %>%
  # Get mean for whole sample
  .[, .(mean_sample = mean(mean_est),
        sd_est = sd(mean_est),
        sample_size = .N),
    by = c('est_stim')] %>%
  .[, cohens_d := mean_sample/sd_est] %>%
  .[, suggested_sample := pwr::pwr.t.test(d = cohens_d,
                                          sig.level = 0.05,
                                          power = 0.9,
                                          type = 'one.sample',
                                          alternative = 'two.sided')$n,
    by = c('est_stim')]
knitr::kable(pwr_across) %>%
  kableExtra::kable_styling()
```

### Difference estimation accuracy in 2 and 3

#### Within run

```{r}
pwr_within = check_est_diff %>%
  # Get mean estimation for each participant per run
  .[, .(mean_est = mean(diff_from_true, na.rm = TRUE),
        sd_est = sd(diff_from_true, na.rm = TRUE)),
    by = c('participant_id', 'run', 'est_stim')] %>%
  data.table::dcast(participant_id + run ~ paste0('mean_est_',est_stim), value.var = 'mean_est') %>%
  .[, diff_3m2 := mean_est_3 - mean_est_2] %>%
  .[, .(mean_diff_3m2 = mean(diff_3m2),
        sd_diff_3m2 = sd(diff_3m2)),
    by = 'run'] %>%
  .[, cohens_d := mean_diff_3m2/sd_diff_3m2] %>%
  .[, suggested_sample := pwr::pwr.t.test(d = cohens_d,
                                          sig.level = 0.05,
                                          power = 0.9,
                                          type = 'one.sample',
                                          alternative = 'two.sided')$n,
    by = 'run']
knitr::kable(pwr_within) %>%
  kableExtra::kable_styling()
```

#### Across runs

```{r}
pwr_across = check_est_diff %>%
  # Get mean estimation for each participant per run
  .[, .(mean_est = mean(diff_from_true, na.rm = TRUE),
        sd_est = sd(diff_from_true, na.rm = TRUE)),
    by = c('participant_id', 'est_stim')] %>%
  data.table::dcast(participant_id ~ paste0('mean_est_',est_stim), value.var = 'mean_est') %>%
  .[, diff_3m2 := mean_est_3 - mean_est_2] %>%
  .[, .(mean_diff_3m2 = mean(diff_3m2),
        sd_diff_3m2 = sd(diff_3m2))] %>%
  .[, cohens_d := mean_diff_3m2/sd_diff_3m2] %>%
  .[, suggested_sample := pwr::pwr.t.test(d = cohens_d,
                                          sig.level = 0.05,
                                          power = 0.9,
                                          type = 'one.sample',
                                          alternative = 'two.sided')$n]
knitr::kable(pwr_across) %>%
  kableExtra::kable_styling()
```

## Correctness

```{r}
# Percent of non-optimal choices
check_noc = data %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  .[trial_type == 'choice',] %>%
  .[, correct_choice := if(option_left > option_right) 'left' else 'right',
    by = c('participant_id', 'run', 'trial')] %>%
  .[, correct := correct_choice == choice] %>%
  # Get percentage of correct choices (exclude timeouts from overall trials)
  .[, .(perc_correct = sum(as.numeric(correct), na.rm = TRUE) / length(which(!is.na(as.numeric(correct))))),
    by = c('participant_id', 'group', 'run', 'comp')]

# Difference between 1v2 and 2v3
check_noc_diff = check_noc %>%
  # Only select critical comparisons
  .[comp %in% c('1v2', '2v3'),] %>%
  data.table::dcast(participant_id + group + run ~ paste('mean_', comp, sep = ''),
                    value.var = 'perc_correct') %>%
  .[, diff_1v2m2v3 := mean_1v2 - mean_2v3]
```

### Within run

```{r}
pwr_within = check_noc_diff %>%
  .[, .(mean_diff_1v2m2v3 = mean(diff_1v2m2v3),
        sd_diff_1v2m2v3 = sd(diff_1v2m2v3)),
    by = 'run'] %>%
  .[, cohens_d := mean_diff_1v2m2v3 / sd_diff_1v2m2v3] %>%
  .[, suggested_sample := pwr::pwr.t.test(d = cohens_d,
                                          sig.level = 0.05,
                                          power = 0.9,
                                          type = 'one.sample',
                                          alternative = 'two.sided')$n,
    by = 'run']
knitr::kable(pwr_within) %>%
  kableExtra::kable_styling()
```

### Across run

```{r}
pwr_across = check_noc_diff %>%
  .[, .(mean_diff_1v2m2v3 = mean(diff_1v2m2v3),
        sd_diff_1v2m2v3 = sd(diff_1v2m2v3))] %>%
  .[, cohens_d := mean_diff_1v2m2v3 / sd_diff_1v2m2v3] %>%
  .[, suggested_sample := pwr::pwr.t.test(d = cohens_d,
                                          sig.level = 0.05,
                                          power = 0.9,
                                          type = 'one.sample',
                                          alternative = 'two.sided')$n]
knitr::kable(pwr_across)  %>%
  kableExtra::kable_styling()
```

