---
title: "Results 02"
output:
  html_document:
    toc: yes
    self_contained: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    toc_depth: 3
    number_sections: False
    highlight: pygments
    theme: cosmo
    code_folding: "hide"
    df_print: paged
    fig_caption: true
  pdf_document:
    toc: yes
    fig_caption: true
    latex_engine: xelatex
fig.align: "center"
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{AgfaRotisSansSerif}
email: koch@mpib-berlin.mpg.de
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(data.table)
library(here)
library(magrittr)
library(ggplot2)
library(viridis)
library(binhf)
library(pwr)
library(knitr)
library(kableExtra)
library(sdamr)
library(gghalves)
library(lme4)
library(emmeans)
library(papeR)
library(ggridges)
library(bmsR)
```

```{r, message=FALSE}
# Get directory of repository
base_path = here::here()

# Load pre-written functions
source_path = file.path(base_path, 'code', 'utils',
                        fsep = .Platform$file.sep)
source_files = list.files(source_path, pattern = "[.][rR]$",
                          full.names = TRUE, recursive = TRUE)
invisible(lapply(source_files, function(x) source(x)))

source_path = file.path(base_path, 'code', 'model_fitting', 'LRfunction.R',
                        fsep = .Platform$file.sep)
source(source_path)


# Get plot colors/linetypes
custom_guides = Get_plot_guides()
```

# Setup

```{r}
# Load data for demographics
data_participants = Load_data() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE)

# Load modelling results
data = Load_model_fits_new() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  .[, ':='(participant_id = as.factor(participant_id),
           group = as.factor(group),
           sex = as.factor(sex),
           starting_values = as.factor(starting_values))]
# Sort model levels by number of parameters
data$model = factor(data$model, levels = c('rw',
                                           'uncertainty',
                                           'surprise',
                                           'uncertainty_surprise'))

# Split data by starting values (random vs. fixed)
data_fixed = data[starting_values == 'fixed']
file = file.path(base_path, 'derivatives', '')
data_random = data[starting_values == 'random']

# get number of participants
n_participants = length(unique(data$participant_id))

# Load nico's model data
file = file.path(base_path, 'derivatives', 'model_fitting', 'fitting_nico.tsv')
data_fixed_nico = data.table::fread(file, sep = '\t', na.strings = 'n/a')
# Name variable levels equally between both approaches
data_fixed_nico = data_fixed_nico[order(participant_id, model)]
data_fixed_nico[x == 'intercept']$x = '(Intercept)'
# Add identifyer for nicos data
data_fixed_nico$author = 'nico'

# Prepare comparison data
data_fit_comp = data_fixed[iter == 1 & variable %in% c('coefs', 'x0')] %>%
  .[order(participant_id, model)]
# Add identifyer for christoph's data
data_fit_comp$author = 'christoph'
# Delete additional columns to make data appandable
select_cols = colnames(data_fixed_nico)
data_fit_comp = data_fit_comp[,.SD, .SDcols = select_cols]

# Append data of both approaches
data_fit_comp = rbind(data_fit_comp, data_fixed_nico)

# Bring into comparison format (approaches as two columns) and get difference
data_fit_comp = data_fit_comp %>%
  .[, ':='(variable = as.factor(variable),
           x = as.factor(x),
           author = as.factor(author))] %>%
  setcolorder(., c('author', 'participant_id')) %>%
  dcast(participant_id + model + variable + x ~ paste0('value_', author), value.var = 'value') %>%
  .[, value_diff := value_christoph - value_nico] %>%
  # Add approach identifiers (n evaluations, tolerance, algorithm)
  .[, ':='(starting_values = unique(data_fixed$starting_values),
           algorithm = unique(data_fixed$algorithm),
           xtol_rel = unique(data_fixed$xtol_rel),
           maxeval = unique(data_fixed$maxeval))]

unique(data_random$x)
```



# Variability in fitting iterations

## Fixed starting values

```{r}
data_fit_var = data_fixed %>%
  .[variable == 'coefs',] %>%
  .[, .(mean_coef = mean(value),
        sd_coef = sd(value),
        n = .N),
    by = c('participant_id', 'model', 'x')]

knitr::kable(data_fit_var) %>%
  kableExtra::kable_styling()
```

## Random starting values

```{r}
data_fit_var = data_random %>%
  .[variable == 'coefs',] %>%
  .[, .(mean_coef = mean(value),
        sd_coef = sd(value),
        n = .N),
    by = c('participant_id', 'model', 'x')]

knitr::kable(data_fit_var) %>%
  kableExtra::kable_styling()
```

```{r}
data_plot = data[variable == 'coefs' & iter == 1]
p = ggplot(data = data_plot,
           aes(x = starting_values,
               y = value,
               group = participant_id)) +
  geom_line(alpha = 0.1) +
  facet_grid(x ~ model, scales = 'free_y')
p
```

```{r}
# See if there were really random starting values
data[variable == 'x0' & starting_values == 'random'] %>%
  .[, .(mean_fixed = mean(value),
        sd_fixed = sd(value)),
    by = c('starting_values', 'model', 'iter', 'x')]
```

```{r}
# Look at difference between fits from fixed and random
check_fixedVSrandom = data[variable == 'coefs'] %>%
  data.table::dcast(., participant_id + group + age + sex + iter + model + x ~ starting_values, value.var = c('value', 'AIC')) %>%
  .[, ':='(value_FmR = value_fixed - value_random,
           AIC_FmR = AIC_fixed - AIC_random)]

p = ggplot(data = check_fixedVSrandom,
           aes(x = x,
               y = value_FmR)) +
  geom_point(alpha = 0.1, position = position_jitter(height = 0,
                                                     width = 0.3,
                                                     seed = 666))
p

#min(check_fixedVSrandom$value_FmR, na.rm = TRUE)
#max(check_fixedVSrandom$value_FmR, na.rm = TRUE)
```

```{r}
# AIC differences between fits
p = ggplot(data = check_fixedVSrandom,
           aes(x = model,
               y = AIC_FmR)) +
  geom_point(alpha = 0.1, position = position_jitter(height = 0,
                                                     width = 0.3,
                                                     seed = 666))
p
```

```{r}
# See if difference in starting values is correlated with difference in solution
data_corr_random = data[variable != 'LRs' & x %in% c('alpha', 'l', 'u', 's')] %>%
  data.table::dcast(participant_id + group + starting_values + iter + model + x ~ variable, value.var = 'value') %>%
  .[starting_values == 'random'] %>%
  .[model == 'uncertainty']

p = ggplot(data = data_corr_random,
           aes(x = x0,
               y = coefs,
               color = participant_id)) +
  geom_point(size = 0.1) +
  geom_smooth(method = 'lm',
              formula = y ~ x,
              size = 0.1) +
  theme(legend.position = 'none')
p
```



# What has Nico looked at?

- RW analyses (Do participants behave like in an RL task?):
   - (?) **Prob choosing left bandit positively related to value of left bandit (younger: p < .001; older: p <.001)**
   - (?) **Prob choosing right bandit negatively related to value of left bandit (both p < .001)**
   - (Y) Beta of left and right values related to percent of correct choices (r = .43, r = -.41; both p < .001)
   - (Y) Average LR = .13
   - (Y) LR positively correlated with overall performance (r = .29; p = .003)
- Average AIC comparison between sophisticated models and RW:
   - **(N)** AIC average over all participants for each model [uncertainty_surprise (242.02) < surprise (242.47) < uncertainty (245.13) < rw (247.40)]
   - Splitting by age group:
      - (?) **YA: uncertainty < rw; surprise < rw; uncertainty_surprise < rw (ps < .01)** **(CORRECTED??)**
      - (?) **OA: uncertainty < rw; surprise < rw; uncertainty_surprise < rw (ps < .03)** **(CORRECTED??)**
- Surprise or uncertainty mechanism better explanation in different age groups?
   - Count best fit of non-nested models:
      - **(N)** YA: uncertainty (24) > surprise (18) > rw (9)
      - (Y) OA: surprise (25) > surprise (13) = rw (13)
   - Protected exceedance probability:
      - **(N)** YA: uncertainty (72%)
      - (Y) OA: surprise (80.1%)
- Chi-Square of model frequencies across age groups
   - **(N)** marginal (p = .077)

## RW: Relationship with performance: Beta left/right & LR

```{r}
# Load choice data
data_behav = Load_data() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  Add_comp(.) %>%
  .[,run := as.factor(run)]

# Percentage of optimal choices
check_noc = data_behav %>%
  Add_comp(.) %>%
  .[, trial := seq(.N),
    by = c('participant_id', 'run')] %>%
  .[trial_type == 'choice',] %>%
  .[, correct_choice := if(option_left > option_right) 'left' else 'right',
    by = c('participant_id', 'run', 'trial')] %>%
  .[, correct := correct_choice == choice] %>%
  # Get percentage of correct choices (exclude timeouts from overall trials)
  .[, .(perc_correct = sum(as.numeric(correct), na.rm = TRUE) / length(which(!is.na(as.numeric(correct))))),
    by = c('participant_id', 'group')]

data_corr = data %>%
  .[variable == 'coefs' & model == 'rw' & starting_values == 'random' & iter == 1] %>%
  data.table::dcast(participant_id + group + AIC ~ x, value.var = 'value') %>%
  data.table::merge.data.table(., check_noc, by = c('participant_id', 'group'))

# Plot correlation
p1 = ggplot(data_corr,
           aes(x = V1,
               y = perc_correct)) +
  geom_point() +
  geom_smooth(method = 'lm',
              formula = y ~ x)
p1

p2 = ggplot(data_corr,
           aes(x = V2,
               y = perc_correct)) +
  geom_point() +
  geom_smooth(method = 'lm',
              formula = y ~ x)
p2

p3 = ggplot(data_corr,
           aes(x = alpha,
               y = perc_correct)) +
  geom_point() +
  geom_smooth(method = 'lm',
              formula = y ~ x)
p3

# V1 ~ performance
cor.test(data_corr$V1, data_corr$perc_correct)
m = lm(perc_correct ~ V1,
       data = data_corr)
Anova(m)

# V2 ~ performance
cor.test(data_corr$V2, data_corr$perc_correct)
m = lm(perc_correct ~ V2,
       data = data_corr)
anova(m)

# If both are in the model, only the first captures variance, likely because of
# the high correlation between V1 and V2
m = lm(perc_correct ~ V1 + V2,
       data = data_corr)
anova(m)

# alpha ~ performance
cor.test(data_corr$alpha, data_corr$perc_correct)
m = lm(perc_correct ~ alpha,
       data = data_corr)
anova(m)

# Full model
m = lm(perc_correct ~ V1 + V2 + alpha,
       data = data_corr)
anova(m)

# Average LR
mean(data_corr$alpha)
```

# AIC average over participants

## Across both age groups

```{r}
data_aic = data %>%
  .[iter == 1 & starting_values == 'random'] %>%
  .[, .(AIC = mean(AIC),
        sd_AIC = sd(AIC)),
    by = c('participant_id', 'group', 'model')]

data_aic_all = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = 'model'] %>%
  # Sort by lowest AIC
  .[order(rank(AIC))]
data_aic_all
```

## Age split

```{r}
data_aic_age = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = c('group', 'model')] %>%
  # Sort by age and lowest AIC
  .[order(group, AIC)]
data_aic_age
```

# Best fitting model counts

## For all models

```{r}
# Get winning model within each participant
data_counts = data_aic %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_all = data_counts %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_age = data_counts %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_all
data_counts_age

chisq.test(cbind(data_counts_age[group == 'older']$n_winning,
                 data_counts_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p

```
### Exceedance probs

```{r}
data_ep = data_aic %>%
  .[, nAIC := -(AIC)] %>%
  data.table::dcast(participant_id + group ~ model, value.var = 'nAIC')

bmsR::VB_bms(cbind(data_ep$rw,
                   data_ep$uncertainty,
                   data_ep$surprise,
                   data_ep$uncertainty_surprise),
             n_samples = 1000)
```


## For non-nested models

```{r}
# Get winning model within each participant
data_counts_nn = data_aic %>%
  .[model != 'uncertainty_surprise'] %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_nn_all = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_nn_age = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_nn_all
data_counts_nn_age

chisq.test(cbind(data_counts_nn_age[group == 'older']$n_winning,
                 data_counts_nn_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_nn_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p
```

## Exceedance probs

```{r}
# Younger
bmsR::VB_bms(cbind(data_ep[group == 'younger']$rw,
                   data_ep[group == 'younger']$uncertainty,
                   data_ep[group == 'younger']$surprise,
                   data_ep[group == 'younger']$uncertainty_surprise),
             n_samples = 1000)$pxp


# Older
bmsR::VB_bms(cbind(data_ep[group == 'older']$rw,
                   data_ep[group == 'older']$uncertainty,
                   data_ep[group == 'older']$surprise,
                   data_ep[group == 'older']$uncertainty_surprise),
             n_samples = 1000)$pxp
```

# Parameters of surprise model

```{r}
# Get data of surprise model
data_surprise = data[starting_values == 'random' & model == 'surprise' & iter == 1] %>%
  # Select learning coefficients
  .[variable == 'coefs' & x %in% c('l','u','s')] %>%
  .[, ':='(variable = as.factor(variable),
           x = as.factor(x))] %>%
  data.table::dcast(participant_id + group ~ x, value.var = 'value') %>%
  .[, uml := u-l] %>%
  data.table::melt(id.vars = c('participant_id', 'group'))

p = ggplot(data = data_surprise,
           aes(x = group,
               y = value,
               color = group)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  facet_wrap(~variable,
             scale = 'free_y')
p
```

```{r}
# Get data of surprise model
data_surprise_v = data[starting_values == 'random' & model == 'surprise' & iter == 1] %>%
  # Select learning coefficients
  .[variable == 'coefs'] %>%
  .[, ':='(variable = as.factor(variable),
           x = as.factor(x))] %>%
  data.table::dcast(participant_id + group ~ x, value.var = 'value') %>%
  .[, uml := u-l]

p = ggplot(data = data_surprise_v,
           aes(x = V1,
               y = l,
               color = group)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  geom_smooth(method='lm',
              formula = y~x)
p
#p


p = ggplot(data = data_surprise_v,
           aes(x = V1,
               y = s,
               color = group)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  geom_smooth(method='lm',
              formula = y~x)
p
#p


p = ggplot(data = data_surprise_v,
           aes(x = V1,
               y = uml,
               color = group)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.7) +
  geom_smooth(method='lm',
              formula = y~x)
p
#p
cor.test(data_surprise_v$V1, data_surprise_v$uml, method = 'spearman')
cor.test(data_surprise_v[group == 'younger']$V1, data_surprise_v[group == 'younger']$uml)
cor.test(data_surprise_v[group == 'older']$V1, data_surprise_v[group == 'older']$uml)



p = ggplot(data = data_surprise_v,
           aes(x = V2,
               y = uml,
               color = group)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.7) +
  geom_smooth(method='lm',
              formula = y~x)
p
```


### Get LR functions

```{r}
data_lr_func = data_surprise %>%
  .[, .(alpha_star = LRfunction(low = value[variable == 'l'],
                                up = value[variable == 'u'],
                                slope = value[variable == 's'],
                                PE = seq(60))[[1]],
        PEscaled = LRfunction(low = value[variable == 'l'],
                              up = value[variable == 'u'],
                              slope = value[variable == 's'],
                              PE = seq(60))[[2]],
        l = value[variable == 'l'],
        s = value[variable == 's'],
        u = value[variable == 'u'],
        uml = value[variable == 'uml']),
    by = c('participant_id', 'group')] %>%
  .[, ':='(slope_pos = s > 0,
           uml_pos = uml > 0)]

p = ggplot(data = data_lr_func,
           aes(x = PEscaled,
               y = alpha_star,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.5) +
  facet_wrap(uml_pos ~ group)
p
```

```{r}
count_uml = data_lr_func %>%
  .[, .(uml_pos = unique(uml_pos)),
    by = c('participant_id', 'group')] %>%
  .[, .(count_uml_pos = sum(uml_pos),
        n = .N),
    by = c('group')]
count_uml
```


```{r}
# Get uml parameter for rising and falling graphs
data_uml = data_surprise[variable == 'uml'] %>%
  .[, uml := value] %>%
  .[, uml_pos := uml >= 0] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'uml_pos')]

data_lrs = data_random[model == 'surprise' & variable == 'LRs' & !is.na(value)] %>%
  # Fuse encountered LRs with uml
  data.table::merge.data.table(., data_uml,
                               by = c('participant_id', 'group')) %>%
  .[, uml_pos := as.factor(uml_pos)]

data_extremes = data_lrs %>%
  .[, x := as.numeric(x)] %>%
  # Get alpha* at lowest and highest |PE|
  .[, .(a_at_min_pe = value[x == min(x)],
        a_at_max_pe = value[x == max(x)]),
    by = c('participant_id', 'group')] %>%
  # Get difference between both to get rising or falling slope
  .[, rising := a_at_max_pe - a_at_min_pe] %>%
  .[, rising := rising > 0]

data_lrs = data_lrs %>%
  data.table::merge.data.table(., data_extremes,
                               by = c('participant_id', 'group'))

p = ggplot(data = data_lrs,
           aes(x = as.numeric(x),
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  labs(x = '|PE|',
        y = 'alpha*') +
  facet_wrap(rising~group)
p
```

```{r}
data_rising = data_lrs %>%
  .[, .(rising = unique(rising)),
    by = c('participant_id', 'group')] %>%
  .[, .(count_rising = sum(rising)),
    by = c('group')]
```

# Parameters of unc model

```{r}
data_unc = data_random[variable == 'coefs' & model == 'uncertainty' & x %in% c('V1u', 'V2u')] %>%
  data.table::dcast(participant_id + group + model + AIC ~ x, value.var = 'value') %>%
  .[, V2umV1u := V2u - V1u] %>%
  data.table::melt(id.vars = c('participant_id', 'group', 'model', 'AIC'),
                   variable.name = 'variable',
                   value.name = 'value')

p = ggplot(data = data_unc,
           aes(x = group,
               y = value,
               color = group)) +
  geom_point() +
  stat_summary(fun = 'mean', geom = 'point', size = 5, shape = 18, color = 'white') +
  facet_wrap(~variable)
p
```


# Only surprise best fitters

```{r}
data_winning_model = data_aic %>%
  .[, .(winning_model = unique(winning_model)),
    by = c('participant_id', 'group')] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'winning_model')]

data_lrs_winning = data.table::merge.data.table(data_lrs, data_winning_model,
                                                by = c('participant_id',
                                                       'group'))
data_lrs_surprise = data_lrs_winning[winning_model == 'surprise']

p = ggplot(data = data_lrs_surprise,
           aes(x = as.numeric(x),
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  facet_wrap(~group)
p
```

```{r}
data_surprise_winning_only = data.table::merge.data.table(data_surprise,
                                                          data_winning_model,
                                                          by = c('participant_id',
                                                                 'group')) %>%
  .[winning_model == 'surprise']

p = ggplot(data = data_surprise_winning_only,
           aes(x = group,
               y = value,
               color = group)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  facet_wrap(~variable,
             scale = 'free_y')
p
```

# Parameters of uncertainty_surprise model

```{r}
# Get data of surprise model
data_u_surprise = data[starting_values == 'random' & model == 'uncertainty_surprise' & iter == 1] %>%
  # Select learning coefficients
  .[variable == 'coefs' & x %in% c('l','u','s')] %>%
  .[, ':='(variable = as.factor(variable),
           x = as.factor(x))] %>%
  data.table::dcast(participant_id + group ~ x, value.var = 'value') %>%
  .[, uml := u-l] %>%
  data.table::melt(id.vars = c('participant_id', 'group'))

p = ggplot(data = data_u_surprise,
           aes(x = group,
               y = value,
               color = group)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  facet_wrap(~variable,
             scale = 'free_y')
p
```

```{r}
# Get uml parameter for rising and falling graphs
data_uml = data_u_surprise[variable == 'uml'] %>%
  .[, uml := value] %>%
  .[, uml_pos := uml >= 0] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'uml_pos')]

data_lrs = data_random[model == 'uncertainty_surprise' & variable == 'LRs' & !is.na(value)] %>%
  # Fuse encountered LRs with uml
  data.table::merge.data.table(., data_uml,
                               by = c('participant_id', 'group')) %>%
  .[, uml_pos := as.factor(uml_pos)]

p = ggplot(data = data_lrs,
           aes(x = as.numeric(x),
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  facet_wrap(uml_pos~group)
p
```


# Uncertainty-surprise parameters

```{r}
# Get data of surprise model
data_u_surprise = data[starting_values == 'random' & model == 'uncertainty_surprise' & iter == 1] %>%
  # Select learning coefficients
  .[variable == 'coefs' & x %in% c('l','u','s', 'pi')] %>%
  .[, ':='(variable = as.factor(variable),
           x = as.factor(x))] %>%
  data.table::dcast(participant_id + group ~ x, value.var = 'value') %>%
  .[, uml := u-l] %>%
  data.table::melt(id.vars = c('participant_id', 'group'))

p = ggplot(data = data_u_surprise,
           aes(x = group,
               y = value,
               color = group)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  facet_wrap(~variable,
             scale = 'free_y')
p
```

# Nico's fitting with 5000 maxeval

```{r}
file = file.path(base_path, 'derivatives', 'model_fitting', 'fitting_nico_5000.tsv',
                 fsep = .Platform$file.sep)
data_nico_5000 = data.table::fread(file, sep = '\t', na.strings = 'n/a')

data_group = data %>%
  .[, .(group = unique(group)),
    by = 'participant_id']
```



# AIC average over participants

## Across both age groups

```{r}
data_aic = data_nico_5000 %>%
  data.table::merge.data.table(., data_group,
                               by = 'participant_id') %>%
  .[, .(AIC = mean(AIC),
        sd_AIC = sd(AIC)),
    by = c('participant_id', 'group', 'model')]

data_aic_all = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = 'model'] %>%
  # Sort by lowest AIC
  .[order(rank(AIC))]
data_aic_all
```

## Age split

```{r}
data_aic_age = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = c('group', 'model')] %>%
  # Sort by age and lowest AIC
  .[order(group, AIC)]
data_aic_age
```

```{r}
# Get winning model within each participant
data_counts = data_aic %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_all = data_counts %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_age = data_counts %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_all
data_counts_age

chisq.test(cbind(data_counts_age[group == 'older']$n_winning,
                 data_counts_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p
```

```{r}
# Get winning model within each participant
data_counts_nn = data_aic %>%
  .[model != 'uncertainty_surprise'] %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_nn_all = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_nn_age = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_nn_all
data_counts_nn_age

chisq.test(cbind(data_counts_nn_age[group == 'older']$n_winning,
                 data_counts_nn_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_nn_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p
```

# Parameters of surprise model

```{r}
# Get data of surprise model
data_surprise = data_nico_5000[model == 'surprise'] %>%
  data.table::merge.data.table(., data_group,
                               by = 'participant_id') %>%
  # Select learning coefficients
  .[variable == 'coefs' & x %in% c('l','u','s')] %>%
  .[, ':='(variable = as.factor(variable),
           x = as.factor(x))] %>%
  data.table::dcast(participant_id + group ~ x, value.var = 'value') %>%
  .[, uml := u-l] %>%
  data.table::melt(id.vars = c('participant_id', 'group'))

p = ggplot(data = data_surprise,
           aes(x = group,
               y = value,
               color = group)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  facet_wrap(~variable,
             scale = 'free_y')
p

summary(data_nico_5000[x == 'l']$value)
summary(data_nico_5000[x == 'u']$value)
```

```{r}
data_lrs = data_random[model == 'surprise' & variable == 'LRs' & !is.na(value)]

p = ggplot(data = data_lrs,
           aes(x = as.numeric(x),
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  facet_wrap(~group)
p
```

# Only bandit 1v2

```{r}
# Load modelling results
data = Load_model_fits_new() %>%
  Apply_exclusion_criteria(., choice_based_exclusion = TRUE) %>%
  .[, ':='(participant_id = as.factor(participant_id),
           group = as.factor(group),
           sex = as.factor(sex),
           starting_values = as.factor(starting_values))]
# Sort model levels by number of parameters
data$model = factor(data$model, levels = c('rw',
                                           'uncertainty',
                                           'surprise',
                                           'uncertainty_surprise'))

# Split data by starting values (random vs. fixed)
data_1v2 = data[starting_values == 'fixed']

# get number of participants
n_participants = length(unique(data$participant_id))

file = file.path('/Users/koch/Desktop/data_pedlr_modelfitting.tsv')
data.table::fwrite(data_1v2, file, sep = '\t', na = 'n/a')


# Load nico's model data
file = file.path(base_path, 'derivatives', 'model_fitting', 'fitting_nico.tsv')
data_fixed_nico = data.table::fread(file, sep = '\t', na.strings = 'n/a')
# Name variable levels equally between both approaches
data_fixed_nico = data_fixed_nico[order(participant_id, model)]
data_fixed_nico[x == 'intercept']$x = '(Intercept)'
# Add identifyer for nicos data
data_fixed_nico$author = 'nico'
```

```{r}
data_aic = data_1v2 %>%
  .[iter == 1] %>%
  .[, .(AIC = mean(AIC),
        sd_AIC = sd(AIC)),
    by = c('participant_id', 'group', 'model')]

data_aic_all = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = 'model'] %>%
  # Sort by lowest AIC
  .[order(rank(AIC))]
data_aic_all
```

## Age split

```{r}
data_aic_age = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = c('group', 'model')] %>%
  # Sort by age and lowest AIC
  .[order(group, AIC)]
data_aic_age
```

```{r}
# Get winning model within each participant
data_counts = data_aic %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_all = data_counts %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_age = data_counts %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_all
data_counts_age

chisq.test(cbind(data_counts_age[group == 'older']$n_winning,
                 data_counts_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p
```

```{r}
# Get data of surprise model
data_surprise = data_1v2[model == 'surprise' & iter == 1] %>%
  # Select learning coefficients
  .[variable == 'coefs' & x %in% c('l','u','s')] %>%
  .[, ':='(variable = as.factor(variable),
           x = as.factor(x))] %>%
  data.table::dcast(participant_id + group ~ x, value.var = 'value') %>%
  .[, uml := u-l] %>%
  data.table::melt(id.vars = c('participant_id', 'group'))

p = ggplot(data = data_surprise,
           aes(x = group,
               y = value,
               color = group)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  geom_point(position = position_jitter(height = 0,
                                        width = 0.1,
                                        seed = 666),
             alpha = 0.2) +
  facet_wrap(~variable,
             scale = 'free_y')
p
```

```{r}
data_u = data_surprise[variable == 'u']

mean(data_u[group == 'older']$value)
mean(data_u[group == 'younger']$value)

wilcox.test(data_u[group == 'older']$value, data_u[group == 'younger']$value)
```


```{r}
# Get uml parameter for rising and falling graphs
data_uml = data_surprise[variable == 'uml'] %>%
  .[, uml := value] %>%
  .[, uml_pos := uml >= 0] %>%
  .[, .SD, .SDcols = c('participant_id', 'group', 'uml_pos')]

data_lrs = data_1v2[model == 'surprise' & variable == 'LRs' & !is.na(value)] %>%
  # Fuse encountered LRs with uml
  data.table::merge.data.table(., data_uml,
                               by = c('participant_id', 'group')) %>%
  .[, uml_pos := as.factor(uml_pos)]

data_extremes = data_lrs %>%
  .[, x := as.numeric(x)] %>%
  # Get alpha* at lowest and highest |PE|
  .[, .(a_at_min_pe = value[x == min(x)],
        a_at_max_pe = value[x == max(x)]),
    by = c('participant_id', 'group')] %>%
  # Get difference between both to get rising or falling slope
  .[, rising := a_at_max_pe - a_at_min_pe] %>%
  .[, rising := rising > 0]

data_lrs = data_lrs %>%
  data.table::merge.data.table(., data_extremes,
                               by = c('participant_id', 'group'))

p = ggplot(data = data_lrs,
           aes(x = as.numeric(x),
               y = value,
               group = participant_id,
               color = group)) +
  geom_line(alpha = 0.2) +
  labs(x = '|PE|',
        y = 'alpha*') +
  facet_wrap(rising~group)
p
```

```{r}
# Get winning model within each participant
data_counts_nn = data_aic %>%
  .[model != 'uncertainty_surprise'] %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_nn_all = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_nn_age = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_nn_all
data_counts_nn_age

chisq.test(cbind(data_counts_nn_age[group == 'older']$n_winning,
                 data_counts_nn_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_nn_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p
```

```{r}
file = file.path(base_path, 'derivatives', 'model_fitting', 'fitting_nico_1000.tsv',
                 fsep = .Platform$file.sep)
data_nico_1000 = data.table::fread(file, sep = '\t', na.strings = 'n/a')
```

```{r}
data_aic = data_nico_1000 %>%
  data.table::merge.data.table(., data_group,
                               by = 'participant_id') %>%
  .[, .(AIC = mean(AIC),
        sd_AIC = sd(AIC)),
    by = c('participant_id', 'group', 'model')]

data_aic_all = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = 'model'] %>%
  # Sort by lowest AIC
  .[order(rank(AIC))]
data_aic_all
```

## Age split

```{r}
data_aic_age = data_aic %>%
  .[, .(AIC = mean(AIC)),
    by = c('group', 'model')] %>%
  # Sort by age and lowest AIC
  .[order(group, AIC)]
data_aic_age
```

```{r}
# Get winning model within each participant
data_counts = data_aic %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_all = data_counts %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_age = data_counts %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_all
data_counts_age

chisq.test(cbind(data_counts_age[group == 'older']$n_winning,
                 data_counts_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p
```

```{r}
# Get winning model within each participant
data_counts_nn = data_aic %>%
  .[model != 'uncertainty_surprise'] %>%
  .[, ':='(lowest_AIC = min(AIC),
           loc_winning = AIC == min(AIC),
           # Name of winning model
           winning_model = model[AIC == min(AIC)]),
    by = 'participant_id'] %>%
  # Only keep winning model
  .[loc_winning == TRUE]

# Count winning models across participants
data_counts_nn_all = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('model')]

# Count winning models within age-groups
data_counts_nn_age = data_counts_nn %>%
  .[, .(n_winning = .N),
    by = c('group', 'model')] %>%
  .[order(group, -rank(model))]
  
data_counts_nn_all
data_counts_nn_age

chisq.test(cbind(data_counts_nn_age[group == 'older']$n_winning,
                 data_counts_nn_age[group == 'younger']$n_winning))

p = ggplot(data = data_counts_nn_age,
           aes(x = model,
               y = n_winning,
               fill = model)) +
  geom_col() +
  facet_wrap(~group)
p
```

```{r}
data_1v2
```

```{r}
data_nico_1000
```

